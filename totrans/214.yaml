- en: Calculus for Machine Learning (7-day mini-course)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://machinelearningmastery.com/calculus-for-machine-learning-7-day-mini-course/](https://machinelearningmastery.com/calculus-for-machine-learning-7-day-mini-course/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Calculus for Machine Learning Crash Course.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Get familiar with the calculus techniques in machine learning in 7 days.
  prefs: []
  type: TYPE_NORMAL
- en: Calculus is an important mathematics technique behind many machine learning
    algorithms. You don’t always need to know it to use the algorithms. When you go
    deeper, you will see it is ubiquitous in every discussion on the theory behind
    a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: As a practitioner, we are most likely not going to encounter very hard calculus
    problems. If we need to do one, there are tools such as computer algebra systems
    to help, or at least, verify our solution. However, what is more important is
    understanding the idea behind calculus and relating the calculus terms to its
    use in our machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In this crash course, you will discover some common calculus ideas used in machine
    learning. You will learn with exercises in Python in seven days.
  prefs: []
  type: TYPE_NORMAL
- en: This is a big and important post. You might want to bookmark it.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: '![Calculus for Machine Learning (7-Day Mini-Course)](../Images/033026b5bed2437a268f6833a6b6dbef.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculus for Machine Learning (7-Day Mini-Course)
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [ArnoldReinhold](https://commons.wikimedia.org/wiki/File:Mechanical_integrator_CHM.agr.jpg),
    some rights reserved.
  prefs: []
  type: TYPE_NORMAL
- en: Who Is This Crash-Course For?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we get started, let’s make sure you are in the right place.
  prefs: []
  type: TYPE_NORMAL
- en: This course is for developers who may know some applied machine learning. Maybe
    you know how to work through a predictive modeling problem end to end, or at least
    most of the main steps, with popular tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'The lessons in this course do assume a few things about you, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: You know your way around basic Python for programming.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may know some basic linear algebra.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may know some basic machine learning models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You do NOT need to be:'
  prefs: []
  type: TYPE_NORMAL
- en: A math wiz!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A machine learning expert!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This crash course will take you from a developer who knows a little machine
    learning to a developer who can effectively talk about the calculus concepts in
    machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: This crash course assumes you have a working Python 3.7 environment with
    some libraries such as SciPy and SymPy installed. If you need help with your environment,
    you can follow the step-by-step tutorial here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[How to Set Up Your Python Environment for Machine Learning With Anaconda](https://machinelearningmastery.com/setup-python-environment-machine-learning-deep-learning-anaconda/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crash-Course Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This crash course is broken down into seven lessons.
  prefs: []
  type: TYPE_NORMAL
- en: You could complete one lesson per day (recommended) or complete all of the lessons
    in one day (hardcore). It really depends on the time you have available and your
    level of enthusiasm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is a list of the seven lessons that will get you started and productive
    with data preparation in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lesson 01**: Differential calculus'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lesson 02**: Integration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lesson 03**: Gradient of a vector function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lesson 04**: Jacobian'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lesson 05**: Backpropagation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lesson 06**: Optimization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lesson 07**: Support vector machine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each lesson could take you 5 minutes or up to 1 hour. Take your time and complete
    the lessons at your own pace. Ask questions, and even post results in the comments
    below.
  prefs: []
  type: TYPE_NORMAL
- en: 'The lessons might expect you to go off and find out how to do things. I will
    give you hints, but part of the point of each lesson is to force you to learn
    where to go to look for help with and about the algorithms and the best-of-breed
    tools in Python. (**Hint**: *I have all of the answers on this blog; use the search
    box*.)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Post your results in the comments**; I’ll cheer you on!'
  prefs: []
  type: TYPE_NORMAL
- en: Hang in there; don’t give up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lesson 01: Differential Calculus'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this lesson, you will discover what is differential calculus or differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: Differentiation is the operation of transforming one mathematical function to
    another, called the derivative. The derivative tells the slope, or the rate of
    change, of the original function.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we have a function $f(x)=x^2$, its derivative is a function
    that tells us the rate of change of this function at $x$. The rate of change is
    defined as: $$f''(x) = \frac{f(x+\delta x)-f(x)}{\delta x}$$ for a small quantity
    $\delta x$.'
  prefs: []
  type: TYPE_NORMAL
- en: Usually we will define the above in the form of a limit, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: $$f'(x) = \lim_{\delta x\to 0} \frac{f(x+\delta x)-f(x)}{\delta x}$$
  prefs: []
  type: TYPE_NORMAL
- en: to mean $\delta x$ should be as close to zero as possible.
  prefs: []
  type: TYPE_NORMAL
- en: There are several rules of differentiation to help us find the derivative easier.
    One rule that fits the above example is $\frac{d}{dx} x^n = nx^{n-1}$. Hence for
    $f(x)=x^2$, we have the derivative $f'(x)=2x$.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can confirm this is the case by plotting the function $f''(x)$ computed
    according to the rate of change together with that computed according to the rule
    of differentiation. The following uses NumPy and matplotlib in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d16e6be8aa378d74ff2d69384f8795cd.png)'
  prefs: []
  type: TYPE_IMG
- en: In the plot above, we can see the derivative function found using the rate of
    change and then using the rule of differentiation coincide perfectly.
  prefs: []
  type: TYPE_NORMAL
- en: Your Task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can similarly do a differentiation of other functions. For example, $f(x)=x^3
    – 2x^2 + 1$. Find the derivative of this function using the rules of differentiation
    and compare your result with the result found using the rate of limits. Verify
    your result with the plot above. If you’re doing it correctly, you should see
    the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37779721566f5c62b0c3c86a74e2f1c2.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next lesson, you will discover that integration is the reverse of differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lesson 02: Integration'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this lesson, you will discover integration is the reverse of differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we consider a function $f(x)=2x$ and at intervals of $\delta x$ each step
    (e.g., $\delta x = 0.1$), we can compute, say, from $x=-10$ to $x=10$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: f(-10), f(-9.9), f(-9.8), \cdots, f(9.8), f(9.9), f(10)
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, if we have a smaller step $\delta x$, there are more terms in the
    above.
  prefs: []
  type: TYPE_NORMAL
- en: If we multiply each of the above with the step size and then add them up, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: f(-10)\times 0.1 + f(-9.9)\times 0.1 + \cdots + f(9.8)\times 0.1 + f(9.9)\times
    0.1
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: this sum is called the integral of $f(x)$. In essence, this sum is the **area
    under the curve** of $f(x)$, from $x=-10$ to $x=10$. A theorem in calculus says
    if we put the area under the curve as a function, its derivative is $f(x)$. Hence
    we can see the integration as a reverse operation of differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw in Lesson 01, the differentiation of $f(x)=x^2$ is $f''(x)=2x$. This
    means for $f(x)=2x$, we can write $\int f(x) dx = x^2$ or we can say the antiderivative
    of $f(x)=x$ is $x^2$. We can confirm this in Python by calculating the area directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c7f4fe67c151e108c8c9316ae5d14eb5.png)'
  prefs: []
  type: TYPE_IMG
- en: This plot has the same shape as $f(x)$ in Lesson 01\. Indeed, all functions
    differ by a constant (e.g., $f(x)$ and $f(x)+5$) that have the same derivative.
    Hence the plot of the antiderivative computed will be the original shifted vertically.
  prefs: []
  type: TYPE_NORMAL
- en: Your Task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider $f(x)=3x^2-4x$, find the antiderivative of this function and plot
    it. Also, try to replace the Python code above with this function. If you plot
    both together, you should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/87b8e495049342186b5993e435f07e7d.png)'
  prefs:
  - PREF_H3
  type: TYPE_IMG
- en: Post your answer in the comments below. I would love to see what you come up
    with.
  prefs: []
  type: TYPE_NORMAL
- en: These two lessons are about functions with one variable. In the next lesson,
    you will discover how to apply differentiation to functions with multiple variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lesson 03: Gradient of a vector function'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this lesson, you will learn the concept of gradient of a multivariate function.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have a function of not one variable but two or more, the differentiation
    is extended naturally to be the differentiation of the function with respect to
    each variable. For example, if we have the function $f(x,y) = x^2 + y^3$, we can
    write the differentiation in each variable as:'
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: \begin{aligned}
  prefs: []
  type: TYPE_NORMAL
- en: \frac{\partial f}{\partial x} &= 2x \\
  prefs: []
  type: TYPE_NORMAL
- en: \frac{\partial f}{\partial y} &= 3y^2
  prefs: []
  type: TYPE_NORMAL
- en: \end{aligned}
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: Here we introduced the notation of a partial derivative, meaning to differentiate
    a function on one variable while assuming the other variables are constants. Hence
    in the above, when we compute $\frac{\partial f}{\partial x}$, we ignored the
    $y^3$ part in the function $f(x,y)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'A function with two variables can be visualized as a surface on a plane. The
    above function $f(x,y)$ can be visualized using matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/596a5ce293f275e44ae03fe55562346f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The gradient of this function is denoted as:'
  prefs: []
  type: TYPE_NORMAL
- en: $$\nabla f(x,y) = \Big(\frac{\partial f}{\partial x},\; \frac{\partial f}{\partial
    y}\Big) = (2x,\;3y^2)$$
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, at each coordinate $(x,y)$, the gradient $\nabla f(x,y)$ is a vector.
    This vector tells us two things:'
  prefs: []
  type: TYPE_NORMAL
- en: The direction of the vector points to where the function $f(x,y)$ is increasing
    the fastest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of the vector is the rate of change of the function $f(x,y)$ in this
    direction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One way to visualize the gradient is to consider it as a **vector field**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7e7f4e1d780d3b95472d1fa598291b7b.png)'
  prefs: []
  type: TYPE_IMG
- en: The viridis color map in matplotlib will show a larger value in yellow and a
    lower value in purple. Hence we see the gradient is “steeper” at the edges than
    in the center in the above plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we consider the coordinate (2,3), we can check which direction $f(x,y)$
    will increase the fastest using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Its output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The gradient vector according to the formula is (4,27), which the numerical
    result above is close enough.
  prefs: []
  type: TYPE_NORMAL
- en: Your Task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider the function $f(x,y)=x^2+y^2$, what is the gradient vector at (1,1)?
    If you get the answer from partial differentiation, can you modify the above Python
    code to confirm it by checking the rate of change at different directions?
  prefs: []
  type: TYPE_NORMAL
- en: Post your answer in the comments below. I would love to see what you come up
    with.
  prefs: []
  type: TYPE_NORMAL
- en: In the next lesson, you will discover the differentiation of a function that
    takes vector input and produces vector output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lesson 04: Jacobian'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this lesson, you will learn about Jacobian matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function $f(x,y)=(p(x,y), q(x,y))=(2xy, x^2y)$ is one with two input and
    two outputs. Sometimes we call this function taking vector arguments and returning
    a vector value. The differentiation of this function is a matrix called the Jacobian.
    The Jacobian of the above function is:'
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{J} =
  prefs: []
  type: TYPE_NORMAL
- en: \begin{bmatrix}
  prefs: []
  type: TYPE_NORMAL
- en: \frac{\partial p}{\partial x} & \frac{\partial p}{\partial y} \\
  prefs: []
  type: TYPE_NORMAL
- en: \frac{\partial q}{\partial x} & \frac{\partial q}{\partial y}
  prefs: []
  type: TYPE_NORMAL
- en: \end{bmatrix}
  prefs: []
  type: TYPE_NORMAL
- en: '='
  prefs: []
  type: TYPE_NORMAL
- en: \begin{bmatrix}
  prefs: []
  type: TYPE_NORMAL
- en: 2y & 2x \\
  prefs: []
  type: TYPE_NORMAL
- en: 2xy & x^2
  prefs: []
  type: TYPE_NORMAL
- en: \end{bmatrix}
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: In the Jacobian matrix, each row has the partial differentiation of each element
    of the output vector, and each column has the partial differentiation with respect
    to each element of the input vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will see the use of Jacobian later. Since finding a Jacobian matrix involves
    a lot of partial differentiations, it would be great if we could let a computer
    check our math. In Python, we can verify the above result using SymPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Its output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We asked SymPy to define the symbols `x` and `y` and then defined the vector
    function `f`. Afterward, the Jacobian can be found by calling the `jacobian()`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Your Task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider the function
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: f(x,y) = \begin{bmatrix}
  prefs: []
  type: TYPE_NORMAL
- en: \frac{1}{1+e^{-(px+qy)}} & \frac{1}{1+e^{-(rx+sy)}} & \frac{1}{1+e^{-(tx+uy)}}
  prefs: []
  type: TYPE_NORMAL
- en: \end{bmatrix}
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: where $p,q,r,s,t,u$ are constants. What is the Jacobian matrix of $f(x,y)$?
    Can you verify it with SymPy?
  prefs: []
  type: TYPE_NORMAL
- en: In the next lesson, you will discover the application of the Jacobian matrix
    in a neural network’s backpropagation algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lesson 05: Backpropagation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this lesson, you will see how the backpropagation algorithm uses the Jacobian
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we consider a neural network with one hidden layer, we can represent it
    as a function:'
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: y = g\Big(\sum_{k=1}^M u_k f_k\big(\sum_{i=1}^N w_{ik}x_i\big)\Big)
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: The input to the neural network is a vector $\mathbf{x}=(x_1, x_2, \cdots, x_N)$
    and each $x_i$ will be multiplied with weight $w_{ik}$ and fed into the hidden
    layer. The output of neuron $k$ in the hidden layer will be multiplied with weight
    $u_k$ and fed into the output layer. The activation function of the hidden layer
    and output layer are $f$ and $g$, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: If we consider
  prefs: []
  type: TYPE_NORMAL
- en: $$z_k = f_k\big(\sum_{i=1}^N w_{ik}x_i\big)$$
  prefs: []
  type: TYPE_NORMAL
- en: then
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: \frac{\partial y}{\partial x_i} = \sum_{k=1}^M \frac{\partial y}{\partial z_k}\frac{\partial
    z_k}{\partial x_i}
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: If we consider the entire layer at once, we have $\mathbf{z}=(z_1, z_2, \cdots,
    z_M)$ and then
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: \frac{\partial y}{\partial \mathbf{x}} = \mathbf{W}^\top\frac{\partial y}{\partial
    \mathbf{z}}
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: where $\mathbf{W}$ is the $M\times N$ Jacobian matrix, where the element on
    row $k$ and column $i$ is $\frac{\partial z_k}{\partial x_i}$.
  prefs: []
  type: TYPE_NORMAL
- en: This is how the backpropagation algorithm works in training a neural network!
    For a network with multiple hidden layers, we need to compute the Jacobian matrix
    for each layer.
  prefs: []
  type: TYPE_NORMAL
- en: Your Task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The code below implements a neural network model that you can try yourself.
    It has two hidden layers and a classification network to separate points in 2-dimension
    into two classes. Try to look at the function `backward()` and identify which
    is the Jacobian matrix.
  prefs: []
  type: TYPE_NORMAL
- en: If you play with this code, the class `mlp` should not be modified, but you
    can change the parameters on how a model is created.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the next lesson, you will discover the use of differentiation to find the
    optimal value of a function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lesson 06: Optimization'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this lesson, you will learn an important use of differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: Because the differentiation of a function is the rate of change, we can make
    use of differentiation to find the optimal point of a function.
  prefs: []
  type: TYPE_NORMAL
- en: If a function attained its maximum, we would expect it to move from a lower
    point to the maximum, and if we move further, it falls to another lower point.
    Hence at the point of maximum, the rate of change of a function is zero. And vice
    versa for the minimum.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, consider $f(x)=x^3-2x^2+1$. The derivative is $f'(x) = 3x^2-4x$
    and $f'(x)=0$ at $x=0$ and $x=4/3$. Hence these positions of $x$ are where $f(x)$
    is at its maximum or minimum. We can visually confirm it by plotting $f(x)$ (see
    the plot in Lesson 01).
  prefs: []
  type: TYPE_NORMAL
- en: Your task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider the function $f(x)=\log x$ and find its derivative. What will be the
    value of $x$ when $f'(x)=0$? What does it tell you about the maximum or minimum
    of the log function? Try to plot the function of $\log x$ to visually confirm
    your answer.
  prefs: []
  type: TYPE_NORMAL
- en: In the next lesson, you will discover the application of this technique in finding
    the support vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lesson 07: Support vector machine'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this lesson, you will learn how we can convert support vector machine into
    an optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a two-dimensional plane, any straight line can be represented by the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ax+by+c=0$$
  prefs: []
  type: TYPE_NORMAL
- en: 'in the $xy$-coordinate system. A result from the study of coordinate geometry
    says that for any point $(x_0,y_0)$, its **distance** to the line $ax+by+c=0$
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: \frac{\vert ax_0+by_0+c \vert}{\sqrt{a^2+b^2}}
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: Consider the points (0,0), (1,2), and (2,1) in the $xy$-plane, in which the
    first point and the latter two points are in different classes. What is the line
    that best separates these two classes? This is the basis of a support vector machine
    classifier. The support vector is the line of maximum separation in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find such a line, we are looking for:'
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: \begin{aligned}
  prefs: []
  type: TYPE_NORMAL
- en: \text{minimize} && a^2 + b^2 \\
  prefs: []
  type: TYPE_NORMAL
- en: \text{subject to} && -1(0a+0b+c) &\ge 1 \\
  prefs: []
  type: TYPE_NORMAL
- en: '&& +1(1a+2b+c) &\ge 1 \\'
  prefs: []
  type: TYPE_NORMAL
- en: '&& +1(2a+1b+c) &\ge 1'
  prefs: []
  type: TYPE_NORMAL
- en: \end{aligned}
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: The objective $a^2+b^2$ is to be minimized so that the distances from each data
    point to the line are maximized. The condition $-1(0a+0b+c)\ge 1$ means the point
    (0,0) is of class $-1$; similarly for the other two points, they are of class
    $+1$. The straight line should put these two classes in different sides of the
    plane.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a **constrained optimization** problem, and the way to solve it is
    to use the Lagrange multiplier approach. The first step in using the Lagrange
    multiplier approach is to find the partial differentials of the following Lagrange
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: L = a^2+b^2 + \lambda_1(-c-1) + \lambda_2 (a+2b+c-1) + \lambda_3 (2a+b+c-1)
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: 'and set the partial differentials to zero, then solve for $a$, $b$, and $c$.
    It would be too lengthy to demonstrate here, but we can use SciPy to find the
    solution to the above numerically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'It will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The above means the line to separate these three points is $0.67x + 0.67y –
    1 = 0$. Note that if you provided $N$ data points, there would be $N$ constraints
    to be defined.
  prefs: []
  type: TYPE_NORMAL
- en: Your Task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s consider the points (-1,-1) and (-3,-1) to be the first class together
    with (0,0) and point (3,3) to be the second class together with points (1,2) and
    (2,1). In this problem of six points, can you modify the above program and find
    the line that separates the two classes? Don’t be surprised to see the solution
    remain the same as above. There is a reason for it. Can you tell?
  prefs: []
  type: TYPE_NORMAL
- en: Post your answer in the comments below. I would love to see what you come up
    with.
  prefs: []
  type: TYPE_NORMAL
- en: This was the final lesson.
  prefs: []
  type: TYPE_NORMAL
- en: The End!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (*Look How Far You Have Come*)
  prefs: []
  type: TYPE_NORMAL
- en: You made it. Well done!
  prefs: []
  type: TYPE_NORMAL
- en: Take a moment and look back at how far you have come.
  prefs: []
  type: TYPE_NORMAL
- en: 'You discovered:'
  prefs: []
  type: TYPE_NORMAL
- en: What is differentiation, and what it means to a function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to extend differentiation to a function of vector argument
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do differentiation on a vector-valued function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The role of Jacobian in the backpropagation algorithm in neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use differentiation to find the optimum points of a function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machine is a constrained optimization problem, which would need
    differentiation to solve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**How did you do with the mini-course?**'
  prefs: []
  type: TYPE_NORMAL
- en: Did you enjoy this crash course?
  prefs: []
  type: TYPE_NORMAL
- en: '**Do you have any questions? Were there any sticking points?**'
  prefs: []
  type: TYPE_NORMAL
- en: Let me know. Leave a comment below.
  prefs: []
  type: TYPE_NORMAL
