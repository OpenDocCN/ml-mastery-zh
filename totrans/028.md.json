["```py\n!pip install llama-index\n!pip install llama-index-embeddings-openai\n!pip install llama-index-llms-openai\n!pip install llama-index-readers-file\n!pip install docx2txt\n```", "```py\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n# initialize the LLM\nllm = OpenAI(model=\"gpt-4o\")\n\n# initialize the embedding\nembed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n```", "```py\nfrom llama_index.core import Settings\n\n# global settings\nSettings.llm = llm\nSettings.embed_model = embed_model\n```", "```py\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n# load documents\ndata = SimpleDirectoryReader(input_dir=\"/work/data/\",required_exts=[\".docx\"]).load_data()\n\n# indexing documents using vector store\nindex = VectorStoreIndex.from_documents(data)\n```", "```py\nfrom llama_index.core <b>import</b> VectorStoreIndex\n\n# converting vector store to query engine\nquery_engine = index.as_query_engine(similarity_top_k=3)\n\n# generating query response\nresponse = query_engine.query(\"What are the common themes of the blogs?\")\nprint(response)\n```", "```py\nfrom llama_index.core.memory import ChatMemoryBuffer\nfrom llama_index.core.chat_engine import CondensePlusContextChatEngine\n\n# creating chat memory buffer\nmemory = ChatMemoryBuffer.from_defaults(token_limit=4500)\n\n# creating chat engine\nchat_engine = CondensePlusContextChatEngine.from_defaults(    \n   index.as_retriever(),    \n   memory=memory,    \n   llm=llm\n)\n\n# generating chat response\nresponse = chat_engine.chat(    \n   \"What is the one best course for mastering Reinforcement Learning?\"\n)\nprint(str(response))\n```", "```py\nresponse = chat_engine.chat(\n    \"Tell me more about the course\"\n)\nprint(str(response))\n```"]