["```py\n!pip install diffusers transformers scipy ftfy peft accelerate -q\n```", "```py\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\npipe = DiffusionPipeline.from_pretrained(pipe_id, torch_dtype=torch.float16)\npipe.to(\"cuda\");\n```", "```py\nprompt = \"photo of young woman, sitting outside restaurant, color, wearing dress, \" \\\n    \"rim lighting, studio lighting, looking at the camera, up close, perfect eyes\"\n\nimage = pipe(prompt).images[0]\n\nimage\n```", "```py\nprompt = \"Astronaut in space, realistic, detailed, 8k\"\nneg_prompt = \"ugly, deformed, disfigured, poor details, bad anatomy\"\ngenerator = torch.Generator(\"cuda\").manual_seed(127)\n\nimage = pipe(\n    prompt,\n    num_inference_steps=50,\n    generator=generator,\n    negative_prompt=neg_prompt,\n    height=512,\n    width=912,\n    guidance_scale=6,\n).images[0]\n\nimage\n```", "```py\npipe.load_lora_weights(\n    \"ostris/ikea-instructions-lora-sdxl\",\n    weight_name=\"ikea_instructions_xl_v1_5.safetensors\",\n    adapter_name=\"ikea\",\n)\n```", "```py\nprompt = \"super villan\"\n\nimage = pipe(\n    prompt,\n    num_inference_steps=30,\n    cross_attention_kwargs={\"scale\": 0.9},\n    generator=torch.manual_seed(125),\n).images[0]\n\nimage\n```", "```py\n!pip install controlnet_aux -q\n```", "```py\nfrom diffusers import ControlNetModel, AutoPipelineForText2Image\nfrom diffusers.utils import load_image\nimport torch\n\ncontrolnet = ControlNetModel.from_pretrained(\n    \"lllyasviel/control_v11p_sd15_OpenPose\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\n\noriginal_image = load_image(\n\"https://images.pexels.com/photos/1701194/pexels-photo-1701194.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=2\"\n)\n```", "```py\nfrom PIL import Image\n\ndef image_grid(imgs, rows, cols, resize=256):\n    assert len(imgs) == rows * cols\n\n    if resize is not None:\n        imgs = [img.resize((resize, resize)) for img in imgs]\n    w, h = imgs[0].size\n    grid_w, grid_h = cols * w, rows * h\n    grid = Image.new(\"RGB\", size=(grid_w, grid_h))\n\n    for i, img in enumerate(imgs):\n        x = i % cols * w\n        y = i // cols * h\n        grid.paste(img, box=(x, y))\n    return grid\n```", "```py\nfrom controlnet_aux import OpenPoseDetector\n\nmodel = OpenPoseDetector.from_pretrained(\"lllyasviel/ControlNet\")\npose_image = model(original_image)\n\nimage_grid([original_image,pose_image], 1, 2)\n```", "```py\ncontrolnet_pipe = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\n```", "```py\nprompt = \"a woman dancing in the rain, masterpiece, best quality, enchanting, \" \\\n         \"striking, beach background\"\nneg_prompt = \"worst quality, low quality, lowres, monochrome, greyscale, \" \\\n             \"multiple views, comic, sketch, bad anatomy, deformed, disfigured, \" \\\n             \"watermark, multiple_views, mutation hands, watermark, bad facial\"\n\nimage = controlnet_pipe(\n    prompt,\n    negative_prompt=neg_prompt,\n    num_images_per_prompt = 4,\n    image=pose_image,\n).images\nimage_grid(image, 1, 4)\n```"]