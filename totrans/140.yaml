- en: Creating a Training Loop for PyTorch Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为 PyTorch 模型创建训练循环
- en: 原文：[https://machinelearningmastery.com/creating-a-training-loop-for-pytorch-models/](https://machinelearningmastery.com/creating-a-training-loop-for-pytorch-models/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/creating-a-training-loop-for-pytorch-models/](https://machinelearningmastery.com/creating-a-training-loop-for-pytorch-models/)
- en: PyTorch provides a lot of building blocks for a deep learning model, but a training
    loop is not part of them. It is a flexibility that allows you to do whatever you
    want during training, but some basic structure is universal across most use cases.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 提供了许多深度学习模型的构建模块，但训练循环并不包括在其中。这种灵活性允许你在训练过程中做任何你想做的事情，但某些基本结构在大多数使用场景中是通用的。
- en: 'In this post, you will see how to make a training loop that provides essential
    information for your model training, with the option to allow any information
    to be displayed. After completing this post, you will know:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，你将看到如何创建一个训练循环，为你的模型训练提供必要的信息，并可以选择显示任何信息。完成本文后，你将了解：
- en: The basic building block of a training loop
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练循环的基本构建块
- en: How to use tqdm to display training progress
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 tqdm 显示训练进度
- en: '**Kick-start your project** with my book [Deep Learning with PyTorch](https://machinelearningmastery.com/deep-learning-with-pytorch/).
    It provides **self-study tutorials** with **working code**.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**用我的书 [Deep Learning with PyTorch](https://machinelearningmastery.com/deep-learning-with-pytorch/)
    启动你的项目**。它提供了 **自学教程** 和 **实用代码**。'
- en: Let’s get started.![](../Images/0672407873f4aff7bd5553961f40e664.png)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。![](../Images/0672407873f4aff7bd5553961f40e664.png)
- en: Creating a training loop for PyTorch models
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为 PyTorch 模型创建训练循环
- en: Photo by [pat pat](https://unsplash.com/photos/4DE9h3fpLiI). Some rights reserved.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [pat pat](https://unsplash.com/photos/4DE9h3fpLiI) 提供。版权所有。
- en: Overview
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'This post is in three parts; they are:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文分为三部分，分别是：
- en: Elements of Training a Deep Learning Model
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型的训练要素
- en: Collecting Statistics During Training
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练期间收集统计数据
- en: Using tqdm to Report the Training Progress
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 tqdm 报告训练进度
- en: Elements of Training a Deep Learning Model
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习模型的训练要素
- en: As with all machine learning models, the model design specifies the algorithm
    to manipulate an input and produce an output. But in the model, there are parameters
    that you need to fine-tune to achieve that. These model parameters are also called
    the weights, biases, kernels, or other names depending on the particular model
    and layers. Training is to feed in the sample data to the model so that an optimizer
    can fine-tune these parameters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有机器学习模型一样，模型设计指定了操作输入并生成输出的算法。但在模型中，有些参数需要调整以实现这一目标。这些模型参数也被称为权重、偏差、内核或其他名称，具体取决于特定模型和层。训练是将样本数据输入模型，以便优化器可以调整这些参数。
- en: 'When you train a model, you usually start with a dataset. Each dataset is a
    fairly large number of data samples. When you get a dataset, it is recommended
    to split it into two portions: the training set and the test set. The training
    set is further split into batches and used in the training loop to drive the gradient
    descent algorithms. The test set, however, is used as a benchmark to tell how
    good your model is. Usually, you do not use the training set as a metric but take
    the test set, which is not seen by the gradient descent algorithm, so you can
    tell if your model fits well to the unseen data.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当你训练一个模型时，你通常从一个数据集开始。每个数据集包含大量的数据样本。当你获得数据集时，建议将其分为两个部分：训练集和测试集。训练集进一步分为批次，并在训练循环中使用，以驱动梯度下降算法。然而，测试集用作基准，以判断你的模型表现如何。通常，你不会将训练集作为度量，而是使用测试集，因为测试集没有被梯度下降算法看到，从而可以判断你的模型是否对未见过的数据适应良好。
- en: Overfitting is when the model fits too well to the training set (i.e., at very
    high accuracy) but performs significantly worse in the test set. Underfitting
    is when the model cannot even fit well to the training set. Naturally, you don’t
    want to see either on a good model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是指模型在训练集上表现得过于好（即，非常高的准确率），但在测试集上的表现显著下降。欠拟合是指模型甚至无法在训练集上表现良好。自然，你不希望在一个好的模型中看到这两种情况。
- en: Training of a neural network model is in epochs. Usually, one epoch means you
    run through the entire training set once, although you only feed one batch at
    a time. It is also customary to do some housekeeping tasks at the end of each
    epoch, such as benchmarking the partially trained model with the test set, checkpointing
    the model, deciding if you want to stop the training early, and collecting training
    statistics, and so on.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络模型的训练是以周期为单位的。通常，一个周期意味着你遍历整个训练集一次，尽管你一次只输入一个批次。在每个周期结束时，通常会做一些例行任务，如使用测试集对部分训练好的模型进行基准测试、检查点模型、决定是否提前停止训练、收集训练统计数据等。
- en: In each epoch, you feed data samples into the model in batches and run a gradient
    descent algorithm. This is one step in the training loop because you run the model
    in one forward pass (i.e., providing input and capturing output), and one backward
    pass (evaluating the loss metric from the output and deriving the gradient of
    each parameter all the way back to the input layer). The backward pass computes
    the gradient using automatic differentiation. Then, this gradient is used by the
    gradient descent algorithm to adjust the model parameters. There are multiple
    steps in one epoch.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个周期中，你将数据样本以批次的形式输入模型，并运行梯度下降算法。这是训练循环中的一步，因为你在一次前向传递（即，提供输入并捕获输出）和一次反向传递（从输出评估损失指标并将每个参数的梯度反向到输入层）中运行模型。反向传递使用自动微分来计算梯度。然后，这些梯度由梯度下降算法用于调整模型参数。一个周期包含多个步骤。
- en: 'Reusing the examples in a [previous tutorial](https://machinelearningmastery.com/develop-your-first-neural-network-with-pytorch-step-by-step/),
    you can download the [dataset](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv)
    and split the dataset into two as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 复用[之前教程](https://machinelearningmastery.com/develop-your-first-neural-network-with-pytorch-step-by-step/)中的示例，你可以下载[数据集](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv)并将数据集拆分为两部分，如下所示：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This dataset is small–only 768 samples. Here, it takes the first 700 as the
    training set and the rest as the test set.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集很小——只有768个样本。在这里，它将前700个样本作为训练集，其余的作为测试集。
- en: 'It is not the focus of this post, but you can reuse the model, the loss function,
    and the optimizer from a previous post:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是本文的重点，但你可以复用[之前文章](https://machinelearningmastery.com/develop-your-first-neural-network-with-pytorch-step-by-step/)中的模型、损失函数和优化器：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With the data and the model, this is the minimal training loop, with the forward
    and backward pass in each step:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有了数据和模型，这就是最简训练循环，每一步都有前向和反向传递：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the inner for-loop, you take each batch in the dataset and evaluate the loss.
    The loss is a PyTorch tensor that remembers how it comes up with its value. Then
    you zero out all gradients that the optimizer manages and call `loss.backward()`
    to run the backpropagation algorithm. The result sets up the gradients of all
    the tensors that the tensor `loss` depends on directly and indirectly. Afterward,
    upon calling `step()`, the optimizer will check each parameter that it manages
    and update them.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部的for循环中，你取数据集中的每一个批次并评估损失。损失是一个PyTorch张量，它记住了如何得出其值。然后你将优化器管理的所有梯度清零，并调用`loss.backward()`来运行反向传播算法。结果设置了所有张量的梯度，这些张量直接或间接地依赖于张量`loss`。随后，调用`step()`时，优化器将检查其管理的每个参数并更新它们。
- en: 'After everything is done, you can run the model with the test set to evaluate
    its performance. The evaluation can be based on a different function than the
    loss function. For example, this classification problem uses accuracy:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 完成所有步骤后，你可以使用测试集运行模型以评估其性能。评估可以基于不同于损失函数的函数。例如，这个分类问题使用准确率：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Putting everything together, this is the complete code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 将一切整合在一起，这就是完整代码：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Collecting Statistics During Training
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练期间收集统计数据
- en: The training loop above should work well with small models that can finish training
    in a few seconds. But for a larger model or a larger dataset, you will find that
    it takes significantly longer to train. While you’re waiting for the training
    to complete, you may want to see how it’s going as you may want to interrupt the
    training if any mistake is made.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 上述训练循环应该适用于可以在几秒钟内完成训练的小模型。但对于较大的模型或较大的数据集，你会发现训练所需的时间显著增加。在等待训练完成的同时，你可能希望查看进度，以便在出现任何错误时中断训练。
- en: 'Usually, during training, you would like to see the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在训练过程中，你希望看到以下内容：
- en: In each step, you would like to know the loss metrics, and you are expecting
    the loss to go down
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每一步中，你想要知道损失指标，并期望损失降低。
- en: In each step, you would like to know other metrics, such as accuracy on the
    training set, that are of interest but not involved in the gradient descent
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每一步中，你想要了解其他指标，例如训练集上的准确率，这些指标是感兴趣的但不参与梯度下降。
- en: At the end of each epoch, you would like to evaluate the partially-trained model
    with the test set and report the evaluation metric
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个epoch结束时，你想要用测试集评估部分训练的模型并报告评估指标。
- en: At the end of the training, you would like to be above to visualize the above
    metrics
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练结束时，你希望能够可视化以上指标。
- en: 'These all are possible, but you need to add more code into the training loop,
    as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是可能的，但是你需要在训练循环中添加更多代码，如下所示：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you collect the loss and accuracy in the list, you can plot them using matplotlib.
    But be careful that you collected training set statistics at each step, but the
    test set accuracy only at the end of the epoch. Thus you would like to show the
    **average accuracy** from the training loop in each epoch, so they are comparable
    to each other.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当你收集损失和准确率到列表中时，你可以使用matplotlib将它们绘制出来。但要小心，你在每一步收集了训练集的统计数据，但测试集的准确率只在每个epoch结束时。因此，你希望在每个epoch中显示训练循环中的平均准确率，以便它们可以相互比较。
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Putting everything together, below is the complete code:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起，以下是完整的代码：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The story does not end here. Indeed, you can add more code to the training loop,
    especially in dealing with a more complex model. One example is checkpointing.
    You may want to save your model (e.g., using pickle) so that, if for any reason,
    your program stops, you can restart the training loop from the middle. Another
    example is early stopping, which lets you monitor the accuracy you obtained with
    the test set at the end of each epoch and interrupt the training if you don’t
    see the model improving for a while. This is because you probably can’t go further,
    given the design of the model, and you do not want to overfit.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 故事还没有结束。事实上，你可以在训练循环中添加更多代码，特别是在处理更复杂的模型时。一个例子是检查点。你可能想要保存你的模型（例如使用pickle），这样，如果出于任何原因你的程序停止，你可以从中间重新启动训练循环。另一个例子是早停，它允许你在每个epoch结束时监视测试集的准确率，并在一段时间内看不到模型改进时中断训练。这是因为你可能不能进一步进行，考虑到模型的设计，而且你不想过拟合。
- en: Want to Get Started With Deep Learning with PyTorch?
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 想要开始使用PyTorch进行深度学习吗？
- en: Take my free email crash course now (with sample code).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在就参加我的免费电子邮件快速课程（附有示例代码）。
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 点击注册并获取课程的免费PDF电子书版本。
- en: Using tqdm to Report the Training Progress
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用tqdm报告训练进度
- en: 'If you run the above code, you will find that there are a lot of lines printed
    on the screen while the training loop is running. Your screen may be cluttered.
    And you may also want to see an animated progress bar to better tell you how far
    you are in the training progress. The library `tqdm` is the popular tool for creating
    the progress bar. Converting the above code to use tqdm cannot be easier:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行以上代码，你会发现在训练循环运行时屏幕上打印了很多行。你的屏幕可能会很杂乱。而且你可能还想看到一个动画进度条，以更好地告诉你训练进度到了哪一步。`tqdm`库是创建进度条的流行工具。将以上代码转换为使用tqdm可以更加简单：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The usage of `tqdm` creates an iterator using `trange()` just like Python’s
    `range()` function, and you can read the number in a loop. You can access the
    progress bar by updating its description or “postfix” data, but you have to do
    that before it exhausts its content. The `set_postfix()` function is powerful
    as it can show you anything.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`tqdm`创建一个迭代器，使用`trange()`就像Python的`range()`函数一样，并且你可以在循环中读取数字。你可以通过更新其描述或“后缀”数据访问进度条，但你必须在其内容耗尽之前这样做。`set_postfix()`函数非常强大，因为它可以显示任何内容。
- en: 'In fact, there is a `tqdm()` function besides `trange()` that iterates over
    an existing list. You may find it easier to use, and you can rewrite the above
    loop as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，除了`trange()`之外还有一个`tqdm()`函数，它迭代现有列表。你可能会发现它更容易使用，并且你可以重写以上循环如下：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following is the complete code (without the matplotlib plotting):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完整的代码（不包括matplotlib绘图）：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Summary
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this post, you looked in detail at how to properly set up a training loop
    for a PyTorch model. In particular, you saw:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，你详细了解了如何为PyTorch模型正确设置训练循环。具体来说，你看到了：
- en: What are the elements needed to implement in a training loop
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现训练循环所需的元素是什么。
- en: How a training loop connects the training data to the gradient descent optimizer
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练循环如何将训练数据与梯度下降优化器连接起来
- en: How to collect information in the training loop and display them
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在训练循环中收集信息并展示这些信息
