- en: Mini-Batch Gradient Descent and DataLoader in PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch中的小批量梯度下降和DataLoader
- en: 原文：[https://machinelearningmastery.com/mini-batch-gradient-descent-and-dataloader-in-pytorch/](https://machinelearningmastery.com/mini-batch-gradient-descent-and-dataloader-in-pytorch/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/mini-batch-gradient-descent-and-dataloader-in-pytorch/](https://machinelearningmastery.com/mini-batch-gradient-descent-and-dataloader-in-pytorch/)
- en: Mini-batch gradient descent is a variant of gradient descent algorithm that
    is commonly used to train deep learning models. The idea behind this algorithm
    is to divide the training data into batches, which are then processed sequentially.
    In each iteration, we update the weights of all the training samples belonging
    to a particular batch together. This process is repeated with different batches
    until the whole training data has been processed. Compared to batch gradient descent,
    the main benefit of this approach is that it can reduce computation time and memory
    usage significantly as compared to processing all training samples in one shot.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量梯度下降是一种用于训练深度学习模型的梯度下降算法变体。该算法的核心思想是将训练数据分成批次，然后逐批次进行处理。在每次迭代中，我们同时更新属于特定批次的所有训练样本的权重。这个过程在不同的批次上重复，直到整个训练数据集被处理完毕。与批量梯度下降相比，这种方法的主要优势在于它可以显著减少计算时间和内存使用，因为它不是一次性处理所有训练样本。
- en: '`DataLoader` is a module in PyTorch that loads and preprocesses data for deep
    learning models. It can be used to load the data from a file, or to generate synthetic
    data.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataLoader`是PyTorch中加载和预处理数据的模块。它可用于从文件加载数据，或生成合成数据。'
- en: 'In this tutorial, we will introduce you to the concept of mini-batch gradient
    descent. You will also get to know how to implement it with PyTorch `DataLoader`.
    Particularly, we’ll cover:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将向您介绍小批量梯度下降的概念。您还将了解如何使用PyTorch的`DataLoader`来实现它。具体来说，我们将涵盖以下内容：
- en: Implementation of Mini-Batch Gradient Descent in PyTorch.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在PyTorch中实现小批量梯度下降。
- en: The concept of DataLoader in PyTorch and how we can load the data with it.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch中DataLoader的概念以及如何使用它加载数据。
- en: The difference between Stochastic Gradient Descent and Mini-Batch Gradient Descent.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机梯度下降与小批量梯度下降的区别。
- en: How to implement Stochastic Gradient Descent with PyTorch DataLoader.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用PyTorch DataLoader实现随机梯度下降。
- en: How to implement Mini-Batch Gradient Descent with PyTorch DataLoader.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用PyTorch DataLoader实现小批量梯度下降。
- en: '**Kick-start your project** with my book [Deep Learning with PyTorch](https://machinelearningmastery.com/deep-learning-with-pytorch/).
    It provides **self-study tutorials** with **working code**.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过我的书籍[《PyTorch深度学习》](https://machinelearningmastery.com/deep-learning-with-pytorch/)来启动您的项目**。它提供了带有**工作代码**的**自学教程**。'
- en: Let’s get started.![](../Images/3f2641e03ddc35049caa853f237fb63f.png)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！[](../Images/3f2641e03ddc35049caa853f237fb63f.png)
- en: Mini-Batch Gradient Descent and DataLoader in PyTorch.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch中的小批量梯度下降和DataLoader。
- en: Picture by [Yannis Papanastasopoulos](https://unsplash.com/photos/kKzbyDeb62M).
    Some rights reserved.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Yannis Papanastasopoulos](https://unsplash.com/photos/kKzbyDeb62M)拍摄。部分权利保留。
- en: Overview
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: This tutorial is in six parts; they are
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程分为六个部分，它们分别是：
- en: DataLoader in PyTorch
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch中的DataLoader
- en: Preparing Data and the Linear Regression Model
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据和线性回归模型
- en: Build Dataset and DataLoader Class
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建数据集和DataLoader类
- en: Training with Stochastic Gradient Descent and DataLoader
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机梯度下降和DataLoader进行训练
- en: Training with Mini-Batch Gradient Descent and DataLoader
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用小批量梯度下降和DataLoader进行训练
- en: Plotting Graphs for Comparison
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制图表进行比较
- en: DataLoader in PyTorch
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch中的DataLoader
- en: It all starts with loading the data when you plan to build a deep learning pipeline
    to train a model. The more complex the data, the more difficult it becomes to
    load it into the pipeline. PyTorch `DataLoader` is a handy tool offering numerous
    options not only to load the data easily, but also helps to apply data augmentation
    strategies, and iterate over samples in larger datasets. You can import `DataLoader`
    class from `torch.utils.data`, as follows.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当您计划构建深度学习管道来训练模型时，一切都始于数据加载。数据越复杂，加载到管道中就越困难。PyTorch的`DataLoader`是一个方便的工具，不仅能够轻松加载数据，还可以帮助应用数据增强策略，并在较大数据集中迭代样本。您可以从`torch.utils.data`中导入`DataLoader`类，如下所示。
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: There are several parameters in the `DataLoader` class, we’ll only discuss about
    `dataset` and `batch_size`. The `dataset` is the first parameter you’ll find in
    the `DataLoader` class and it loads your data into the pipeline. The second parameter
    is the `batch_size` which indicates the number of training examples processed
    in one iteration.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataLoader`类中有几个参数，我们只讨论`dataset`和`batch_size`。`dataset`是你在`DataLoader`类中找到的第一个参数，它将数据加载到管道中。第二个参数是`batch_size`，表示在一次迭代中处理的训练样本数。'
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Preparing Data and the Linear Regression Model
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据和线性回归模型
- en: 'Let’s reuse the same linear regression data as we produced in the previous
    tutorial:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重用在之前教程中生成的线性回归数据：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Same as in the previous tutorial, we initialized a variable `X` with values
    ranging from $-5$ to $5$, and created a linear function with a slope of $-5$.
    Then, Gaussian noise is added to create the variable `Y`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前的教程一样，我们初始化了一个变量`X`，其值范围从$-5$到$5$，并创建了一个斜率为$-5$的线性函数。然后，加入高斯噪声以生成变量`Y`。
- en: 'We can plot the data using matplotlib to visualize the pattern:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用matplotlib绘制数据以可视化模式：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/c7d2910711d5a9cf82e04f012cd344f1.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7d2910711d5a9cf82e04f012cd344f1.png)'
- en: Data points for regression model
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型的数据点
- en: 'Next, we’ll build a forward function based on a simple linear regression equation.
    We’ll train the model for two parameters ($w$ and $b$). So, let’s define a function
    for the forward pass of the model as well as a loss criterion function (MSE loss).
    The parameter variables `w` and `b` will be defined outside of the function:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将基于简单的线性回归方程构建一个前向函数。我们将训练模型以获取两个参数（$w$和$b$）。所以，让我们定义一个模型的前向传播函数以及一个损失标准函数（MSE损失）。参数变量`w`和`b`将定义在函数外部：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Want to Get Started With Deep Learning with PyTorch?
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 想开始使用PyTorch进行深度学习吗？
- en: Take my free email crash course now (with sample code).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 立即获取我的免费电子邮件速成课程（包含示例代码）。
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 点击注册并获取课程的免费PDF电子书版本。
- en: Build Dataset and DataLoader Class
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建数据集和`DataLoader`类
- en: Let’s build our `Dataset` and `DataLoader` classes. The `Dataset` class allows
    us to build custom datasets and apply various transforms on them. The `DataLoader`
    class, on the other hand, is used to load the datasets into the pipeline for model
    training. They are created as follows.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建我们的`Dataset`和`DataLoader`类。`Dataset`类允许我们构建自定义数据集并对其应用各种变换。`DataLoader`类则用于将数据集加载到模型训练的管道中。它们的创建方式如下。
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Training with Stochastic Gradient Descent and `DataLoader`
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用随机梯度下降和`DataLoader`进行训练
- en: When the batch size is set to one, the training algorithm is referred to as
    **stochastic gradient descent**. Likewise, when the batch size is greater than
    one but less than the size of the entire training data, the training algorithm
    is known as **mini-batch gradient descent**. For simplicity, let’s train with
    stochastic gradient descent and `DataLoader`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当批量大小设置为1时，训练算法称为**随机梯度下降**。类似地，当批量大小大于1但小于整个训练数据的大小时，训练算法称为**迷你批量梯度下降**。为了简便起见，我们将使用随机梯度下降和`DataLoader`进行训练。
- en: As before, we’ll randomly initialize the trainable parameters $w$ and $b$, define
    other parameters such as learning rate or step size, create an empty list to store
    the loss, and set the number of epochs of training.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如之前所示，我们将随机初始化可训练参数$w$和$b$，定义其他参数如学习率或步长，创建一个空列表来存储损失，并设置训练的轮数。
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In SGD, we just need to pick one sample from the dataset in each iteration
    of training. Hence a simple for loop with a forward and backward pass is all we
    needed:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在SGD中，我们只需在每次训练迭代中从数据集中选择一个样本。因此，一个简单的for循环加上前向和反向传播就是我们所需要的：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Putting everything together, the following is a complete code to train the
    model, namely, `w` and `b`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容结合起来，下面是训练模型的完整代码，即`w`和`b`：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Training with Mini-Batch Gradient Descent and `DataLoader`
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用迷你批量梯度下降和`DataLoader`进行训练
- en: 'Moving one step further, we’ll train our model with mini-batch gradient descent
    and `DataLoader`. We’ll set various batch sizes for training, i.e., batch sizes
    of 10 and 20. Training with batch size of 10 is as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 更进一步，我们将使用迷你批量梯度下降和`DataLoader`训练我们的模型。我们将设置不同的批量大小进行训练，即10和20。批量大小为10的训练如下：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And, here is how we’ll implement the same with batch size of 20:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何以20的批量大小实现相同功能：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Putting all together, the following is the complete code:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容结合起来，以下是完整的代码：
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Plotting Graphs for Comparison
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制比较图表
- en: Finally, let’s visualize how the loss decreases in all the three algorithms
    (i.e., stochastic gradient descent, mini-batch gradient descent with batch size
    of 10, and with batch size of 20) during training.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们可视化所有三种算法（即随机梯度下降、小批量梯度下降（批量大小为 10）和批量大小为 20）在训练期间损失的减少情况。
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/3cdffead736887fa03b6fcfd99f9086e.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cdffead736887fa03b6fcfd99f9086e.png)'
- en: As we can see from the plot, mini-batch gradient descent can converge faster
    because we can make more precise update to the parameters by calculating the average
    loss in each step.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看出，小批量梯度下降可能会更快收敛，因为我们可以通过计算每一步的平均损失来对参数进行更精确的更新。
- en: 'Putting all together, the following is the complete code:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 综合来看，以下是完整代码：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Summary
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this tutorial, you learned about mini-batch gradient descent, `DataLoader`,
    and their implementation in PyTorch. Particularly, you learned:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你了解了小批量梯度下降、`DataLoader`及其在 PyTorch 中的实现。特别是，你学到了：
- en: Implementation of mini-batch gradient descent in PyTorch.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 PyTorch 中实现小批量梯度下降。
- en: The concept of `DataLoader` in PyTorch and how we can load the data with it.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 中的 `DataLoader` 概念以及如何使用它加载数据。
- en: The difference between stochastic gradient descent and mini-batch gradient descent.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机梯度下降与小批量梯度下降之间的区别。
- en: How to implement stochastic gradient descent with PyTorch `DataLoader`.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 PyTorch `DataLoader` 实现随机梯度下降。
- en: How to implement mini-batch gradient descent with PyTorch `DataLoader`.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 PyTorch `DataLoader` 实现小批量梯度下降。
