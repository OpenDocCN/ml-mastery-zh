- en: 'Calculus in Action: Neural Networks'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微积分的应用：神经网络
- en: 原文：[https://machinelearningmastery.com/calculus-in-action-neural-networks/](https://machinelearningmastery.com/calculus-in-action-neural-networks/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/calculus-in-action-neural-networks/](https://machinelearningmastery.com/calculus-in-action-neural-networks/)
- en: An artificial neural network is a computational model that approximates a mapping
    between inputs and outputs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络是一个计算模型，用于逼近输入和输出之间的映射。
- en: It is inspired by the structure of the human brain, in that it is similarly
    composed of a network of interconnected neurons that propagate information upon
    receiving sets of stimuli from neighbouring neurons.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 它的灵感来自于人脑的结构，因为它类似地由一个互联的神经元网络组成，这些神经元在接收到来自邻近神经元的一组刺激后传播信息。
- en: Training a neural network involves a process that employs the backpropagation
    and gradient descent algorithms in tandem. As we will be seeing, both of these
    algorithms make extensive use of calculus.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络涉及一个过程，该过程同时使用反向传播和梯度下降算法。正如我们将看到的，这两个算法都广泛使用微积分。
- en: In this tutorial, you will discover how aspects of calculus are applied in neural
    networks.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你将发现微积分的各个方面如何应用于神经网络。
- en: 'After completing this tutorial, you will know:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本教程后，你将了解：
- en: An artificial neural network is organized into layers of neurons and connections,
    where the latter are attributed a weight value each.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经网络被组织成神经元和连接的层次结构，其中后者赋予每个权重值。
- en: Each neuron implements a nonlinear function that maps a set of inputs to an
    output activation.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个神经元实现一个非线性函数，将一组输入映射到一个输出激活。
- en: In training a neural network, calculus is used extensively by the backpropagation
    and gradient descent algorithms.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练神经网络时，反向传播和梯度下降算法广泛使用微积分。
- en: Let’s get started.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: '[![](../Images/c943918fb26ecdd5530552ea67e22bc1.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_cover-scaled.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/c943918fb26ecdd5530552ea67e22bc1.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_cover-scaled.jpg)'
- en: 'Calculus in Action: Neural Networks'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 微积分的应用：神经网络
- en: Photo by [Tomoe Steineck](https://unsplash.com/photos/T1Wru10gKhg), some rights
    reserved.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Tomoe Steineck](https://unsplash.com/photos/T1Wru10gKhg)提供，保留部分版权。
- en: '**Tutorial Overview**'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**教程概述**'
- en: 'This tutorial is divided into three parts; they are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程分为三个部分，它们是：
- en: An Introduction to the Neural Network
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络简介
- en: The Mathematics of a Neuron
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元的数学
- en: Training the Network
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练网络
- en: '**Prerequisites**'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**先决条件**'
- en: 'For this tutorial, we assume that you already know what are:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本教程，我们假设你已经知道以下内容：
- en: '[Function approximation](https://machinelearningmastery.com/neural-networks-are-function-approximators/)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[函数逼近](https://machinelearningmastery.com/neural-networks-are-function-approximators/)'
- en: '[Rate of change](https://machinelearningmastery.com/key-concepts-in-calculus-rate-of-change/)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[变化率](https://machinelearningmastery.com/key-concepts-in-calculus-rate-of-change/)'
- en: '[Partial derivatives](https://machinelearningmastery.com/a-gentle-introduction-to-partial-derivatives-and-gradient-vectors)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[偏导数](https://machinelearningmastery.com/a-gentle-introduction-to-partial-derivatives-and-gradient-vectors)'
- en: '[The chain rule](https://machinelearningmastery.com/?p=12720&preview=true)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[链式法则](https://machinelearningmastery.com/?p=12720&preview=true)'
- en: '[The chain rule on more functions](https://machinelearningmastery.com/?p=12732&preview=true)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[链式法则在更多函数中的应用](https://machinelearningmastery.com/?p=12732&preview=true)'
- en: '[Gradient descent](https://machinelearningmastery.com/a-gentle-introduction-to-gradient-descent-procedure/)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[梯度下降](https://machinelearningmastery.com/a-gentle-introduction-to-gradient-descent-procedure/)'
- en: You can review these concepts by clicking on the links given above.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过点击上面给出的链接来复习这些概念。
- en: '**An Introduction to the Neural Network**'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**神经网络简介**'
- en: Artificial neural networks can be considered as function approximation algorithms.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络可以被视为函数逼近算法。
- en: In a supervised learning setting, when presented with many input observations
    representing the problem of interest, together with their corresponding target
    outputs, the artificial neural network will seek to approximate the mapping that
    exists between the two.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习环境中，当提供多个输入观察值表示关注的问题，以及相应的目标输出时，人工神经网络将尝试逼近这两者之间存在的映射。
- en: '*A neural network is a computational model that is inspired by the structure
    of the human brain.*'
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*神经网络是一个计算模型，灵感来自于人脑的结构。*'
- en: ''
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – Page 65, [Deep Learning](https://www.amazon.com/Deep-Learning-Press-Essential-Knowledge/dp/0262537559/ref=sr_1_11?dchild=1&keywords=deep+learning&qid=1627991691&sr=8-11),
    2019.
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – 第65页，[深度学习](https://www.amazon.com/Deep-Learning-Press-Essential-Knowledge/dp/0262537559/ref=sr_1_11?dchild=1&keywords=deep+learning&qid=1627991691&sr=8-11)，2019年。
- en: 'The human brain consists of a massive network of interconnected neurons (around
    one hundred billion of them), with each comprising a cell body, a set of fibres
    called dendrites, and an axon:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 人脑由一个庞大的互联神经元网络组成（约有一百亿个神经元），每个神经元包括一个细胞体、一组称为树突的纤维和一个轴突：
- en: '[![](../Images/a36ac67380bb5a415df1ce97f20b7127.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_1.png)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/a36ac67380bb5a415df1ce97f20b7127.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_1.png)'
- en: A Neuron in the Human Brain
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 人脑中的神经元
- en: The dendrites act as the input channels to a neuron, whereas the axon acts as
    the output channel. Therefore, a neuron would receive input signals through its
    dendrites, which in turn would be connected to the (output) axons of other neighbouring
    neurons. In this manner, a sufficiently strong electrical pulse (also called an
    action potential) can be transmitted along the axon of one neuron, to all the
    other neurons that are connected to it. This permits signals to be propagated
    along the structure of the human brain.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 树突作为神经元的输入通道，而轴突则作为输出通道。因此，神经元通过其树突接收输入信号，这些树突又连接到其他邻近神经元的（输出）轴突。通过这种方式，一个足够强的电信号（也称为动作电位）可以沿着一个神经元的轴突传递到所有连接到它的其他神经元。这允许信号在大脑结构中传播。
- en: '*So, a neuron acts as an all-or-none switch, that takes in a set of inputs
    and either outputs an action potential or no output. *'
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*因此，神经元充当全或无的开关，接受一组输入并输出一个动作电位或没有输出。*'
- en: ''
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – Page 66, [Deep Learning](https://www.amazon.com/Deep-Learning-Press-Essential-Knowledge/dp/0262537559/ref=sr_1_11?dchild=1&keywords=deep+learning&qid=1627991691&sr=8-11),
    2019.
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – 第66页，[深度学习](https://www.amazon.com/Deep-Learning-Press-Essential-Knowledge/dp/0262537559/ref=sr_1_11?dchild=1&keywords=deep+learning&qid=1627991691&sr=8-11)，2019年。
- en: An artificial neural network is analogous to the structure of the human brain,
    because (1) it is similarly composed of a large number of interconnected neurons
    that, (2) seek to propagate information across the network by, (3) receiving sets
    of stimuli from neighbouring neurons and mapping these to outputs, to be fed to
    the next layer of neurons.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络类似于人脑的结构，因为（1）它由大量互联的神经元组成，（2）这些神经元通过（3）接收来自邻近神经元的一组刺激并将其映射到输出，从而在网络中传播信息，以便传递到下一层神经元。
- en: 'The structure of an artificial neural network is typically organised into layers
    of neurons ([recall](https://machinelearningmastery.com/?p=12720&preview=true)
    the depiction of a tree diagram). For example, the following diagram illustrates
    a fully-connected  neural network, where all the neurons in one layer are connected
    to all the neurons in the next layer:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络的结构通常组织成神经元的层级（[回顾](https://machinelearningmastery.com/?p=12720&preview=true)树状图的描述）。例如，以下图示例展示了一个完全连接的神经网络，其中一层中的所有神经元都连接到下一层的所有神经元：
- en: '[![](../Images/3e46e8325a0497372613d60f625f4839.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_21.png)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/3e46e8325a0497372613d60f625f4839.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_21.png)'
- en: A Fully-Connected, Feedforward Neural Network
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 完全连接的前馈神经网络
- en: The inputs are presented on the left hand side of the network, and the information
    propagates (or flows) rightward towards the outputs at the opposite end. Since
    the information is, hereby, propagating in the *forward* direction through the
    network, then we would also refer to such a network as a *feedforward neural network*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 输入位于网络的左侧，信息向右传播（或流动）到对侧的输出端。由于信息在网络中以*前馈*方向传播，因此我们也将这种网络称为*前馈神经网络*。
- en: The layers of neurons in between the input and output layers are called *hidden*
    layers, because they are not directly accessible.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层和输出层之间的神经元层称为*隐藏*层，因为它们无法直接访问。
- en: Each connection (represented by an arrow in the diagram) between two neurons
    is attributed a weight, which acts on the data flowing through the network, as
    we will see shortly.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 每两个神经元之间的连接（在图中由箭头表示）被赋予一个权重，该权重作用于通过网络的数据，正如我们稍后将看到的。
- en: Want to Get Started With Calculus for Machine Learning?
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 想要开始学习机器学习中的微积分吗？
- en: Take my free 7-day email crash course now (with sample code).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在就获取我的免费7天邮件速成课程（包括示例代码）。
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 点击注册并获得课程的免费PDF电子书版本。
- en: '**The Mathematics of a Neuron**'
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**神经元的数学**'
- en: More specifically, let’s say that a particular artificial neuron (or a *perceptron*,
    as Frank Rosenblatt had initially named it) receives *n* inputs, [*x*[1], …, *x*[n]],
    where each connection is attributed a corresponding weight, [*w*[1], …, *w*[n]].
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，假设一个特定的人工神经元（或弗兰克·罗森布拉特最初称之为*感知器*）接收 *n* 个输入，[*x*[1], …, *x*[n]]，每个连接都有一个对应的权重[*w*[1],
    …, *w*[n]]。
- en: 'The first operation that is carried out multiplies the input values by their
    corresponding weight, and adds a bias term, *b*, to their sum, producing an output,
    *z*:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 执行的第一个操作是将输入值乘以其相应的权重，并将偏置项 *b* 加到它们的总和中，生成输出 *z*：
- en: '*z* = ((*x*[1] × *w*[1]) + (*x*[2] × *w*[2]) + … + (*x*[n] × *w*[n])) + *b*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*z* = ((*x*[1] × *w*[1]) + (*x*[2] × *w*[2]) + … + (*x*[n] × *w*[n])) + *b*'
- en: 'We can, alternatively, represent this operation in a more compact form as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个操作以更紧凑的形式表示如下：
- en: '[![](../Images/c5772f48da8251cfc7a2e3fcac969a0f.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_5.png)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/c5772f48da8251cfc7a2e3fcac969a0f.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_5.png)'
- en: This weighted sum calculation that we have performed so far is a linear operation.
    If every neuron had to implement this particular calculation alone, then the neural
    network would be restricted to learning only linear input-output mappings.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们执行的加权和计算是一个线性操作。如果每个神经元必须单独实现这种特定的计算，那么神经网络将被限制于仅学习线性输入输出映射。
- en: '*However, many of the relationships in the world that we might want to model
    are nonlinear, and if we attempt to model these relationships using a linear model,
    then the model will be very inaccurate. *'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*然而，我们可能希望建模的许多世界中的关系是非线性的，如果我们尝试使用线性模型来建模这些关系，那么模型将非常不准确。*'
- en: ''
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – Page 77, [Deep Learning](https://www.amazon.com/Deep-Learning-Press-Essential-Knowledge/dp/0262537559/ref=sr_1_11?dchild=1&keywords=deep+learning&qid=1627991691&sr=8-11),
    2019.
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – 第77页，[深度学习](https://www.amazon.com/Deep-Learning-Press-Essential-Knowledge/dp/0262537559/ref=sr_1_11?dchild=1&keywords=deep+learning&qid=1627991691&sr=8-11)，2019年。
- en: 'Hence, a second operation is performed by each neuron that transforms the weighted
    sum by the application of a nonlinear activation function, *a*(.):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个神经元执行第二个操作，通过应用非线性激活函数 *a*(.) 转换加权和：
- en: '[![](../Images/6210c55c3400231a53a7cd6efdfce77e.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_6.png)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/6210c55c3400231a53a7cd6efdfce77e.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_6.png)'
- en: 'We can represent the operations performed by each neuron even more compactly,
    if we had to integrate the bias term into the sum as another weight, *w*[0] (notice
    that the sum now starts from 0):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将偏置项作为另一个权重* w *[0]（注意总和现在从0开始）集成到和中，我们可以更紧凑地表示每个神经元执行的操作：
- en: '[![](../Images/13c58fc2adb48e1b7e859205b5c3b076.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_7.png)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/13c58fc2adb48e1b7e859205b5c3b076.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_7.png)'
- en: 'The operations performed by each neuron can be illustrated as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元执行的操作可以如下所示：
- en: '[![](../Images/cfd7af5a999b605f09c16c0f5e062e37.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_3.png)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/cfd7af5a999b605f09c16c0f5e062e37.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_3.png)'
- en: Nonlinear Function Implemented by a Neuron
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元实现的非线性函数
- en: Therefore, each neuron can be considered to implement a nonlinear function that
    maps a set of inputs to an output activation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个神经元可以被视为实现一个将输入集映射到输出激活的非线性函数。
- en: '**Training the Network**'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**训练网络**'
- en: Training an artificial neural network involves the process of searching for
    the set of weights that model best the patterns in the data. It is a process that
    employs the backpropagation and gradient descent algorithms in tandem. Both of
    these algorithms make extensive use of calculus.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 训练人工神经网络涉及寻找最佳建模数据模式的权重集的过程。这是一个同时使用反向传播和梯度下降算法的过程。这两种算法都大量使用微积分。
- en: Each time that the network is traversed in the forward (or rightward) direction,
    the error of the network can be calculated as the difference between the output
    produced by the network and the expected ground truth, by means of a loss function
    (such as the sum of squared errors (SSE)). The backpropagation algorithm, then,
    calculates the gradient (or the rate of change) of this error to changes in the
    weights. In order to do so, it requires the use of the chain rule and partial
    derivatives.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 每当网络向前（或向右）方向遍历时，可以通过损失函数（如平方误差的总和（SSE））计算网络的误差，即网络输出与预期目标之间的差异。然后，反向传播算法计算此误差对权重变化的梯度（或变化率）。为了做到这一点，它需要使用链式法则和偏导数。
- en: 'For simplicity, consider a network made up of two neurons connected by a single
    path of activation. If we had to break them open, we would find that the neurons
    perform the following operations in cascade:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，考虑一个由单条激活路径连接的两个神经元组成的网络。如果我们需要打开它们，我们会发现神经元按照以下级联操作进行：
- en: '[![](../Images/63c0a794e2b450482a8396ab72e09c83.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_4.png)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/63c0a794e2b450482a8396ab72e09c83.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_4.png)'
- en: Operations Performed by Two Neurons in Cascade
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 两个神经元级联执行的操作
- en: 'The first application of the chain rule connects the overall error of the network
    to the input, *z*[2], of the activation function *a*[2] of the second neuron,
    and subsequently to the weight, *w*[2], as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则的第一个应用将网络的整体误差连接到激活函数*a*[2]的输入*z*[2]，随后连接到权重*w*[2]，如下所示：
- en: '[![](../Images/100a9c7564696760ab6d36aa245e3bcc.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_8.png)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/100a9c7564696760ab6d36aa245e3bcc.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_8.png)'
- en: 'You may notice that the application of the chain rule involves, among other
    terms, a multiplication by the partial derivative of the neuron’s activation function
    with respect to its input, *z*[2]. There are different activation functions to
    choose from, such as the sigmoid or the logistic functions. If we had to take
    the logistic function as an example, then its partial derivative would be computed
    as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到链式法则的应用涉及到神经元激活函数关于其输入*z*[2]的偏导数乘积，还有其他项。有不同的激活函数可供选择，例如sigmoid或logistic函数。如果我们以logistic函数为例，那么其偏导数将如下计算：
- en: '[![](../Images/344a5101574611b3c5098e2d74f99a3e.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_9.png)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/344a5101574611b3c5098e2d74f99a3e.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_9.png)'
- en: 'Hence, we can compute ????[2] as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以如下计算*t*[2]：
- en: '[![](../Images/5d6ad863ffab43e0cf7796e004150573.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_10.png)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/5d6ad863ffab43e0cf7796e004150573.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_10.png)'
- en: Here, *t*[2] is the expected activation, and in finding the difference between
    *t*[2] and *a*[2] we are, therefore, computing the error between the activation
    generated by the network and the expected ground truth.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*t*[2]是期望的激活，通过计算*t*[2]和*a*[2]之间的差异，我们因此计算了网络生成的激活与预期目标之间的误差。
- en: Since we are computing the derivative of the activation function, it should,
    therefore, be continuous and differentiable over the entire space of real numbers.
    In the case of deep neural networks, the error gradient is propagated backwards
    over a large number of hidden layers. This can cause the error signal to rapidly
    diminish to zero, especially if the maximum value of the derivative function is
    already small to begin with (for instance, the inverse of the logistic function
    has a maximum value of 0.25). This is known as the *vanishing gradient problem*.
    The ReLU function has been so popularly used in deep learning to alleviate this
    problem, because its derivative in the positive portion of its domain is equal
    to 1.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在计算激活函数的导数，因此它应在整个实数空间上是连续且可微的。在深度神经网络的情况下，误差梯度向后传播经过大量隐藏层。这可能导致误差信号迅速减少到零，尤其是如果导数函数的最大值已经很小（例如，logistic函数的倒数最大值为0.25）。这被称为*梯度消失问题*。ReLU函数在深度学习中非常流行，以减轻这个问题，因为其在其正部分的导数等于1。
- en: 'The next weight backwards is deeper into the network and, hence, the application
    of the chain rule can similarly be extended to connect the overall error to the
    weight, *w*[1], as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的权重反向传播到网络的深层，因此链式法则的应用也可以类似地扩展，以将整体误差与权重 *w*[1] 连接起来，如下所示：
- en: '[![](../Images/1a6c819c27eff3c1afa55aca30ff8469.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_11.png)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/1a6c819c27eff3c1afa55aca30ff8469.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_11.png)'
- en: 'If we take the logistic function again as the activation function of choice,
    then we would compute ????[1] as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再次以逻辑函数作为激活函数，那么我们将如下计算 ????[1]：
- en: '[![](../Images/8b87ee2770c239ec81e586aab65e2d5d.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_12.png)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/8b87ee2770c239ec81e586aab65e2d5d.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_12.png)'
- en: 'Once we have computed the gradient of the network error with respect to each
    weight, then the gradient descent algorithm can be applied to update each weight
    for the next *forward propagation* at time, *t*+1\. For the weight, *w*[1], the
    weight update rule using gradient descent would be specified as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们计算了网络误差相对于每个权重的梯度，就可以应用梯度下降算法来更新每个权重，以进行下一个时间点 *t*+1 的 *前向传播*。对于权重 *w*[1]，使用梯度下降的权重更新规则如下：
- en: '[![](../Images/2568402f23c0819e9e2e36f79d27c2c3.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_13.png)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/2568402f23c0819e9e2e36f79d27c2c3.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_13.png)'
- en: Even though we have hereby considered a simple network, the process that we
    have gone through can be extended to evaluate more complex and deeper ones, such
    convolutional neural networks (CNNs).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们这里考虑的是一个简单的网络，我们经历的过程也可以扩展到评估更复杂和更深的网络，例如卷积神经网络（CNNs）。
- en: If the network under consideration is characterised by multiple branches coming
    from multiple inputs (and possibly flowing towards multiple outputs), then its
    evaluation would involve the summation of different derivative chains for each
    path, similarly to how we have previously derived the generalized chain rule.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果考虑的网络具有来自多个输入的多个分支（并可能流向多个输出），则其评估将涉及对每条路径的不同导数链的求和，类似于我们之前推导的广义链式法则。
- en: '**Further Reading**'
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: This section provides more resources on the topic if you are looking to go deeper.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了更多关于这个主题的资源，如果你想深入了解。
- en: '**Books**'
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**书籍**'
- en: '[Deep Learning](https://www.amazon.com/Deep-Learning-Press-Essential-Knowledge/dp/0262537559/ref=sr_1_11?dchild=1&keywords=deep+learning&qid=1627991691&sr=8-11),
    2019.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习](https://www.amazon.com/Deep-Learning-Press-Essential-Knowledge/dp/0262537559/ref=sr_1_11?dchild=1&keywords=deep+learning&qid=1627991691&sr=8-11)，2019。'
- en: '[Pattern Recognition and Machine Learning](https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/1493938436/ref=sr_1_2?dchild=1&keywords=Pattern+Recognition+and+Machine+Learning&qid=1627991645&sr=8-2),
    2016.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[模式识别与机器学习](https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/1493938436/ref=sr_1_2?dchild=1&keywords=Pattern+Recognition+and+Machine+Learning&qid=1627991645&sr=8-2)，2016。'
- en: '**Summary**'
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this tutorial, you discovered how aspects of calculus are applied in neural
    networks.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你发现了微积分在神经网络中的应用。
- en: 'Specifically, you learned:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，你学到了：
- en: An artificial neural network is organized into layers of neurons and connections,
    where the latter are each attributed a weight value.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经网络被组织成由神经元和连接层组成，后者每个都分配一个权重值。
- en: Each neuron implements a nonlinear function that maps a set of inputs to an
    output activation.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个神经元实现一个非线性函数，将一组输入映射到输出激活值。
- en: In training a neural network, calculus is used extensively by the backpropagation
    and gradient descent algorithms.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练神经网络时，反向传播和梯度下降算法广泛使用微积分。
- en: Do you have any questions?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你有什么问题吗？
- en: Ask your questions in the comments below and I will do my best to answer.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的评论中提出你的问题，我会尽力回答。
