["```py\nfrom torch.utils.data import DataLoader\n```", "```py\nDataLoader(dataset, batch_size=n)\n```", "```py\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Creating a function f(X) with a slope of -5\nX = torch.arange(-5, 5, 0.1).view(-1, 1)\nfunc = -5 * X\n\n# Adding Gaussian noise to the function f(X) and saving it in Y\nY = func + 0.4 * torch.randn(X.size())\n```", "```py\n...\n# Plot and visualizing the data points in blue\nplt.plot(X.numpy(), Y.numpy(), 'b+', label='Y')\nplt.plot(X.numpy(), func.numpy(), 'r', label='func')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.grid('True', color='y')\nplt.show()\n```", "```py\n...\n# defining the function for forward pass for prediction\ndef forward(x):\n    return w * x + b\n\n# evaluating data points with Mean Square Error (MSE)\ndef criterion(y_pred, y):\n    return torch.mean((y_pred - y) ** 2)\n```", "```py\n# Creating our dataset class\nclass Build_Data(Dataset):    \n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-5, 5, 0.1).view(-1, 1)\n        self.y = -5 * X\n        self.len = self.x.shape[0]        \n    # Getting the data\n    def __getitem__(self, index):    \n        return self.x[index], self.y[index]    \n    # Getting length of the data\n    def __len__(self):\n        return self.len\n\n# Creating DataLoader object\ndataset = Build_Data()\ntrain_loader = DataLoader(dataset = dataset, batch_size = 1)\n```", "```py\nw = torch.tensor(-10.0, requires_grad = True)\nb = torch.tensor(-20.0, requires_grad = True)\n\nstep_size = 0.1\nloss_SGD = []\nn_iter = 20\n```", "```py\nfor i in range (n_iter):    \n    # calculating loss as in the beginning of an epoch and storing it\n    y_pred = forward(X)\n    loss_SGD.append(criterion(y_pred, Y).tolist())\n    for x, y in train_loader:\n        # making a prediction in forward pass\n        y_hat = forward(x)\n        # calculating the loss between original and predicted data points\n        loss = criterion(y_hat, y)    \n        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n        loss.backward()\n        # updating the parameters after each iteration\n        w.data = w.data - step_size * w.grad.data\n        b.data = b.data - step_size * b.grad.data    \n        # zeroing gradients after each iteration\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n```", "```py\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\ntorch.manual_seed(42)\n\n# Creating a function f(X) with a slope of -5\nX = torch.arange(-5, 5, 0.1).view(-1, 1)\nfunc = -5 * X\n# Adding Gaussian noise to the function f(X) and saving it in Y\nY = func + 0.4 * torch.randn(X.size())\n\nw = torch.tensor(-10.0, requires_grad = True)\nb = torch.tensor(-20.0, requires_grad = True)\n\n# defining the function for forward pass for prediction\ndef forward(x):\n    return w * x + b\n\n# evaluating data points with Mean Square Error (MSE)\ndef criterion(y_pred, y):\n    return torch.mean((y_pred - y) ** 2)\n\n# Creating our dataset class\nclass Build_Data(Dataset):\n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-5, 5, 0.1).view(-1, 1)\n        self.y = -5 * X\n        self.len = self.x.shape[0]\n    # Getting the data\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n    # Getting length of the data\n    def __len__(self):\n        return self.len\n\n# Creating DataLoader object\ndataset = Build_Data()\ntrain_loader = DataLoader(dataset=dataset, batch_size=1)\n\nstep_size = 0.1\nloss_SGD = []\nn_iter = 20\n\nfor i in range (n_iter):\n    # calculating loss as in the beginning of an epoch and storing it\n    y_pred = forward(X)\n    loss_SGD.append(criterion(y_pred, Y).tolist())\n    for x, y in train_loader:\n        # making a prediction in forward pass\n        y_hat = forward(x)\n        # calculating the loss between original and predicted data points\n        loss = criterion(y_hat, y)\n        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n        loss.backward()\n        # updating the parameters after each iteration\n        w.data = w.data - step_size * w.grad.data\n        b.data = b.data - step_size * b.grad.data\n        # zeroing gradients after each iteration\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n```", "```py\n...\ntrain_loader_10 = DataLoader(dataset=dataset, batch_size=10)\n\nw = torch.tensor(-10.0, requires_grad=True)\nb = torch.tensor(-20.0, requires_grad=True)\n\nstep_size = 0.1\nloss_MBGD_10 = []\niter = 20\n\nfor i in range (iter):    \n    # calculating loss as in the beginning of an epoch and storing it\n    y_pred = forward(X)\n    loss_MBGD_10.append(criterion(y_pred, Y).tolist())\n    for x, y in train_loader_10:\n        # making a prediction in forward pass\n        y_hat = forward(x)\n        # calculating the loss between original and predicted data points\n        loss = criterion(y_hat, y)\n        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n        loss.backward()\n        # updating the parameters after each iteration\n        w.data = w.data - step_size * w.grad.data\n        b.data = b.data - step_size * b.grad.data    \n        # zeroing gradients after each iteration\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n```", "```py\n...\ntrain_loader_20 = DataLoader(dataset=dataset, batch_size=20)\n\nw = torch.tensor(-10.0, requires_grad=True)\nb = torch.tensor(-20.0, requires_grad=True)\n\nstep_size = 0.1\nloss_MBGD_20 = []\niter = 20\n\nfor i in range(iter):    \n    # calculating loss as in the beginning of an epoch and storing it\n    y_pred = forward(X)\n    loss_MBGD_20.append(criterion(y_pred, Y).tolist())\n    for x, y in train_loader_20:\n        # making a prediction in forward pass\n        y_hat = forward(x)\n        # calculating the loss between original and predicted data points\n        loss = criterion(y_hat, y)    \n        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n        loss.backward()\n        # updating the parameters after each iteration\n        w.data = w.data - step_size * w.grad.data\n        b.data = b.data - step_size * b.grad.data    \n        # zeroing gradients after each iteration\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n```", "```py\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\ntorch.manual_seed(42)\n\n# Creating a function f(X) with a slope of -5\nX = torch.arange(-5, 5, 0.1).view(-1, 1)\nfunc = -5 * X\n# Adding Gaussian noise to the function f(X) and saving it in Y\nY = func + 0.4 * torch.randn(X.size())\n\nw = torch.tensor(-10.0, requires_grad=True)\nb = torch.tensor(-20.0, requires_grad=True)\n\n# defining the function for forward pass for prediction\ndef forward(x):\n    return w * x + b\n\n# evaluating data points with Mean Square Error (MSE)\ndef criterion(y_pred, y):\n    return torch.mean((y_pred - y) ** 2)\n\n# Creating our dataset class\nclass Build_Data(Dataset):\n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-5, 5, 0.1).view(-1, 1)\n        self.y = -5 * X\n        self.len = self.x.shape[0]\n    # Getting the data\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n    # Getting length of the data\n    def __len__(self):\n        return self.len\n\n# Creating DataLoader object\ndataset = Build_Data()\ntrain_loader_10 = DataLoader(dataset=dataset, batch_size=10)\n\nstep_size = 0.1\nloss_MBGD_10 = []\niter = 20\n\nfor i in range(n_iter):\n    # calculating loss as in the beginning of an epoch and storing it\n    y_pred = forward(X)\n    loss_MBGD_10.append(criterion(y_pred, Y).tolist())\n    for x, y in train_loader_10:\n        # making a prediction in forward pass\n        y_hat = forward(x)\n        # calculating the loss between original and predicted data points\n        loss = criterion(y_hat, y)\n        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n        loss.backward()\n        # updateing the parameters after each iteration\n        w.data = w.data - step_size * w.grad.data\n        b.data = b.data - step_size * b.grad.data\n        # zeroing gradients after each iteration\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n\ntrain_loader_20 = DataLoader(dataset=dataset, batch_size=20)\n\n# Reset w and b\nw = torch.tensor(-10.0, requires_grad=True)\nb = torch.tensor(-20.0, requires_grad=True)\n\nloss_MBGD_20 = []\n\nfor i in range(n_iter):\n    # calculating loss as in the beginning of an epoch and storing it\n    y_pred = forward(X)\n    loss_MBGD_20.append(criterion(y_pred, Y).tolist())\n    for x, y in train_loader_20:\n        # making a prediction in forward pass\n        y_hat = forward(x)\n        # calculating the loss between original and predicted data points\n        loss = criterion(y_hat, y)\n        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n        loss.backward()\n        # updating the parameters after each iteration\n        w.data = w.data - step_size * w.grad.data\n        b.data = b.data - step_size * b.grad.data\n        # zeroing gradients after each iteration\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n```", "```py\nplt.plot(loss_SGD,label = \"Stochastic Gradient Descent\")\nplt.plot(loss_MBGD_10,label = \"Mini-Batch-10 Gradient Descent\")\nplt.plot(loss_MBGD_20,label = \"Mini-Batch-20 Gradient Descent\")\nplt.xlabel('epoch')\nplt.ylabel('Cost/total loss')\nplt.legend()\nplt.show()\n```", "```py\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\ntorch.manual_seed(42)\n\n# Creating a function f(X) with a slope of -5\nX = torch.arange(-5, 5, 0.1).view(-1, 1)\nfunc = -5 * X\n# Adding Gaussian noise to the function f(X) and saving it in Y\nY = func + 0.4 * torch.randn(X.size())\n\nw = torch.tensor(-10.0, requires_grad=True)\nb = torch.tensor(-20.0, requires_grad=True)\n\n# defining the function for forward pass for prediction\ndef forward(x):\n    return w * x + b\n\n# evaluating data points with Mean Square Error (MSE)\ndef criterion(y_pred, y):\n    return torch.mean((y_pred - y) ** 2)\n\n# Creating our dataset class\nclass Build_Data(Dataset):\n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-5, 5, 0.1).view(-1, 1)\n        self.y = -5 * X\n        self.len = self.x.shape[0]\n    # Getting the data\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n    # Getting length of the data\n    def __len__(self):\n        return self.len\n\n# Creating DataLoader object\ndataset = Build_Data()\ntrain_loader = DataLoader(dataset=dataset, batch_size=1)\n\nstep_size = 0.1\nloss_SGD = []\nn_iter = 20\n\nfor i in range(n_iter):\n    # calculating loss as in the beginning of an epoch and storing it\n    y_pred = forward(X)\n    loss_SGD.append(criterion(y_pred, Y).tolist())\n    for x, y in train_loader:\n        # making a prediction in forward pass\n        y_hat = forward(x)\n        # calculating the loss between original and predicted data points\n        loss = criterion(y_hat, y)\n        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n        loss.backward()\n        # updating the parameters after each iteration\n        w.data = w.data - step_size * w.grad.data\n        b.data = b.data - step_size * b.grad.data\n        # zeroing gradients after each iteration\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n\ntrain_loader_10 = DataLoader(dataset=dataset, batch_size=10)\n\n# Reset w and b\nw = torch.tensor(-10.0, requires_grad=True)\nb = torch.tensor(-20.0, requires_grad=True)\n\nloss_MBGD_10 = []\n\nfor i in range(n_iter):\n    # calculating loss as in the beginning of an epoch and storing it\n    y_pred = forward(X)\n    loss_MBGD_10.append(criterion(y_pred, Y).tolist())\n    for x, y in train_loader_10:\n        # making a prediction in forward pass\n        y_hat = forward(x)\n        # calculating the loss between original and predicted data points\n        loss = criterion(y_hat, y)\n        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n        loss.backward()\n        # updating the parameters after each iteration\n        w.data = w.data - step_size * w.grad.data\n        b.data = b.data - step_size * b.grad.data\n        # zeroing gradients after each iteration\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n\ntrain_loader_20 = DataLoader(dataset=dataset, batch_size=20)\n\n# Reset w and b\nw = torch.tensor(-10.0, requires_grad=True)\nb = torch.tensor(-20.0, requires_grad=True)\n\nloss_MBGD_20 = []\n\nfor i in range(n_iter):\n    # calculating loss as in the beginning of an epoch and storing it\n    y_pred = forward(X)\n    loss_MBGD_20.append(criterion(y_pred, Y).tolist())\n    for x, y in train_loader_20:\n        # making a prediction in forward pass\n        y_hat = forward(x)\n        # calculating the loss between original and predicted data points\n        loss = criterion(y_hat, y)\n        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n        loss.backward()\n        # updating the parameters after each iteration\n        w.data = w.data - step_size * w.grad.data\n        b.data = b.data - step_size * b.grad.data\n        # zeroing gradients after each iteration\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n\nplt.plot(loss_SGD,label=\"Stochastic Gradient Descent\")\nplt.plot(loss_MBGD_10,label=\"Mini-Batch-10 Gradient Descent\")\nplt.plot(loss_MBGD_20,label=\"Mini-Batch-20 Gradient Descent\")\nplt.xlabel('epoch')\nplt.ylabel('Cost/total loss')\nplt.legend()\nplt.show()\n```"]