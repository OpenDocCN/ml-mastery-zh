["```py\nfrom sklearn.datasets import fetch_california_housing\n\ndata = fetch_california_housing()\nprint(data.feature_names)\n\nX, y = data.data, data.target\n```", "```py\nimport torch.nn as nn\n\n# Define the model\nmodel = nn.Sequential(\n    nn.Linear(8, 24),\n    nn.ReLU(),\n    nn.Linear(24, 12),\n    nn.ReLU(),\n    nn.Linear(12, 6),\n    nn.ReLU(),\n    nn.Linear(6, 1)\n)\n```", "```py\nimport torch.nn as nn\nimport torch.optim as optim\n\n# loss function and optimizer\nloss_fn = nn.MSELoss()  # mean square error\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n```", "```py\nimport copy\nimport numpy as np\nimport torch\nimport tqdm\nfrom sklearn.model_selection import train_test_split\n\n# train-test split of the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n\n# training parameters\nn_epochs = 100   # number of epochs to run\nbatch_size = 10  # size of each batch\nbatch_start = torch.arange(0, len(X_train), batch_size)\n\n# Hold the best model\nbest_mse = np.inf   # init to infinity\nbest_weights = None\nhistory = []\n\n# training loop\nfor epoch in range(n_epochs):\n    model.train()\n    with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n        bar.set_description(f\"Epoch {epoch}\")\n        for start in bar:\n            # take a batch\n            X_batch = X_train[start:start+batch_size]\n            y_batch = y_train[start:start+batch_size]\n            # forward pass\n            y_pred = model(X_batch)\n            loss = loss_fn(y_pred, y_batch)\n            # backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            # update weights\n            optimizer.step()\n            # print progress\n            bar.set_postfix(mse=float(loss))\n    # evaluate accuracy at end of each epoch\n    model.eval()\n    y_pred = model(X_test)\n    mse = loss_fn(y_pred, y_test)\n    mse = float(mse)\n    history.append(mse)\n    if mse < best_mse:\n        best_mse = mse\n        best_weights = copy.deepcopy(model.state_dict())\n\n# restore model and return best accuracy\nmodel.load_state_dict(best_weights)\n```", "```py\nprint(\"MSE: %.2f\" % best_mse)\nprint(\"RMSE: %.2f\" % np.sqrt(best_mse))\nplt.plot(history)\nplt.show()\n```", "```py\nMSE: 0.47\nRMSE: 0.68\n```", "```py\nimport copy\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_california_housing\n\n# Read data\ndata = fetch_california_housing()\nX, y = data.data, data.target\n\n# train-test split for model evaluation\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n\n# Convert to 2D PyTorch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n\n# Define the model\nmodel = nn.Sequential(\n    nn.Linear(8, 24),\n    nn.ReLU(),\n    nn.Linear(24, 12),\n    nn.ReLU(),\n    nn.Linear(12, 6),\n    nn.ReLU(),\n    nn.Linear(6, 1)\n)\n\n# loss function and optimizer\nloss_fn = nn.MSELoss()  # mean square error\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\nn_epochs = 100   # number of epochs to run\nbatch_size = 10  # size of each batch\nbatch_start = torch.arange(0, len(X_train), batch_size)\n\n# Hold the best model\nbest_mse = np.inf   # init to infinity\nbest_weights = None\nhistory = []\n\nfor epoch in range(n_epochs):\n    model.train()\n    with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n        bar.set_description(f\"Epoch {epoch}\")\n        for start in bar:\n            # take a batch\n            X_batch = X_train[start:start+batch_size]\n            y_batch = y_train[start:start+batch_size]\n            # forward pass\n            y_pred = model(X_batch)\n            loss = loss_fn(y_pred, y_batch)\n            # backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            # update weights\n            optimizer.step()\n            # print progress\n            bar.set_postfix(mse=float(loss))\n    # evaluate accuracy at end of each epoch\n    model.eval()\n    y_pred = model(X_test)\n    mse = loss_fn(y_pred, y_test)\n    mse = float(mse)\n    history.append(mse)\n    if mse < best_mse:\n        best_mse = mse\n        best_weights = copy.deepcopy(model.state_dict())\n\n# restore model and return best accuracy\nmodel.load_state_dict(best_weights)\nprint(\"MSE: %.2f\" % best_mse)\nprint(\"RMSE: %.2f\" % np.sqrt(best_mse))\nplt.plot(history)\nplt.show()\n```", "```py\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import StandardScaler\n\n# Read data\ndata = fetch_california_housing()\nX, y = data.data, data.target\n\n# train-test split for model evaluation\nX_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n\n# Standardizing data\nscaler = StandardScaler()\nscaler.fit(X_train_raw)\nX_train = scaler.transform(X_train_raw)\nX_test = scaler.transform(X_test_raw)\n\n# Convert to 2D PyTorch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n```", "```py\nMSE: 0.29\nRMSE: 0.54\n```", "```py\nmodel.eval()\nwith torch.no_grad():\n    # Test out inference with 5 samples from the original test set\n    for i in range(5):\n        X_sample = X_test_raw[i: i+1]\n        X_sample = scaler.transform(X_sample)\n        X_sample = torch.tensor(X_sample, dtype=torch.float32)\n        y_pred = model(X_sample)\n        print(f\"{X_test_raw[i]} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")\n```", "```py\nimport copy\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import StandardScaler\n\n# Read data\ndata = fetch_california_housing()\nX, y = data.data, data.target\n\n# train-test split for model evaluation\nX_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n\n# Standardizing data\nscaler = StandardScaler()\nscaler.fit(X_train_raw)\nX_train = scaler.transform(X_train_raw)\nX_test = scaler.transform(X_test_raw)\n\n# Convert to 2D PyTorch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n\n# Define the model\nmodel = nn.Sequential(\n    nn.Linear(8, 24),\n    nn.ReLU(),\n    nn.Linear(24, 12),\n    nn.ReLU(),\n    nn.Linear(12, 6),\n    nn.ReLU(),\n    nn.Linear(6, 1)\n)\n\n# loss function and optimizer\nloss_fn = nn.MSELoss()  # mean square error\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\nn_epochs = 100   # number of epochs to run\nbatch_size = 10  # size of each batch\nbatch_start = torch.arange(0, len(X_train), batch_size)\n\n# Hold the best model\nbest_mse = np.inf   # init to infinity\nbest_weights = None\nhistory = []\n\nfor epoch in range(n_epochs):\n    model.train()\n    with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n        bar.set_description(f\"Epoch {epoch}\")\n        for start in bar:\n            # take a batch\n            X_batch = X_train[start:start+batch_size]\n            y_batch = y_train[start:start+batch_size]\n            # forward pass\n            y_pred = model(X_batch)\n            loss = loss_fn(y_pred, y_batch)\n            # backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            # update weights\n            optimizer.step()\n            # print progress\n            bar.set_postfix(mse=float(loss))\n    # evaluate accuracy at end of each epoch\n    model.eval()\n    y_pred = model(X_test)\n    mse = loss_fn(y_pred, y_test)\n    mse = float(mse)\n    history.append(mse)\n    if mse < best_mse:\n        best_mse = mse\n        best_weights = copy.deepcopy(model.state_dict())\n\n# restore model and return best accuracy\nmodel.load_state_dict(best_weights)\nprint(\"MSE: %.2f\" % best_mse)\nprint(\"RMSE: %.2f\" % np.sqrt(best_mse))\nplt.plot(history)\nplt.show()\n\nmodel.eval()\nwith torch.no_grad():\n    # Test out inference with 5 samples\n    for i in range(5):\n        X_sample = X_test_raw[i: i+1]\n        X_sample = scaler.transform(X_sample)\n        X_sample = torch.tensor(X_sample, dtype=torch.float32)\n        y_pred = model(X_sample)\n        print(f\"{X_test_raw[i]} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")\n```"]