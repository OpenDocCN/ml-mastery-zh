["```py\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow.keras.datasets import mnist\n\n# Load MNIST data\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint(X_train.shape)\nprint(y_train.shape)\n\n# Check visually\nfig, ax = plt.subplots(4, 5, sharex=True, sharey=True)\nidx = np.random.randint(len(X_train), size=4*5).reshape(4,5)\nfor i in range(4):\n    for j in range(5):\n        ax[i][j].imshow(X_train[idx[i][j]], cmap=\"gray\")\nplt.show()\n```", "```py\n(60000, 28, 28)\n(60000,)\n```", "```py\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dense, AveragePooling2D, Flatten\n\n# LeNet5 model\nmodel = Sequential([\n    Conv2D(6, (5,5), input_shape=(28,28,1), padding=\"same\", activation=\"tanh\"),\n    AveragePooling2D((2,2), strides=2),\n    Conv2D(16, (5,5), activation=\"tanh\"),\n    AveragePooling2D((2,2), strides=2),\n    Conv2D(120, (5,5), activation=\"tanh\"),\n    Flatten(),\n    Dense(84, activation=\"tanh\"),\n    Dense(10, activation=\"softmax\")\n])\nmodel.summary()\n```", "```py\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                            Output Shape           Param\n                                                                 #\n================================================================================\n conv2d (Conv2D)                         (None, 28, 28, 6)      156\n\n average_pooling2d (AveragePooling2D)    (None, 14, 14, 6)      0\n\n conv2d_1 (Conv2D)                       (None, 10, 10, 16)     2416\n\n average_pooling2d_1 (AveragePooling2D)  (None, 5, 5, 16)       0\n\n conv2d_2 (Conv2D)                       (None, 1, 1, 120)      48120\n\n flatten (Flatten)                       (None, 120)            0\n\n dense (Dense)                           (None, 84)             10164\n\n dense_1 (Dense)                         (None, 10)             850\n\n================================================================================\nTotal params: 61706 (241.04 KB)\nTrainable params: 61706 (241.04 KB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n```", "```py\nimport numpy as np\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Reshape data to shape of (n_sample, height, width, n_channel)\nX_train = np.expand_dims(X_train, axis=3).astype('float32')\nX_test = np.expand_dims(X_test, axis=3).astype('float32')\nprint(X_train.shape)\n\n# One-hot encode the output\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nearlystopping = EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[earlystopping])\n```", "```py\nEpoch 1/100\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.1567 - accuracy: 0.9528 - val_loss: 0.0795 - val_accuracy: 0.9739\nEpoch 2/100\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0683 - accuracy: 0.9794 - val_loss: 0.0677 - val_accuracy: 0.9791\nEpoch 3/100\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0513 - accuracy: 0.9838 - val_loss: 0.0446 - val_accuracy: 0.9865\nEpoch 4/100\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0416 - accuracy: 0.9869 - val_loss: 0.0438 - val_accuracy: 0.9863\nEpoch 5/100\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0349 - accuracy: 0.9891 - val_loss: 0.0389 - val_accuracy: 0.9869\nEpoch 6/100\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0300 - accuracy: 0.9903 - val_loss: 0.0435 - val_accuracy: 0.9864\nEpoch 7/100\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0259 - accuracy: 0.9914 - val_loss: 0.0469 - val_accuracy: 0.9864\nEpoch 8/100\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0254 - accuracy: 0.9918 - val_loss: 0.0375 - val_accuracy: 0.9891\nEpoch 9/100\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0209 - accuracy: 0.9929 - val_loss: 0.0479 - val_accuracy: 0.9853\nEpoch 10/100\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0178 - accuracy: 0.9942 - val_loss: 0.0396 - val_accuracy: 0.9882\nEpoch 11/100\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0182 - accuracy: 0.9938 - val_loss: 0.0359 - val_accuracy: 0.9891\nEpoch 12/100\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0150 - accuracy: 0.9952 - val_loss: 0.0445 - val_accuracy: 0.9876\nEpoch 13/100\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0146 - accuracy: 0.9950 - val_loss: 0.0427 - val_accuracy: 0.9876\nEpoch 14/100\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0141 - accuracy: 0.9954 - val_loss: 0.0453 - val_accuracy: 0.9871\nEpoch 15/100\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0147 - accuracy: 0.9951 - val_loss: 0.0404 - val_accuracy: 0.9890\n```", "```py\nmodel.save(\"lenet5.h5\")\n```", "```py\n#!/usr/bin/env python\n\nimport numpy as np\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Conv2D, Dense, AveragePooling2D, Flatten\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\n\n# Load MNIST data\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint(X_train.shape)\nprint(y_train.shape)\n\n# LeNet5 model\nmodel = Sequential([\n    Conv2D(6, (5,5), input_shape=(28,28,1), padding=\"same\", activation=\"tanh\"),\n    AveragePooling2D((2,2), strides=2),\n    Conv2D(16, (5,5), activation=\"tanh\"),\n    AveragePooling2D((2,2), strides=2),\n    Conv2D(120, (5,5), activation=\"tanh\"),\n    Flatten(),\n    Dense(84, activation=\"tanh\"),\n    Dense(10, activation=\"softmax\")\n])\n\n# Reshape data to shape of (n_sample, height, width, n_channel)\nX_train = np.expand_dims(X_train, axis=3).astype('float32')\nX_test = np.expand_dims(X_test, axis=3).astype('float32')\n\n# One-hot encode the output\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\n# Training\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nearlystopping = EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[earlystopping])\n\nmodel.save(\"lenet5.h5\")\n```", "```py\npip install tf2onnx\n```", "```py\npython -m tf2onnx.convert --keras lenet5.h5 --output lenet5.onnx\n```", "```py\nnet = cv2.dnn.readNetFromONNX('model.onnx')\nblob = cv2.dnn.blobFromImage(numpyarray, scale, size, mean)\nnet.setInput(blob)\noutput = net.forward()\n```", "```py\nimport numpy as np\nimport cv2\nfrom tensorflow.keras.datasets import mnist\n\n# Load the frozen model in OpenCV\nnet = cv2.dnn.readNetFromONNX('lenet5.onnx')\n\n# Prepare input image\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\ncorrect = 0\nwrong = 0\nfor i in range(len(X_test)):\n    img = X_test[i]\n    label = y_test[i]\n\n    blob = cv2.dnn.blobFromImage(img, 1.0, (28, 28))\n\n    # Run inference\n    net.setInput(blob)\n    output = net.forward()\n    prediction = np.argmax(output)\n    if prediction == label:\n        correct += 1\n    else:\n        wrong += 1\n\nprint(\"count of test samples:\", len(X_test))\nprint(\"accuracy:\", (correct/(correct+wrong)))\n```", "```py\ncount of test samples: 10000\naccuracy: 0.9889\n```"]