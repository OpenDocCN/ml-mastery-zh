["```py\nclass DotProductAttention(Layer):\n    def __init__(self, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n\n    def call(self, queries, keys, values, d_k, mask=None):\n        ...\n```", "```py\n...\nscores = matmul(queries, keys, transpose_b=True) / sqrt(d_k)\n...\n```", "```py\n...\nif mask is not None:\n    scores += -1e9 * mask\n...\n```", "```py\n...\nweights = softmax(scores)\n...\n```", "```py\n...\nreturn matmul(weights, values)\n```", "```py\nfrom tensorflow import matmul, math, cast, float32\nfrom tensorflow.keras.layers import Layer\nfrom keras.backend import softmax\n\n# Implementing the Scaled-Dot Product Attention\nclass DotProductAttention(Layer):\n    def __init__(self, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n\n    def call(self, queries, keys, values, d_k, mask=None):\n        # Scoring the queries against the keys after transposing the latter, and scaling\n        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n\n        # Apply mask to the attention scores\n        if mask is not None:\n            scores += -1e9 * mask\n\n        # Computing the weights by a softmax operation\n        weights = softmax(scores)\n\n        # Computing the attention by a weighted sum of the value vectors\n        return matmul(weights, values)\n```", "```py\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nbatch_size = 64  # Batch size from the training process\n...\n```", "```py\n...\ninput_seq_length = 5  # Maximum length of the input sequence\n\nqueries = random.random((batch_size, input_seq_length, d_k))\nkeys = random.random((batch_size, input_seq_length, d_k))\nvalues = random.random((batch_size, input_seq_length, d_v))\n...\n```", "```py\n...\nattention = DotProductAttention()\n...\n```", "```py\n...\nprint(attention(queries, keys, values, d_k))\n```", "```py\nfrom numpy import random\n\ninput_seq_length = 5  # Maximum length of the input sequence\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nbatch_size = 64  # Batch size from the training process\n\nqueries = random.random((batch_size, input_seq_length, d_k))\nkeys = random.random((batch_size, input_seq_length, d_k))\nvalues = random.random((batch_size, input_seq_length, d_v))\n\nattention = DotProductAttention()\nprint(attention(queries, keys, values, d_k))\n```", "```py\ntf.Tensor(\n[[[0.60413814 0.52436507 0.46551135 ... 0.5260341  0.33879933 0.43999898]\n  [0.60433316 0.52383804 0.465411   ... 0.5262608  0.33915892 0.43782598]\n  [0.62321603 0.5349194  0.46824688 ... 0.531323   0.34432083 0.43554053]\n  [0.60013235 0.54162943 0.47391182 ... 0.53600514 0.33722004 0.4192218 ]\n  [0.6295709  0.53511244 0.46552944 ... 0.5317217  0.3462567  0.43129003]]\n ...\n\n[[0.20291057 0.18463902 0.641182   ... 0.4706118  0.4194418  0.39908117]\n  [0.19932748 0.18717204 0.64831126 ... 0.48373622 0.3995132  0.37968236]\n  [0.20611541 0.18079443 0.6374859  ... 0.48258874 0.41704425 0.4016996 ]\n  [0.19703123 0.18210654 0.6400498  ... 0.47037745 0.4257752  0.3962079 ]\n  [0.19237372 0.18474475 0.64944196 ... 0.49497223 0.38804317 0.36352912]]], \nshape=(64, 5, 64), dtype=float32)\n```"]