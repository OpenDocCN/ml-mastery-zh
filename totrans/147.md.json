["```py\nfrom torchvision import datasets\n\ntrain_data = datasets.FashionMNIST('data', train=True, download=True)\ntrain_data = list(train_data)[:4000]\n```", "```py\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n  0%|          | 0/26421880 [00:00<?, ?it/s]\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n  0%|          | 0/29515 [00:00<?, ?it/s]\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n  0%|          | 0/4422102 [00:00<?, ?it/s]\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n  0%|          | 0/5148 [00:00<?, ?it/s]\nExtracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n```", "```py\nimport matplotlib.pyplot as plt\n\n# plot the first 10 images in the training data\nfor i, (img, label) in enumerate(train_data[:10]):\n    plt.subplot(4, 3, i+1)\n    plt.imshow(img, cmap=\"gray\")\n\nplt.show()\n```", "```py\nfrom torchvision import datasets, transforms\n\n# download and apply the transform\ntrain_data = datasets.FashionMNIST('data', train=True, download=True, transform=transforms.ToTensor())\ntrain_data = list(train_data)[:4000]\n```", "```py\n# splitting the dataset into train and validation sets\ntrain_data, val_data = train_data[:3500], train_data[3500:]\n```", "```py\nimport torch\n\n# build custom softmax module\nclass Softmax(torch.nn.Module):\n    def __init__(self, n_inputs, n_outputs):\n        super().__init__()\n        self.linear = torch.nn.Linear(n_inputs, n_outputs)\n\n    def forward(self, x):\n        pred = self.linear(x)\n        return pred\n```", "```py\n# call Softmax Classifier\nmodel_softmax = Softmax(784, 10)\nprint(model_softmax.state_dict())\n```", "```py\nOrderedDict([('linear.weight',\n              tensor([[-0.0344,  0.0334, -0.0278,  ..., -0.0232,  0.0198, -0.0123],\n                      [-0.0274, -0.0048, -0.0337,  ..., -0.0340,  0.0274, -0.0091],\n                      [ 0.0078, -0.0057,  0.0178,  ..., -0.0013,  0.0322, -0.0219],\n                      ...,\n                      [ 0.0158, -0.0139, -0.0220,  ..., -0.0054,  0.0284, -0.0058],\n                      [-0.0142, -0.0268,  0.0172,  ...,  0.0099, -0.0145, -0.0154],\n                      [-0.0172, -0.0224,  0.0016,  ...,  0.0107,  0.0147,  0.0252]])),\n             ('linear.bias',\n              tensor([-0.0156,  0.0061,  0.0285,  0.0065,  0.0122, -0.0184, -0.0197,  0.0128,\n                       0.0251,  0.0256]))])\n```", "```py\nclass Softmax(torch.nn.Module):\n    \"custom softmax module\"\n    def __init__(self, n_inputs, n_outputs):\n        super().__init__()\n        self.linear = torch.nn.Linear(n_inputs, n_outputs)\n\n    def forward(self, x):\n        pred = self.linear(x)\n        return pred\n```", "```py\nepochs = 200\nLoss = []\nacc = []\nfor epoch in range(epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n        outputs = model_softmax(images.view(-1, 28*28))\n        loss = criterion(outputs, labels)\n        # Loss.append(loss.item())\n        loss.backward()\n        optimizer.step()\n    Loss.append(loss.item())\n    correct = 0\n    for images, labels in val_loader:\n        outputs = model_softmax(images.view(-1, 28*28))\n        _, predicted = torch.max(outputs.data, 1)\n        correct += (predicted == labels).sum()\n    accuracy = 100 * (correct.item()) / len(val_data)\n    acc.append(accuracy)\n    if epoch % 10 == 0:\n        print('Epoch: {}. Loss: {}. Accuracy: {}'.format(epoch, loss.item(), accuracy))\n```", "```py\nEpoch: 0\\. Loss: 1.0223602056503296\\. Accuracy: 67.2\nEpoch: 10\\. Loss: 0.5806267857551575\\. Accuracy: 78.4\nEpoch: 20\\. Loss: 0.5087125897407532\\. Accuracy: 81.2\nEpoch: 30\\. Loss: 0.46658074855804443\\. Accuracy: 82.0\nEpoch: 40\\. Loss: 0.4357391595840454\\. Accuracy: 82.4\nEpoch: 50\\. Loss: 0.4111904203891754\\. Accuracy: 82.8\nEpoch: 60\\. Loss: 0.39078089594841003\\. Accuracy: 83.4\nEpoch: 70\\. Loss: 0.37331104278564453\\. Accuracy: 83.4\nEpoch: 80\\. Loss: 0.35801735520362854\\. Accuracy: 83.4\nEpoch: 90\\. Loss: 0.3443795442581177\\. Accuracy: 84.2\nEpoch: 100\\. Loss: 0.33203184604644775\\. Accuracy: 84.2\nEpoch: 110\\. Loss: 0.32071244716644287\\. Accuracy: 84.0\nEpoch: 120\\. Loss: 0.31022894382476807\\. Accuracy: 84.2\nEpoch: 130\\. Loss: 0.30044111609458923\\. Accuracy: 84.4\nEpoch: 140\\. Loss: 0.29124370217323303\\. Accuracy: 84.6\nEpoch: 150\\. Loss: 0.28255513310432434\\. Accuracy: 84.6\nEpoch: 160\\. Loss: 0.2743147313594818\\. Accuracy: 84.4\nEpoch: 170\\. Loss: 0.26647457480430603\\. Accuracy: 84.2\nEpoch: 180\\. Loss: 0.2589966356754303\\. Accuracy: 84.2\nEpoch: 190\\. Loss: 0.2518490254878998\\. Accuracy: 84.2\n```", "```py\nplt.plot(Loss)\nplt.xlabel(\"no. of epochs\")\nplt.ylabel(\"total loss\")\nplt.show()\n```", "```py\nplt.plot(acc)\nplt.xlabel(\"no. of epochs\")\nplt.ylabel(\"total accuracy\")\nplt.show()\n```", "```py\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision import datasets\n\n# download and apply the transform\ntrain_data = datasets.FashionMNIST('data', train=True, download=True, transform=transforms.ToTensor())\ntrain_data = list(train_data)[:4000]\n\n# splitting the dataset into train and validation sets\ntrain_data, val_data = train_data[:3500], train_data[3500:]\n\n# build custom softmax module\nclass Softmax(torch.nn.Module):\n    def __init__(self, n_inputs, n_outputs):\n        super(Softmax, self).__init__()\n        self.linear = torch.nn.Linear(n_inputs, n_outputs)\n\n    def forward(self, x):\n        pred = self.linear(x)\n        return pred\n\n# call Softmax Classifier\nmodel_softmax = Softmax(784, 10)\nmodel_softmax.state_dict()\n\n# define loss, optimizier, and dataloader for train and validation sets\noptimizer = torch.optim.SGD(model_softmax.parameters(), lr = 0.01)\ncriterion = torch.nn.CrossEntropyLoss()\nbatch_size = 16\ntrain_loader = DataLoader(dataset = train_data, batch_size = batch_size)\nval_loader = DataLoader(dataset = val_data, batch_size = batch_size)\n\nepochs = 200\nLoss = []\nacc = []\nfor epoch in range(epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n        outputs = model_softmax(images.view(-1, 28*28))\n        loss = criterion(outputs, labels)\n        # Loss.append(loss.item())\n        loss.backward()\n        optimizer.step()\n    Loss.append(loss.item())\n    correct = 0\n    for images, labels in val_loader:\n        outputs = model_softmax(images.view(-1, 28*28))\n        _, predicted = torch.max(outputs.data, 1)\n        correct += (predicted == labels).sum()\n    accuracy = 100 * (correct.item()) / len(val_data)\n    acc.append(accuracy)\n    if epoch % 10 == 0:\n        print('Epoch: {}. Loss: {}. Accuracy: {}'.format(epoch, loss.item(), accuracy))\n\nplt.plot(Loss)\nplt.xlabel(\"no. of epochs\")\nplt.ylabel(\"total loss\")\nplt.show()\n\nplt.plot(acc)\nplt.xlabel(\"no. of epochs\")\nplt.ylabel(\"total accuracy\")\nplt.show()\n```"]