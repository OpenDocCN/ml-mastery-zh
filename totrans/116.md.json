["```py\nsudo pip install torch\n```", "```py\nsudo pip install torchvision\n```", "```py\n# check pytorch version\nimport torch\nprint(torch.__version__)\n```", "```py\npython versions.py\n```", "```py\n1.3.1\n```", "```py\n# dataset definition\nclass CSVDataset(Dataset):\n    # load the dataset\n    def __init__(self, path):\n        # store the inputs and outputs\n        self.X = ...\n        self.y = ...\n\n    # number of rows in the dataset\n    def __len__(self):\n        return len(self.X)\n\n    # get a row at an index\n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n```", "```py\n...\n# create the dataset\ndataset = CSVDataset(...)\n# select rows from the dataset\ntrain, test = random_split(dataset, [[...], [...]])\n# create a data loader for train and test sets\ntrain_dl = DataLoader(train, batch_size=32, shuffle=True)\ntest_dl = DataLoader(test, batch_size=1024, shuffle=False)\n```", "```py\n...\n# train the model\nfor i, (inputs, targets) in enumerate(train_dl):\n\t...\n```", "```py\n# model definition\nclass MLP(Module):\n    # define model elements\n    def __init__(self, n_inputs):\n        super(MLP, self).__init__()\n        self.layer = Linear(n_inputs, 1)\n        self.activation = Sigmoid()\n\n    # forward propagate input\n    def forward(self, X):\n        X = self.layer(X)\n        X = self.activation(X)\n        return X\n```", "```py\n...\nxavier_uniform_(self.layer.weight)\n```", "```py\n# define the optimization\ncriterion = MSELoss()\noptimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n```", "```py\n...\n# enumerate epochs\nfor epoch in range(100):\n    # enumerate mini batches\n    for i, (inputs, targets) in enumerate(train_dl):\n    \t...\n```", "```py\n...\n# clear the gradients\noptimizer.zero_grad()\n# compute the model output\nyhat = model(inputs)\n# calculate loss\nloss = criterion(yhat, targets)\n# credit assignment\nloss.backward()\n# update model weights\noptimizer.step()\n```", "```py\n...\nfor i, (inputs, targets) in enumerate(test_dl):\n    # evaluate the model on the test set\n    yhat = model(inputs)\n    ...\n```", "```py\n...\n# convert row to data\nrow = Variable(Tensor([row]).float())\n# make prediction\nyhat = model(row)\n# retrieve numpy array\nyhat = yhat.detach().numpy()\n```", "```py\n# pytorch mlp for binary classification\nfrom numpy import vstack\nfrom pandas import read_csv\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nfrom torch import Tensor\nfrom torch.nn import Linear\nfrom torch.nn import ReLU\nfrom torch.nn import Sigmoid\nfrom torch.nn import Module\nfrom torch.optim import SGD\nfrom torch.nn import BCELoss\nfrom torch.nn.init import kaiming_uniform_\nfrom torch.nn.init import xavier_uniform_\n\n# dataset definition\nclass CSVDataset(Dataset):\n    # load the dataset\n    def __init__(self, path):\n        # load the csv file as a dataframe\n        df = read_csv(path, header=None)\n        # store the inputs and outputs\n        self.X = df.values[:, :-1]\n        self.y = df.values[:, -1]\n        # ensure input data is floats\n        self.X = self.X.astype('float32')\n        # label encode target and ensure the values are floats\n        self.y = LabelEncoder().fit_transform(self.y)\n        self.y = self.y.astype('float32')\n        self.y = self.y.reshape((len(self.y), 1))\n\n    # number of rows in the dataset\n    def __len__(self):\n        return len(self.X)\n\n    # get a row at an index\n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n\n    # get indexes for train and test rows\n    def get_splits(self, n_test=0.33):\n        # determine sizes\n        test_size = round(n_test * len(self.X))\n        train_size = len(self.X) - test_size\n        # calculate the split\n        return random_split(self, [train_size, test_size])\n\n# model definition\nclass MLP(Module):\n    # define model elements\n    def __init__(self, n_inputs):\n        super(MLP, self).__init__()\n        # input to first hidden layer\n        self.hidden1 = Linear(n_inputs, 10)\n        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n        self.act1 = ReLU()\n        # second hidden layer\n        self.hidden2 = Linear(10, 8)\n        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n        self.act2 = ReLU()\n        # third hidden layer and output\n        self.hidden3 = Linear(8, 1)\n        xavier_uniform_(self.hidden3.weight)\n        self.act3 = Sigmoid()\n\n    # forward propagate input\n    def forward(self, X):\n        # input to first hidden layer\n        X = self.hidden1(X)\n        X = self.act1(X)\n         # second hidden layer\n        X = self.hidden2(X)\n        X = self.act2(X)\n        # third hidden layer and output\n        X = self.hidden3(X)\n        X = self.act3(X)\n        return X\n\n# prepare the dataset\ndef prepare_data(path):\n    # load the dataset\n    dataset = CSVDataset(path)\n    # calculate split\n    train, test = dataset.get_splits()\n    # prepare data loaders\n    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n    return train_dl, test_dl\n\n# train the model\ndef train_model(train_dl, model):\n    # define the optimization\n    criterion = BCELoss()\n    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n    # enumerate epochs\n    for epoch in range(100):\n        # enumerate mini batches\n        for i, (inputs, targets) in enumerate(train_dl):\n            # clear the gradients\n            optimizer.zero_grad()\n            # compute the model output\n            yhat = model(inputs)\n            # calculate loss\n            loss = criterion(yhat, targets)\n            # credit assignment\n            loss.backward()\n            # update model weights\n            optimizer.step()\n\n# evaluate the model\ndef evaluate_model(test_dl, model):\n    predictions, actuals = list(), list()\n    for i, (inputs, targets) in enumerate(test_dl):\n        # evaluate the model on the test set\n        yhat = model(inputs)\n        # retrieve numpy array\n        yhat = yhat.detach().numpy()\n        actual = targets.numpy()\n        actual = actual.reshape((len(actual), 1))\n        # round to class values\n        yhat = yhat.round()\n        # store\n        predictions.append(yhat)\n        actuals.append(actual)\n    predictions, actuals = vstack(predictions), vstack(actuals)\n    # calculate accuracy\n    acc = accuracy_score(actuals, predictions)\n    return acc\n\n# make a class prediction for one row of data\ndef predict(row, model):\n    # convert row to data\n    row = Tensor([row])\n    # make prediction\n    yhat = model(row)\n    # retrieve numpy array\n    yhat = yhat.detach().numpy()\n    return yhat\n\n# prepare the data\npath = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\ntrain_dl, test_dl = prepare_data(path)\nprint(len(train_dl.dataset), len(test_dl.dataset))\n# define the network\nmodel = MLP(34)\n# train the model\ntrain_model(train_dl, model)\n# evaluate the model\nacc = evaluate_model(test_dl, model)\nprint('Accuracy: %.3f' % acc)\n# make a single prediction (expect class=1)\nrow = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\nyhat = predict(row, model)\nprint('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))\n```"]