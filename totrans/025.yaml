- en: 3 Ways of Using Gemma 2 Locally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://machinelearningmastery.com/3-ways-of-using-gemma-2-locally/](https://machinelearningmastery.com/3-ways-of-using-gemma-2-locally/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![3 Ways of Using Gemma 2 Locally](../Images/dc201ce24fbf2415fcd6b8efaa23d8b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: After the highly successful launch of Gemma 1, the Google team introduced an
    even more advanced model series called Gemma 2\. This new family of Large Language
    Models (LLMs) includes models with 9 billion (9B) and 27 billion (27B) parameters.
    Gemma 2 offers higher performance and greater inference efficiency than its predecessor,
    with significant safety advancements built in. Both models outperform the Llama
    3 and Gork 1 models.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will learn about the three applications that will help
    you run the Gemma 2 model locally faster than online. To experience the state-of-the-art
    model locally, you just have to install the application, download the model, and
    start using it. It is that simple.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Jan
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Download and install [Jan](https://jan.ai/) from the official website. Jan is
    my favorite application for running and testing various open-source and property
    LLMs. It is super easy to set up and highly flexible in terms of importing and
    using local models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Launch the Jan application and go to the Model Hub menu. Then, paste the following
    link of the Hugging Face repository into the search bar and press enter: **bartowski/gemma-2-9b-it-GGUF**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![3 Ways of Using Gemma 2 Locally](../Images/c045fd6057727f77d34b830d311e49d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: You will be redirected to a new window, where you have the option to select
    various quantized versions of the model. We will be downloading the “Q4-K-M” version.
  prefs: []
  type: TYPE_NORMAL
- en: '![3 Ways of Using Gemma 2 Locally](../Images/feecad737c904007ebe0ce768d31faab.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Select the downloaded model from the model menu on the right panel and start
    using it.
  prefs: []
  type: TYPE_NORMAL
- en: This version of the quantized model currently gives me 37 tokens per second,
    but you might improve your speed even further if you use a different version.
  prefs: []
  type: TYPE_NORMAL
- en: '![3 Ways of Using Gemma 2 Locally](../Images/381427dd20956e2f431dcf699c811325.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Ollama
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Go to the official website to download and install [Ollama](https://ollama.com/download).
    It is a favorite among developers and people who are familiar with terminals and
    CLI tools. Even for new users, it is simple to set up.
  prefs: []
  type: TYPE_NORMAL
- en: After installation is completed, please launch the Ollama application and type
    the following command in your favorite terminal. I am using Powershell on Windows
    11.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Depending on your internet speed, it will take approximately half an hour to
    download the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![3 Ways of Using Gemma 2 Locally](../Images/90d90e4cc42e6e641be1ee8f86ef102d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: After the download is complete, you can start prompting and start using it within
    your terminal.
  prefs: []
  type: TYPE_NORMAL
- en: '![3 Ways of Using Gemma 2 Locally](../Images/cb577dd2885bfbb57d95a15b4ce7a945.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Using Gemma2 by Importing from GGUF model file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you already have a GGUF model file and want to use it with Ollama, then
    you have to first create a new file with the name “Modelfile” and type the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: After that, create the model using the Modelfile, which points to the GGUF file
    in your directory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: When model transferring is done successfully, please type the following command
    to start using it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![3 Ways of Using Gemma 2 Locally](../Images/09f32729029bc5a6271fb85a2e1a366c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Msty
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Download and install [Msty](https://msty.app/) from the official website. Msty
    is a new contender, and it is becoming my favorite. It offers tons of features
    and models. You can even connect to proprietary models or the Ollama servers.
    It is a simple and powerful application that you should give a try.
  prefs: []
  type: TYPE_NORMAL
- en: After successfully installing the application, please launch the program and
    navigate to “Local AI Models” by clicking the button on the left panel.
  prefs: []
  type: TYPE_NORMAL
- en: '![3 Ways of Using Gemma 2 Locally](../Images/65b590dc4d6f53a22d6d6ed3309221f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the “Download More Models” button and type the following link into
    the search bar: **bartowski/gemma-2-9b-it-GGUF**. Make sure you have selected
    Hugging Face as the model Hub.'
  prefs: []
  type: TYPE_NORMAL
- en: '![3 Ways of Using Gemma 2 Locally](../Images/a3916fdae1506bc3d0579bd6f49f3fc5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: After downloading is completed, start using it.
  prefs: []
  type: TYPE_NORMAL
- en: '![3 Ways of Using Gemma 2 Locally](../Images/366b19acdb4f31ea717fac23260f2b66.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Using Msty with Ollama
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to use the Ollama model in a chatbot application instead of a terminal,
    you can use Msty’s connect with Ollama option. It is straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: First, go to the terminal and start the Ollama server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Copy the server link.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the “Local AI Models” menu and click on the settings button located
    in the top right corner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, select “Remote Model Providers” and click on the “Add New Provider” button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, choose the model provider as “Ollama remote” and enter the service endpoint
    link for the Ollama server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the “Re-fetch models” button and select “gemma2:latest”, then click
    the “Add” button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![3 Ways of Using Gemma 2 Locally](../Images/3de2e35d87ed8a7b7c1a1ef031eaac78.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: In the chat menu, select the new model and start using it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![3 Ways of Using Gemma 2 Locally](../Images/23a14432ea4ae3189717e92abefc426f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The three applications we reviewed are powerful and come with tons of features
    that will enhance your experience of using AI models locally. All you have to
    do is download the application and models, and the rest is pretty simple.
  prefs: []
  type: TYPE_NORMAL
- en: I use the Jan application to test open-source LLM performance and generate the
    code and content. It is fast and private, and my data never leaves my laptop.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we have learned how to use the Jan, Ollama, and Msty to run
    the Gemma 2 model locally. These applications comes with important features that
    will enhance your experience of using LLMs locally.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you enjoyed my brief tutorial. I enjoy sharing the products and applications
    I am passionate about and use regularly.
  prefs: []
  type: TYPE_NORMAL
