- en: Building Your mini-ChatGPT at Home
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://machinelearningmastery.com/building-your-mini-chatgpt-at-home/](https://machinelearningmastery.com/building-your-mini-chatgpt-at-home/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ChatGPT is fun to play with. Chances are, you also want to have your own copy
    running privately. Realistically, that’s impossible because ChatGPT is not a software
    for download, and it needs tremendous computer power to run. But you can build
    a trimmed-down version that can run on commodity hardware. In this post, you will
    learn about
  prefs: []
  type: TYPE_NORMAL
- en: What are language models that can behave like ChatGPT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build a chatbot using the advanced language models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/8ab0fc958bd90cb6da8d1ffa2cb9687c.png)'
  prefs: []
  type: TYPE_IMG
- en: Building Your mini-ChatGPT at Home
  prefs: []
  type: TYPE_NORMAL
- en: Picture generated by the author using Stable Diffusion. Some rights reserved.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This post is divided into three parts; they are:'
  prefs: []
  type: TYPE_NORMAL
- en: What are Instruction-Following Models?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to Find Instruction Following Models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a Simple Chatbot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are Instruction-Following Models?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Language models are machine learning models that can predict word probability
    based on the sentence’s prior words. If we ask the model for the next word and
    feed it back to the model regressively to ask for more, the model is doing text
    generation.
  prefs: []
  type: TYPE_NORMAL
- en: Text generation model is the idea behind many large language models such as
    GPT3\. Instruction-following models, however, are fine-tuned text generation models
    that learn about dialog and instructions. It is operated as a conversation between
    two people, and when one finishes a sentence, another person responds accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, a text generation model can help you finish a paragraph with a leading
    sentence. But an instruction following model can answer your questions or respond
    as requested.
  prefs: []
  type: TYPE_NORMAL
- en: It doesn’t mean you cannot use a text generation model to build a chatbot. But
    you should find a better quality result with an instruction-following model, which
    is fine-tuned for such use.
  prefs: []
  type: TYPE_NORMAL
- en: How to Find Instruction Following Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may find a lot of instruction following models nowadays. But to build a
    chatbot, you need something you can easily work with.
  prefs: []
  type: TYPE_NORMAL
- en: One handy repository that you can search on is Hugging Face. The models there
    are supposed to use with the transformers library from Hugging Face. It is helpful
    because different models may work slightly differently. It would be tedious to
    make your Python code to support multiple models, but the transformers library
    unified them and hide all those differences from your code.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cae7a36c1110886474acb7651db3e9a0.png)'
  prefs: []
  type: TYPE_IMG
- en: Usually, the instruction following models carries the keyword “instruct” in
    the model name. Searching with this keyword on Hugging Face can give you more
    than a thousand models. But not all can work. You need to check out each of them
    and read their model card to understand what this model can do in order to pick
    the most suitable one.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several technical criteria to pick your model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**What the model was trained on:** Specifically, that means which language
    the model can speak. A model trained with English text from novels probably is
    not helpful for a German chatbot for Physics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What is the deep learning library it uses:** Usually models in Hugging Face
    are built with TensorFlow, PyTorch, and Flax. Not all models have a version for
    all libraries. You need to make sure you have that specific library installed
    before you can run a model with transformers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What resources the model needs:** The model can be enormous. Often it would
    require a GPU to run. But some model needs a very high-end GPU or even multiple
    high-end GPUs. You need to verify if your resources can support the model inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a Simple Chatbot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s build a simple chatbot. The chatbot is just a program that runs on the
    command line, which takes one line of text as input from the user and responds
    with one line of text generated by the language model.
  prefs: []
  type: TYPE_NORMAL
- en: The model chosen for this task is `falcon-7b-instruct`. It is a 7-billion parameters
    model. You may need to run on a modern GPU such as nVidia RTX 3000 series since
    it was designed to run on bfloat16 floating point for best performance. Using
    the GPU resources on Google Colab, or from a suitable EC2 instance on AWS are
    also options.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build a chatbot in Python, it is as simple as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `input("> ")` function takes one line of input from the user. You will see
    the string `"> "` on the screen for your input. Input is captured once you press
    Enter.
  prefs: []
  type: TYPE_NORMAL
- en: The reminaing question is how to get the response. In LLM, you provide your
    input, or prompt, as a sequence of token IDs (integers), and it will respond with
    another sequence of token IDs. You should convert between the sequence of integers
    and text string before and after interacting with LLMs. The token IDs are specific
    for each model; that is, for the same integer, it means a different word for a
    different model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hugging Face library `transformers` is to make these steps easier. All you
    need is to create a pipeline and specify the model name some a few other paramters.
    Setting up a pipeline with the model name `tiiuae/falcon-7b-instruct`, with bfloat16
    floating point, and allows the model to use GPU if available is as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The pipeline is created as `"text-generation"` because it is the way the model
    card suggested you to work with this model. A pipeline in `transformers` is a
    sequence of steps for a specific task. Text-generation is one of these tasks.
  prefs: []
  type: TYPE_NORMAL
- en: To use the pipeline, you need to specify a few more parameters for generating
    the text. Recall that the model is not generating the text directly but the probabilities
    of tokens. You have to determine what is the next word from these probabilities
    and repeat the process to generate more words. Usually, this process will introduce
    some variations by not picking the single token with the highest probability but
    sampling according to the probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is how you’re going to use the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You provided the prompt in the variable `prompt` to generate the output sequences.
    You can ask the model to give you a few options, but here you set `num_return_sequences=1`
    so there would only be one. You also let the model to generate text using sampling,
    but only from the 10 highest probability tokens (`top_k=10`). The returned sequence
    will not contain your prompt since you have `return_full_text=False`. The most
    important one parameters are `eos_token_id=newline_token` and `pad_token_id=tokenizer.eos_token_id`.
    These are to let the model to generate text continuously, but only until a newline
    character. The newline character’s token id is 193, as obtained from the first
    line in the code snippet.
  prefs: []
  type: TYPE_NORMAL
- en: 'The returned `sequences` is a list of dictionaries (list of one dict in this
    case). Each dictionary contains the token sequence and string. We can easily print
    the string as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'A language model is memoryless. It will not remember how many times you used
    the model and the prompts you used before. Every time is new, so you need to provide
    the history of the previous dialog to the model. This is easily done. But because
    it is an instruction-following model that knows how to process a dialog, you need
    to remember to identify which person said what in the prompt. Let’s assume it
    is a dialog between Alice and Bob (or any names). You prefix the name in each
    sentence they spoke in the prompt, like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the model should generate text that match the dialog. Once the response
    from the model is obtained, to append it together with another text from Alice
    to the prompt, and send to the model again. Putting everything together, below
    is a simple chatbot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the `dialog` variable is updated to keep track on the dialog in each
    iteration, and how it is used to set variable `prompt` for the next run of the
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you try to ask “What is relativity” with the chatbot, it doesn’t sound
    very knowledgable. That’s where you need to do some prompt engineering. You can
    make Bob a Physics professor so he can have more detailed answer on this topic.
    That’s the magic of LLMs that can adjust the response by a simple change in the
    prompt. All you need is to add a description before the dialog started. Updated
    code is as follows (see now `dialog` is initialized with a persona description):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This chatbot may be slow if you do not have powerful enough hardware. You may
    not see the exact result, but the following is an example dialog from the above
    code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The chatbot will run until you press Ctrl-C to stop it or meet the maximum length
    (`max_length=500`) in the pipeline input. The maximum length is how much your
    model can read at a time. Your prompt must be no more than this many tokens. The
    higher this maximum length will make the model run slower, and every model has
    a limit on how large you can set this length. The `falcon-7b-instruct` model allows
    you to set this to 2048 only. ChatGPT, on the other hand, is 4096.
  prefs: []
  type: TYPE_NORMAL
- en: You may also notice the output quality is not perfect. Partially because you
    didn’t attempt to polish the response from the model before sending back to the
    user, and partially because the model we chose is a 7-billion parameters model,
    which is the smallest in its family. Usually you will see a better result with
    a larger model. But that would also require more resources to run.
  prefs: []
  type: TYPE_NORMAL
- en: Further Readings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Below is a paper that may help you understand better about the instruction
    following model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Ouyang et al, Training language models to follow instructions with human feedback
    (2022)](https://arxiv.org/pdf/2203.02155.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this post, you learned how to create a chatbot using a large language model
    from the Hugging Face library. Specifically, you learned:'
  prefs: []
  type: TYPE_NORMAL
- en: A language model that can do conversation is called instruction-following models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to find such models in Hugging Face
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use the models using the `transformers` library, and build a chatbot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
