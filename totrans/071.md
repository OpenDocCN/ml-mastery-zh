# 使用OpenCV的k-最近邻分类

> [https://machinelearningmastery.com/k-nearest-neighbors-classification-using-opencv/](https://machinelearningmastery.com/k-nearest-neighbors-classification-using-opencv/)

OpenCV库有一个模块实现了用于机器学习应用的k-最近邻算法。

在本教程中，你将学习如何应用OpenCV的k-最近邻算法来分类手写数字。

完成本教程后，你将了解：

+   k-最近邻算法的几个最重要特征。

+   如何在OpenCV中使用k-最近邻算法进行图像分类。

**用我的书[《OpenCV中的机器学习》](https://machinelearning.samcart.com/products/machine-learning-opencv/)** **启动你的项目**。它提供了**自学教程**和**可工作的代码**。

让我们开始吧。[![](../Images/84c2d4834131494865dc8e314a88819b.png)](https://machinelearningmastery.com/wp-content/uploads/2023/01/kNN_cover-scaled.jpg)

使用OpenCV的k-最近邻分类

图片由[Gleren Meneghin](https://unsplash.com/photos/VSLPOL9PwB8)提供，部分版权保留。

## **教程概述**

本教程分为两个部分；它们是：

+   k-最近邻算法如何工作的提醒

+   使用k-最近邻算法进行OpenCV中的图像分类

## **前提条件**

对于本教程，我们假设你已经熟悉：

+   [k-最近邻算法如何工作](https://machinelearningmastery.com/k-nearest-neighbors-for-machine-learning/)

+   [使用OpenCV读取和显示图像](https://machinelearningmastery.com/?p=14402&preview=true)

## **k-最近邻算法如何工作的提醒**

k-最近邻（kNN）算法已经在[Jason Brownlee的这篇教程](https://machinelearningmastery.com/k-nearest-neighbors-for-machine-learning/)中讲解得很好，但让我们先回顾一下他教程中一些最重要的点：

+   **kNN算法不涉及任何学习。它只是存储和使用整个训练数据集作为其模型表示。因此，kNN也被称为*懒惰学习*算法。** 

***   **由于整个训练数据集被存储，因此保持其整理良好、经常用新数据更新，并尽可能避免异常值是有意义的。** 

***   **新的实例通过根据选择的距离度量在整个训练数据集中搜索最相似的实例来预测。距离度量的选择通常基于数据的特性。**

***   **如果kNN用于解决回归问题，则通常使用k个最相似实例的均值或中位数来生成预测。** 

***   **如果kNN用于解决分类问题，则可以从k个最相似实例中频率最高的类别生成预测。** 

***   **可以通过尝试不同的值来调整 *k* 的值，并查看哪种值最适合问题。**

***   **kNN 算法的计算成本随着训练数据集的大小增加而增加。kNN 算法在输入数据的维度增加时也会遇到困难。**

**## **在 OpenCV 中使用 k-最近邻进行图像分类**

在本教程中，我们将考虑对手写数字进行分类的应用。

在[之前的教程](https://machinelearningmastery.com/image-datasets-for-practicing-machine-learning-in-opencv)中，我们看到 OpenCV 提供了图像 digits.png，该图像由 5,000 个 $20\times 20$ 像素的子图像组成，每个子图像展示了从 0 到 9 的手写数字。

我们还看到如何[将数据集图像转换为特征向量表示](https://machinelearningmastery.com/image-vector-representation-for-machine-learning-using-opencv)，然后再输入到机器学习算法中。

我们将把 OpenCV 的数字数据集拆分为训练集和测试集，将其转换为特征向量，然后使用这些特征向量来*训练*和测试一个 kNN 分类器以分类手写数字。

**注意**：我们之前提到 kNN 算法不涉及任何训练/学习，但我们将参考一个*训练数据集*，以区分用于模型表示的图像和稍后用于测试的图像。

我们首先加载 OpenCV 的数字图像，将其拆分为训练集和测试集，并使用方向梯度直方图（HOG）技术将其转换为特征向量：

Python

```py
from cv2 import imshow, waitKey
from digits_dataset import split_images, split_data
from feature_extraction import hog_descriptors

# Load the full training image
img, sub_imgs = split_images('Images/digits.png', 20)

# Check that the correct image has been loaded
imshow('Training image', img)
waitKey(0)

# Check that the sub-images have been correctly split
imshow('Sub-image', sub_imgs[0, 0, :, :].reshape(20, 20))
waitKey(0)

# Split the dataset into training and testing
train_imgs, train_labels, test_imgs, test_labels = split_data(20, sub_imgs, 0.5)

# Convert the training and testing images into feature vectors using the HOG technique
train_hog = hog_descriptors(train_imgs)
test_hog = hog_descriptors(test_imgs)
```

接下来，我们将初始化一个 kNN 分类器：

Python

```py
from cv2 import ml

knn = ml.KNearest_create()
```

然后在数据集的训练分割上“训练”它。对于数据集的训练分割，我们可以使用图像像素本身的强度值（类型转换为 32 位浮点值，以符合函数的预期输入）：

Python

```py
knn.train(float32(train_imgs), ml.ROW_SAMPLE, train_labels)
```

或者使用 HOG 技术生成的特征向量。在前一节中，我们提到 kNN 算法在处理高维数据时会遇到困难。使用 HOG 技术生成更紧凑的图像数据表示有助于缓解这个问题：

Python

```py
knn.train(train_hog, ml.ROW_SAMPLE, train_labels)
```

我们继续这个教程，利用 HOG 特征向量。

训练好的 kNN 分类器现在可以在数据集的测试分割上进行测试，然后通过计算与真实值匹配的正确预测的百分比来计算其准确性。暂时将 `k` 的值经验性地设置为 3：

Python

```py
from numpy import sum

k = 3
ret, result, neighbours, dist = knn.findNearest(test_hog, k)

accuracy = (sum(result == test_labels) / test_labels.size) * 100
```

然而，正如我们在前一节中提到的，通常做法是通过尝试不同的 *k* 值来调整，并查看哪种值最适合当前问题。我们还可以尝试使用不同的比例值拆分数据集，以观察它们对预测准确性的影响。

为此，我们将把 kNN 分类器代码放入一个嵌套的 `for` 循环中，其中外部循环迭代不同的比率值，而内部循环迭代不同的 *k* 值。在内部循环中，我们还将填充一个字典，记录计算出的准确度值，以便稍后使用 Matplotlib 绘制它们。

我们将包括的最后一个细节是检查我们是否正确加载了图像并将其正确拆分为子图像。为此，我们将使用 OpenCV 的 `imshow` 方法来显示图像，然后使用输入为零的 `waitKey` 方法来停止并等待键盘事件：

Python

```py
from cv2 import imshow, waitKey, ml
from numpy import sum
from matplotlib.pyplot import plot, show, title, xlabel, ylabel, legend
from digits_dataset import split_images, split_data
from feature_extraction import hog_descriptors

# Load the full training image
img, sub_imgs = split_images('Images/digits.png', 20)

# Check that the correct image has been loaded
imshow('Training image', img)
waitKey(0)

# Check that the sub-images have been correctly split
imshow('Sub-image', sub_imgs[0, 0, :, :].reshape(20, 20))
waitKey(0)

# Define different training-testing splits
ratio = [0.5, 0.7, 0.9]

for i in ratio:

    # Split the dataset into training and testing
    train_imgs, train_labels, test_imgs, test_labels = split_data(20, sub_imgs, i)

    # Convert the training and testing images into feature vectors using the HOG technique
    train_hog = hog_descriptors(train_imgs)
    test_hog = hog_descriptors(test_imgs)

    # Initiate a kNN classifier and train it on the training data
    knn = ml.KNearest_create()
    knn.train(train_hog, ml.ROW_SAMPLE, train_labels)

    # Initiate a dictionary to hold the ratio and accuracy values
    accuracy_dict = {}

    # Populate the dictionary with the keys corresponding to the values of 'k'
    keys = range(3, 16)

    for k in keys:

        # Test the kNN classifier on the testing data
        ret, result, neighbours, dist = knn.findNearest(test_hog, k)

        # Compute the accuracy and print it
        accuracy = (sum(result == test_labels) / test_labels.size) * 100
        print("Accuracy: {0:.2f}%, Training: {1:.0f}%, k: {2}".format(accuracy, i*100, k))

        # Populate the dictionary with the values corresponding to the accuracy
        accuracy_dict[k] = accuracy

    # Plot the accuracy values against the value of 'k'
    plot(accuracy_dict.keys(), accuracy_dict.values(), marker='o', label=str(i * 100) + '%')
    title('Accuracy of the k-nearest neighbors model')
    xlabel('k')
    ylabel('Accuracy')
    legend(loc='upper right')

show()
```

绘制不同比率值和不同 *k* 值下计算出的预测准确度，可以更好地了解这些不同值对特定应用中预测准确度的影响：

[![](../Images/0c6a71bb4b294afaeede172838c4ca62.png)](https://machinelearningmastery.com/wp-content/uploads/2023/01/kNN.png)

不同训练数据拆分和不同 ‘k’ 值下的预测准确度线图

尝试使用不同的图像描述符，并调整所选算法的不同参数，然后将数据输入到 kNN 算法中，并调查你更改所导致的 kNN 输出。

### 想要开始使用 OpenCV 进行机器学习吗？

现在立即报名参加我的免费邮件速成课程（附带示例代码）。

点击注册并获取课程的免费 PDF 电子书版本。

## **进一步阅读**

本节提供了更多关于该主题的资源，如果你想深入了解。

### **书籍**

+   [掌握 OpenCV 4 与 Python](https://www.amazon.com/Mastering-OpenCV-Python-practical-processing/dp/1789344913)，2019 年。

### **网站**

+   OpenCV，[https://opencv.org/](https://opencv.org/)

+   OpenCV KNearest 类， [https://docs.opencv.org/4.7.0/dd/de1/classcv_1_1ml_1_1KNearest.html](https://docs.opencv.org/4.7.0/dd/de1/classcv_1_1ml_1_1KNearest.html)

## **总结**

在本教程中，你学会了如何应用 OpenCV 的 k-最近邻算法来分类手写数字。

具体来说，你学到了：

+   k-最近邻算法的一些重要特性。

+   如何在 OpenCV 中使用 k-最近邻算法进行图像分类。

你有任何问题吗？

在下面的评论中提出你的问题，我会尽力回答。**************
