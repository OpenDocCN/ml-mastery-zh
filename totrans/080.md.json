["```py\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dense, AveragePooling2D, Dropout, Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Load MNIST data\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n# Reshape data to shape of (n_sample, height, width, n_channel)\nX_train = np.expand_dims(X_train, axis=3).astype('float32')\nX_test = np.expand_dims(X_test, axis=3).astype('float32')\n\n# LeNet5 model: ReLU can be used intead of tanh\nmodel = Sequential([\n    Conv2D(6, (5,5), input_shape=(28,28,1), padding=\"same\", activation=\"tanh\"),\n    AveragePooling2D((2,2), strides=2),\n    Conv2D(16, (5,5), activation=\"tanh\"),\n    AveragePooling2D((2,2), strides=2),\n    Conv2D(120, (5,5), activation=\"tanh\"),\n    Flatten(),\n    Dense(84, activation=\"tanh\"),\n    Dense(10, activation=\"softmax\")\n])\n\n# Training\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"sparse_categorical_accuracy\"])\nearlystopping = EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[earlystopping])\n```", "```py\nmodel.save(\"lenet5-mnist.h5\")\n```", "```py\nprint(np.argmax(model.predict(X_test), axis=1))\nprint(y_test)\n```", "```py\n[7 2 1 ... 4 5 6]\n[7 2 1 ... 4 5 6]\n```", "```py\n# tflite conversion with dynamic range optimization\nimport tensorflow as tf\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_model = converter.convert()\n\n# Optional: Save the data for testing\nimport numpy as np\nnp.savez('mnist-test.npz', X=X_test, y=y_test)\n\n# Save the model.\nwith open('lenet5-mnist.tflite', 'wb') as f:\n    f.write(tflite_model)\n```", "```py\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\nloaded = np.load('mnist-test.npz')\nX_test = loaded[\"X\"]\ny_test = loaded[\"y\"]\ninterpreter = tflite.Interpreter(model_path=\"lenet5-mnist.tflite\")\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nprint(input_details[0]['shape'])\n\nrows = []\nfor n in range(len(X_test)):\n    # this model has single input and single output\n    interpreter.set_tensor(input_details[0]['index'], X_test[n:n+1])\n    interpreter.invoke()\n    row = interpreter.get_tensor(output_details[0]['index'])\n    rows.append(row)\nrows = np.vstack(rows)\n\naccuracy = np.sum(np.argmax(rows, axis=1) == y_test) / len(y_test)\nprint(accuracy)\n```", "```py\nimport numpy as np\nimport tensorflow as tf\n\ninterpreter = tf.lite.Interpreter(model_path=\"lenet5-mnist.tflite\")\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nrows = []\nfor n in range(len(X_test)):\n    # this model has single input and single output\n    interpreter.set_tensor(input_details[0]['index'], X_test[n:n+1])\n    interpreter.invoke()\n    row = interpreter.get_tensor(output_details[0]['index'])\n    rows.append(row)\nrows = np.vstack(rows)\n\naccuracy = np.sum(np.argmax(rows, axis=1) == y_test) / len(y_test)\nprint(accuracy)\n```", "```py\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dense, AveragePooling2D, Dropout, Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Load MNIST data\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n# Reshape data to shape of (n_sample, height, width, n_channel)\nX_train = np.expand_dims(X_train, axis=3).astype('float32')\nX_test = np.expand_dims(X_test, axis=3).astype('float32')\n\n# LeNet5 model: ReLU can be used intead of tanh\nmodel = Sequential([\n    Conv2D(6, (5,5), input_shape=(28,28,1), padding=\"same\", activation=\"tanh\"),\n    AveragePooling2D((2,2), strides=2),\n    Conv2D(16, (5,5), activation=\"tanh\"),\n    AveragePooling2D((2,2), strides=2),\n    Conv2D(120, (5,5), activation=\"tanh\"),\n    Flatten(),\n    Dense(84, activation=\"tanh\"),\n    Dense(10, activation=\"softmax\")\n])\n\n# Training\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"sparse_categorical_accuracy\"])\nearlystopping = EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[earlystopping])\n\n# Save model\nmodel.save(\"lenet5-mnist.h5\")\n\n# Compare the prediction vs test data\nprint(np.argmax(model.predict(X_test), axis=1))\nprint(y_test)\n\n# tflite conversion with dynamic range optimization\nimport tensorflow as tf\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_model = converter.convert()\n\n# Optional: Save the data for testing\nimport numpy as np\nnp.savez('mnist-test.npz', X=X_test, y=y_test)\n\n# Save the tflite model.\nwith open('lenet5-mnist.tflite', 'wb') as f:\n    f.write(tflite_model)\n\n# Load the tflite model and run test\ninterpreter = tf.lite.Interpreter(model_path=\"lenet5-mnist.tflite\")\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nrows = []\nfor n in range(len(X_test)):\n    # this model has single input and single output\n    interpreter.set_tensor(input_details[0]['index'], X_test[n:n+1])\n    interpreter.invoke()\n    row = interpreter.get_tensor(output_details[0]['index'])\n    rows.append(row)\nrows = np.vstack(rows)\n\naccuracy = np.sum(np.argmax(rows, axis=1) == y_test) / len(y_test)\nprint(accuracy)\n```", "```py\n...\ninterpreter.predict()\n```", "```py\nTraceback (most recent call last):\n  File \"/Users/MLM/pred_error.py\", line 13, in <module>\n    interpreter.predict()\nAttributeError: 'Interpreter' object has no attribute 'predict'\n```", "```py\n...\n\n# Monkey patching the tflite model\ndef predict(self, input_batch):\n    batch_size = len(input_batch)\n    output = []\n\n    input_details = self.get_input_details()\n    output_details = self.get_output_details()\n    # Run each sample from the batch\n    for sample in range(batch_size):\n        self.set_tensor(input_details[0][\"index\"], input_batch[sample:sample+1])\n        self.invoke()\n        sample_output = self.get_tensor(output_details[0][\"index\"])\n        output.append(sample_output)\n\n    # vstack the output of each sample\n    return np.vstack(output)\n\ninterpreter.predict = predict.__get__(interpreter)\n```", "```py\n...\nout_proba = interpreter.predict(X_test)\nout = np.argmax(out_proba, axis=1)\nprint(out)\n\naccuracy = np.sum(out == y_test) / len(y_test)\nprint(accuracy)\n```", "```py\n[7 2 1 ... 4 5 6]\n0.9879\n```", "```py\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\n\n# Load MNIST data and reshape\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = np.expand_dims(X_train, axis=3).astype('float32')\nX_test = np.expand_dims(X_test, axis=3).astype('float32')\n\n# Monkey patching the tflite model\ndef predict(self, input_batch):\n    batch_size = len(input_batch)\n    output = []\n\n    input_details = self.get_input_details()\n    output_details = self.get_output_details()\n    # Run each sample from the batch\n    for sample in range(batch_size):\n        self.set_tensor(input_details[0][\"index\"], input_batch[sample:sample+1])\n        self.invoke()\n        sample_output = self.get_tensor(output_details[0][\"index\"])\n        output.append(sample_output)\n\n    # vstack the output of each sample\n    return np.vstack(output)\n\n# Load and monkey patch\ninterpreter = tf.lite.Interpreter(model_path=\"lenet5-mnist.tflite\")\ninterpreter.predict = predict.__get__(interpreter)\ninterpreter.allocate_tensors()\n\n# test output\nout_proba = interpreter.predict(X_test)\nout = np.argmax(out_proba, axis=1)\nprint(out)\naccuracy = np.sum(out == y_test) / len(y_test)\nprint(accuracy)\n```", "```py\n# https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n# Example of Dropout on the Sonar Dataset: Hidden Layer\nfrom pandas import read_csv\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.constraints import maxnorm\nfrom keras.optimizers import SGD\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n# load dataset\ndataframe = read_csv(\"sonar.csv\", header=None)\ndataset = dataframe.values\n# split into input (X) and output (Y) variables\nX = dataset[:,0:60].astype(float)\nY = dataset[:,60]\n# encode class values as integers\nencoder = LabelEncoder()\nencoder.fit(Y)\nencoded_Y = encoder.transform(Y)\n\n# dropout in hidden layers with weight constraint\ndef create_model():\n\t# create model\n\tmodel = Sequential()\n\tmodel.add(Dense(60, input_dim=60, activation='relu', kernel_constraint=maxnorm(3)))\n\tmodel.add(Dropout(0.2))\n\tmodel.add(Dense(30, activation='relu', kernel_constraint=maxnorm(3)))\n\tmodel.add(Dropout(0.2))\n\tmodel.add(Dense(1, activation='sigmoid'))\n\t# Compile model\n\tsgd = SGD(lr=0.1, momentum=0.9)\n\tmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\treturn model\n\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=300, batch_size=16, verbose=0)))\npipeline = Pipeline(estimators)\nkfold = StratifiedKFold(n_splits=10, shuffle=True)\nresults = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\nprint(\"Hidden: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n```", "```py\n# Example of Dropout on the Sonar Dataset: Hidden Layer\nfrom pandas import read_csv\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras.constraints import MaxNorm as maxnorm\nfrom tensorflow.keras.optimizers import SGD\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n# load dataset\ndataframe = read_csv(\"sonar.csv\", header=None)\ndataset = dataframe.values\n# split into input (X) and output (Y) variables\nX = dataset[:,0:60].astype(float)\nY = dataset[:,60]\n# encode class values as integers\nencoder = LabelEncoder()\nencoder.fit(Y)\nencoded_Y = encoder.transform(Y)\n\n# dropout in hidden layers with weight constraint\ndef create_model():\n\t# create model\n\tmodel = Sequential()\n\tmodel.add(Dense(60, input_dim=60, activation='relu', kernel_constraint=maxnorm(3)))\n\tmodel.add(Dropout(0.2))\n\tmodel.add(Dense(30, activation='relu', kernel_constraint=maxnorm(3)))\n\tmodel.add(Dropout(0.2))\n\tmodel.add(Dense(1, activation='sigmoid'))\n\t# Compile model\n\tsgd = SGD(lr=0.1, momentum=0.9)\n\tmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\treturn model\n\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=300, batch_size=16, verbose=0)))\npipeline = Pipeline(estimators)\nkfold = StratifiedKFold(n_splits=10, shuffle=True)\nresults = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\nprint(\"Hidden: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n```", "```py\n# monkey patching\nimport sys\nimport tensorflow.keras\ntensorflow.keras.constraints.maxnorm = tensorflow.keras.constraints.MaxNorm\nfor x in sys.modules.keys():\n    if x.startswith(\"tensorflow.keras\"):\n        sys.modules[x[len(\"tensorflow.\"):]] = sys.modules[x]\n\n# Old code below:\n\n# Example of Dropout on the Sonar Dataset: Hidden Layer\nfrom pandas import read_csv\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.constraints import maxnorm\nfrom keras.optimizers import SGD\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n# load dataset\ndataframe = read_csv(\"sonar.csv\", header=None)\ndataset = dataframe.values\n# split into input (X) and output (Y) variables\nX = dataset[:,0:60].astype(float)\nY = dataset[:,60]\n# encode class values as integers\nencoder = LabelEncoder()\nencoder.fit(Y)\nencoded_Y = encoder.transform(Y)\n\n# dropout in hidden layers with weight constraint\ndef create_model():\n\t# create model\n\tmodel = Sequential()\n\tmodel.add(Dense(60, input_dim=60, activation='relu', kernel_constraint=maxnorm(3)))\n\tmodel.add(Dropout(0.2))\n\tmodel.add(Dense(30, activation='relu', kernel_constraint=maxnorm(3)))\n\tmodel.add(Dropout(0.2))\n\tmodel.add(Dense(1, activation='sigmoid'))\n\t# Compile model\n\tsgd = SGD(lr=0.1, momentum=0.9)\n\tmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\treturn model\n\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=300, batch_size=16, verbose=0)))\npipeline = Pipeline(estimators)\nkfold = StratifiedKFold(n_splits=10, shuffle=True)\nresults = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\nprint(\"Hidden: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n```"]