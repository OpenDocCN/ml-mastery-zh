["```py\nfrom tensorflow import math, cast, float32\n\ndef padding_mask(input):\n    # Create mask which marks the zero padding values in the input by a 1\n    mask = math.equal(input, 0)\n    mask = cast(mask, float32)\n\n    return mask\n```", "```py\nfrom numpy import array\n\ninput = array([1, 2, 3, 4, 0, 0, 0])\nprint(padding_mask(input))\n```", "```py\ntf.Tensor([0\\. 0\\. 0\\. 0\\. 1\\. 1\\. 1.], shape=(7,), dtype=float32)\n```", "```py\nfrom tensorflow import linalg, ones\n\ndef lookahead_mask(shape):\n    # Mask out future entries by marking them with a 1.0\n    mask = 1 - linalg.band_part(ones((shape, shape)), -1, 0)\n\n    return mask\n```", "```py\nprint(lookahead_mask(5))\n```", "```py\ntf.Tensor(\n[[0\\. 1\\. 1\\. 1\\. 1.]\n [0\\. 0\\. 1\\. 1\\. 1.]\n [0\\. 0\\. 0\\. 1\\. 1.]\n [0\\. 0\\. 0\\. 0\\. 1.]\n [0\\. 0\\. 0\\. 0\\. 0.]], shape=(5, 5), dtype=float32)\n```", "```py\nclass TransformerModel(Model):\n    def __init__(self, enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate, **kwargs):\n        super(TransformerModel, self).__init__(**kwargs)\n\n        # Set up the encoder\n        self.encoder = Encoder(enc_vocab_size, enc_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n\n        # Set up the decoder\n        self.decoder = Decoder(dec_vocab_size, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n\n        # Define the final dense layer\n        self.model_last_layer = Dense(dec_vocab_size)\n        ...\n```", "```py\n...\ndef call(self, encoder_input, decoder_input, training):\n\n    # Create padding mask to mask the encoder inputs and the encoder outputs in the decoder\n    enc_padding_mask = self.padding_mask(encoder_input)\n...\n```", "```py\n...\n# Create and combine padding and look-ahead masks to be fed into the decoder\ndec_in_padding_mask = self.padding_mask(decoder_input)\ndec_in_lookahead_mask = self.lookahead_mask(decoder_input.shape[1])\ndec_in_lookahead_mask = maximum(dec_in_padding_mask, dec_in_lookahead_mask)\n...\n```", "```py\n...\n# Feed the input into the encoder\nencoder_output = self.encoder(encoder_input, enc_padding_mask, training)\n\n# Feed the encoder output into the decoder\ndecoder_output = self.decoder(decoder_input, encoder_output, dec_in_lookahead_mask, enc_padding_mask, training)\n\n# Pass the decoder output through a final dense layer\nmodel_output = self.model_last_layer(decoder_output)\n\nreturn model_output\n```", "```py\nfrom encoder import Encoder\nfrom decoder import Decoder\nfrom tensorflow import math, cast, float32, linalg, ones, maximum, newaxis\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense\n\nclass TransformerModel(Model):\n    def __init__(self, enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate, **kwargs):\n        super(TransformerModel, self).__init__(**kwargs)\n\n        # Set up the encoder\n        self.encoder = Encoder(enc_vocab_size, enc_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n\n        # Set up the decoder\n        self.decoder = Decoder(dec_vocab_size, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n\n        # Define the final dense layer\n        self.model_last_layer = Dense(dec_vocab_size)\n\n    def padding_mask(self, input):\n        # Create mask which marks the zero padding values in the input by a 1.0\n        mask = math.equal(input, 0)\n        mask = cast(mask, float32)\n\n        # The shape of the mask should be broadcastable to the shape\n        # of the attention weights that it will be masking later on\n        return mask[:, newaxis, newaxis, :]\n\n    def lookahead_mask(self, shape):\n        # Mask out future entries by marking them with a 1.0\n        mask = 1 - linalg.band_part(ones((shape, shape)), -1, 0)\n\n        return mask\n\n    def call(self, encoder_input, decoder_input, training):\n\n        # Create padding mask to mask the encoder inputs and the encoder outputs in the decoder\n        enc_padding_mask = self.padding_mask(encoder_input)\n\n        # Create and combine padding and look-ahead masks to be fed into the decoder\n        dec_in_padding_mask = self.padding_mask(decoder_input)\n        dec_in_lookahead_mask = self.lookahead_mask(decoder_input.shape[1])\n        dec_in_lookahead_mask = maximum(dec_in_padding_mask, dec_in_lookahead_mask)\n\n        # Feed the input into the encoder\n        encoder_output = self.encoder(encoder_input, enc_padding_mask, training)\n\n        # Feed the encoder output into the decoder\n        decoder_output = self.decoder(decoder_input, encoder_output, dec_in_lookahead_mask, enc_padding_mask, training)\n\n        # Pass the decoder output through a final dense layer\n        model_output = self.model_last_layer(decoder_output)\n\n        return model_output\n```", "```py\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nd_model = 512  # Dimensionality of the model sub-layers' outputs\nn = 6  # Number of layers in the encoder stack\n\ndropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n...\n```", "```py\n...\nenc_vocab_size = 20 # Vocabulary size for the encoder\ndec_vocab_size = 20 # Vocabulary size for the decoder\n\nenc_seq_length = 5  # Maximum length of the input sequence\ndec_seq_length = 5  # Maximum length of the target sequence\n...\n```", "```py\nfrom model import TransformerModel\n\n# Create model\ntraining_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n```", "```py\nenc_vocab_size = 20 # Vocabulary size for the encoder\ndec_vocab_size = 20 # Vocabulary size for the decoder\n\nenc_seq_length = 5  # Maximum length of the input sequence\ndec_seq_length = 5  # Maximum length of the target sequence\n\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nd_model = 512  # Dimensionality of the model sub-layers' outputs\nn = 6  # Number of layers in the encoder stack\n\ndropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n\n# Create model\ntraining_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n```", "```py\nself.build(input_shape=[None, sequence_length, d_model])\n```", "```py\ndef build_graph(self):\n    input_layer = Input(shape=(self.sequence_length, self.d_model))\n    return Model(inputs=[input_layer], outputs=self.call(input_layer, None, True))\n```", "```py\ndef build_graph(self):\n    input_layer = Input(shape=(self.sequence_length, self.d_model))\n    return Model(inputs=[input_layer], outputs=self.call(input_layer, input_layer, None, None, True))\n```", "```py\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras import Model\n\nclass EncoderLayer(Layer):\n    def __init__(self, sequence_length, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n        super(EncoderLayer, self).__init__(**kwargs)\n        self.build(input_shape=[None, sequence_length, d_model])\n        self.d_model = d_model\n        self.sequence_length = sequence_length\n        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n        self.dropout1 = Dropout(rate)\n        self.add_norm1 = AddNormalization()\n        self.feed_forward = FeedForward(d_ff, d_model)\n        self.dropout2 = Dropout(rate)\n        self.add_norm2 = AddNormalization()\n\n    def build_graph(self):\n        input_layer = Input(shape=(self.sequence_length, self.d_model))\n        return Model(inputs=[input_layer], outputs=self.call(input_layer, None, True))\n\n    def call(self, x, padding_mask, training):\n        ...\n```", "```py\nfrom encoder import EncoderLayer\nfrom decoder import DecoderLayer\n\nencoder = EncoderLayer(enc_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\nencoder.build_graph().summary()\n\ndecoder = DecoderLayer(dec_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\ndecoder.build_graph().summary()\n```", "```py\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 5, 512)]     0           []                               \n\n multi_head_attention_18 (Multi  (None, 5, 512)      131776      ['input_1[0][0]',                \n HeadAttention)                                                   'input_1[0][0]',                \n                                                                  'input_1[0][0]']                \n\n dropout_32 (Dropout)           (None, 5, 512)       0           ['multi_head_attention_18[0][0]']\n\n add_normalization_30 (AddNorma  (None, 5, 512)      1024        ['input_1[0][0]',                \n lization)                                                        'dropout_32[0][0]']             \n\n feed_forward_12 (FeedForward)  (None, 5, 512)       2099712     ['add_normalization_30[0][0]']   \n\n dropout_33 (Dropout)           (None, 5, 512)       0           ['feed_forward_12[0][0]']        \n\n add_normalization_31 (AddNorma  (None, 5, 512)      1024        ['add_normalization_30[0][0]',   \n lization)                                                        'dropout_33[0][0]']             \n\n==================================================================================================\nTotal params: 2,233,536\nTrainable params: 2,233,536\nNon-trainable params: 0\n__________________________________________________________________________________________________\n```", "```py\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_2 (InputLayer)           [(None, 5, 512)]     0           []                               \n\n multi_head_attention_19 (Multi  (None, 5, 512)      131776      ['input_2[0][0]',                \n HeadAttention)                                                   'input_2[0][0]',                \n                                                                  'input_2[0][0]']                \n\n dropout_34 (Dropout)           (None, 5, 512)       0           ['multi_head_attention_19[0][0]']\n\n add_normalization_32 (AddNorma  (None, 5, 512)      1024        ['input_2[0][0]',                \n lization)                                                        'dropout_34[0][0]',             \n                                                                  'add_normalization_32[0][0]',   \n                                                                  'dropout_35[0][0]']             \n\n multi_head_attention_20 (Multi  (None, 5, 512)      131776      ['add_normalization_32[0][0]',   \n HeadAttention)                                                   'input_2[0][0]',                \n                                                                  'input_2[0][0]']                \n\n dropout_35 (Dropout)           (None, 5, 512)       0           ['multi_head_attention_20[0][0]']\n\n feed_forward_13 (FeedForward)  (None, 5, 512)       2099712     ['add_normalization_32[1][0]']   \n\n dropout_36 (Dropout)           (None, 5, 512)       0           ['feed_forward_13[0][0]']        \n\n add_normalization_34 (AddNorma  (None, 5, 512)      1024        ['add_normalization_32[1][0]',   \n lization)                                                        'dropout_36[0][0]']             \n\n==================================================================================================\nTotal params: 2,365,312\nTrainable params: 2,365,312\nNon-trainable params: 0\n__________________________________________________________________________________________________\n```"]