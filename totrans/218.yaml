- en: 'Method of Lagrange Multipliers: The Theory Behind Support Vector Machines (Part
    1: The Separable Case)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://machinelearningmastery.com/method-of-lagrange-multipliers-the-theory-behind-support-vector-machines-part-1-the-separable-case/](https://machinelearningmastery.com/method-of-lagrange-multipliers-the-theory-behind-support-vector-machines-part-1-the-separable-case/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This tutorial is designed for anyone looking for a deeper understanding of how
    Lagrange multipliers are used in building up the model for support vector machines
    (SVMs). SVMs were initially designed to solve binary classification problems and
    later extended and applied to regression and unsupervised learning. They have
    shown their success in solving many complex machine learning classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we’ll look at the simplest SVM that assumes that the positive
    and negative examples can be completely separated via a linear hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: 'After completing this tutorial, you will know:'
  prefs: []
  type: TYPE_NORMAL
- en: How the hyperplane acts as the decision boundary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mathematical constraints on the positive and negative examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the margin and how to maximize the margin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Role of Lagrange multipliers in maximizing the margin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to determine the separating hyperplane for the separable case
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/6f93676eab3bacf5325f9f146d8e677b.png)](https://machinelearningmastery.com/wp-content/uploads/2021/11/IMG_9900-scaled.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Method Of Lagrange Multipliers: The Theory Behind Support Vector Machines (Part
    1: The separable case)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by Mehreen Saeed, some rights reserved.
  prefs: []
  type: TYPE_NORMAL
- en: 'This tutorial is divided into three parts; they are:'
  prefs: []
  type: TYPE_NORMAL
- en: Formulation of the mathematical model of SVM
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solution of finding the maximum margin hyperplane via the method of Lagrange
    multipliers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solved example to demonstrate all concepts
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Notations Used In This Tutorial
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '$m$: Total training points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$n$: Total features or the dimensionality of all data points'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$x$: Data point, which is an n-dimensional vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$x^+$: Data point labelled as +1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$x^-$: Data point labelled as -1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$i$: Subscript used to index the training points. $0 \leq i < m$'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$j$: Subscript used to index the individual dimension of a data point. $1 \leq
    j \leq n$'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$t$: Label of a data point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'T: Transpose operator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$w$: Weight vector denoting the coefficients of the hyperplane. It is also
    an n-dimensional vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$\alpha$: Lagrange multipliers, one per each training point. This is an m-dimensional
    vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$d$: Perpendicular distance of a data point from the decision boundary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Hyperplane As The Decision Boundary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[![](../Images/7db5f534c187624cfb49d8de0e2da574.png)](https://machinelearningmastery.com/wp-content/uploads/2021/11/intro1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The support vector machine is designed to discriminate data points belonging
    to two different classes. One set of points is labelled as +1 also called the
    positive class. The other set of points is labeled as -1 also called the negative
    class. For now, we’ll make a simplifying assumption that points from both classes
    can be discriminated via linear hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: The SVM assumes a linear decision boundary between the two classes and the goal
    is to find a hyperplane that gives the maximum separation between the two classes.
    For this reason, the alternate term `maximum margin classifier` is also sometimes
    used to refer to an SVM. The perpendicular distance between the closest data point
    and the decision boundary is referred to as the `margin`. As the margin completely
    separates the positive and negative examples and does not tolerate any errors,
    it is also called the `hard margin`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical expression for a hyperplane is given below with \(w_j\) being
    the coefficients and \(w_0\) being the arbitrary constant that determines the
    distance of the hyperplane from the origin:'
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: w^T x_i + w_0 = 0
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: 'For the ith 2-dimensional point $(x_{i1}, x_{i2})$ the above expression is
    reduced to:'
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: w_1x_{i1} + w_2 x_{i2} + w_0 = 0
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical Constraints On Positive and Negative Data Points
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we are looking to maximize the margin between positive and negative data
    points, we would like the positive data points to satisfy the following constraint:'
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: w^T x_i^+ + w_0 \geq +1
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the negative data points should satisfy:'
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: w^T x_i^- + w_0 \leq -1
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use a neat trick to write a uniform equation for both set of points
    by using $t_i \in \{-1,+1\}$ to denote the class label of data point $x_i$:'
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: t_i(w^T x_i + w_0) \geq +1
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: The Maximum Margin Hyperplane
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The perpendicular distance $d_i$ of a data point $x_i$ from the margin is given
    by:'
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: d_i = \frac{|w^T x_i + w_0|}{||w||}
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: 'To maximize this distance, we can minimize the square of the denominator to
    give us a quadratic programming problem given by:'
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: \min \frac{1}{2}||w||^2 \;\text{ subject to } t_i(w^Tx_i+w_0) \geq +1, \forall
    i
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: Solution Via The Method Of Lagrange Multipliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To solve the above quadratic programming problem with inequality constraints,
    we can use the method of Lagrange multipliers. The Lagrange function is therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: L(w, w_0, \alpha) = \frac{1}{2}||w||^2 + \sum_i \alpha_i\big(t_i(w^Tx_i+w_0)
    – 1\big)
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve the above, we set the following:'
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation}
  prefs: []
  type: TYPE_NORMAL
- en: \frac{\partial L}{ \partial w} = 0, \\
  prefs: []
  type: TYPE_NORMAL
- en: \frac{\partial L}{ \partial \alpha} = 0, \\
  prefs: []
  type: TYPE_NORMAL
- en: \frac{\partial L}{ \partial w_0} = 0 \\
  prefs: []
  type: TYPE_NORMAL
- en: \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: 'Plugging above in the Lagrange function gives us the following optimization
    problem, also called the dual:'
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: L_d = -\frac{1}{2} \sum_i \sum_k \alpha_i \alpha_k t_i t_k (x_i)^T (x_k) + \sum_i
    \alpha_i
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: 'We have to maximize the above subject to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: w = \sum_i \alpha_i t_i x_i
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: 0=\sum_i \alpha_i t_i
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: The nice thing about the above is that we have an expression for \(w\) in terms
    of Lagrange multipliers. The objective function involves no $w$ term. There is
    a Lagrange multiplier associated with each data point. The computation of $w_0$
    is also explained later.
  prefs: []
  type: TYPE_NORMAL
- en: Deciding The Classification of a Test Point
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The classification of any test point $x$ can be determined using this expression:'
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: y(x) = \sum_i \alpha_i t_i x^T x_i + w_0
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: A positive value of $y(x)$ implies $x\in+1$ and a negative value means $x\in-1$
  prefs: []
  type: TYPE_NORMAL
- en: Want to Get Started With Calculus for Machine Learning?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Take my free 7-day email crash course now (with sample code).
  prefs: []
  type: TYPE_NORMAL
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  prefs: []
  type: TYPE_NORMAL
- en: Karush-Kuhn-Tucker Conditions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Also, Karush-Kuhn-Tucker (KKT) conditions are satisfied by the above constrained
    optimization problem as given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \begin{eqnarray}
  prefs: []
  type: TYPE_NORMAL
- en: \alpha_i &\geq& 0 \\
  prefs: []
  type: TYPE_NORMAL
- en: t_i y(x_i) -1 &\geq& 0 \\
  prefs: []
  type: TYPE_NORMAL
- en: \alpha_i(t_i y(x_i) -1) &=& 0
  prefs: []
  type: TYPE_NORMAL
- en: \end{eqnarray}
  prefs: []
  type: TYPE_NORMAL
- en: Interpretation Of KKT Conditions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The KKT conditions dictate that for each data point one of the following is
    true:'
  prefs: []
  type: TYPE_NORMAL
- en: The Lagrange multiplier is zero, i.e., \(\alpha_i=0\). This point, therefore,
    plays no role in classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OR
  prefs: []
  type: TYPE_NORMAL
- en: '$ t_i y(x_i) = 1$ and $\alpha_i > 0$: In this case, the data point has a role
    in deciding the value of $w$. Such a point is called a support vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing w_0
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For $w_0$, we can select any support vector $x_s$ and solve
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: t_s y(x_s) = 1
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: 'giving us:'
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: t_s(\sum_i \alpha_i t_i x_s^T x_i + w_0) = 1
  prefs: []
  type: TYPE_NORMAL
- en: $$
  prefs: []
  type: TYPE_NORMAL
- en: A Solved Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To help you understand the above concepts, here is a simple arbitrarily solved
    example. Of course, for a large number of points you would use an optimization
    software to solve this. Also, this is one possible solution that satisfies all
    the constraints. The objective function can be maximized further but the slope
    of the hyperplane will remain the same for an optimal solution. Also, for this
    example, $w_0$ was computed by taking the average of $w_0$ from all three support
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: This example will show you that the model is not as complex as it looks.[![](../Images/8c090023e87641c9d86abd51936e9465.png)](https://machinelearningmastery.com/wp-content/uploads/2021/11/intro2-1.png)
  prefs: []
  type: TYPE_NORMAL
- en: For the above set of points, we can see that (1,2), (2,1) and (0,0) are points
    closest to the separating hyperplane and hence, act as support vectors. Points
    far away from the boundary (e.g. (-3,1)) do not play any role in determining the
    classification of the points.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section provides more resources on the topic if you are looking to go deeper.
  prefs: []
  type: TYPE_NORMAL
- en: Books
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Pattern Recognition and Machine Learning](https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738)
    by Christopher M. Bishop'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Thomas’ Calculus](https://amzn.to/35Yeolv), 14th edition, 2017  (based on
    the original works of George B. Thomas, revised by Joel Hass, Christopher Heil,
    Maurice Weir)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Articles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Support Vector Machines for Machine Learning](https://machinelearningmastery.com/support-vector-machines-for-machine-learning/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Tutorial on Support Vector Machines for Pattern Recognition](https://www.di.ens.fr/~mallat/papiers/svmtutorial.pdf)
    by Christopher J.C. Burges'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this tutorial, you discovered how to use the method of Lagrange multipliers
    to solve the problem of maximizing the margin via a quadratic programming problem
    with inequality constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, you learned:'
  prefs: []
  type: TYPE_NORMAL
- en: The mathematical expression for a separating linear hyperplane
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum margin as a solution of a quadratic programming problem with inequality
    constraint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to find a linear hyperplane between positive and negative examples using
    the method of Lagrange multipliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you have any questions about the SVM discussed in this post? Ask your questions
    in the comments below and I will do my best to answer.
  prefs: []
  type: TYPE_NORMAL
