- en: 'Method of Lagrange Multipliers: The Theory Behind Support Vector Machines (Part
    1: The Separable Case)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拉格朗日乘子法：支持向量机背后的理论（第1部分：可分离情况）
- en: 原文：[https://machinelearningmastery.com/method-of-lagrange-multipliers-the-theory-behind-support-vector-machines-part-1-the-separable-case/](https://machinelearningmastery.com/method-of-lagrange-multipliers-the-theory-behind-support-vector-machines-part-1-the-separable-case/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/method-of-lagrange-multipliers-the-theory-behind-support-vector-machines-part-1-the-separable-case/](https://machinelearningmastery.com/method-of-lagrange-multipliers-the-theory-behind-support-vector-machines-part-1-the-separable-case/)
- en: This tutorial is designed for anyone looking for a deeper understanding of how
    Lagrange multipliers are used in building up the model for support vector machines
    (SVMs). SVMs were initially designed to solve binary classification problems and
    later extended and applied to regression and unsupervised learning. They have
    shown their success in solving many complex machine learning classification problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程旨在为任何希望深入理解拉格朗日乘子如何在支持向量机（SVM）模型构建中使用的人提供指导。SVM最初设计用于解决二分类问题，后来扩展并应用于回归和无监督学习。它们在解决许多复杂的机器学习分类问题上取得了成功。
- en: In this tutorial, we’ll look at the simplest SVM that assumes that the positive
    and negative examples can be completely separated via a linear hyperplane.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将讨论最简单的SVM，假设正例和负例可以通过线性超平面完全分开。
- en: 'After completing this tutorial, you will know:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本教程后，你将了解：
- en: How the hyperplane acts as the decision boundary
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超平面如何作为决策边界
- en: Mathematical constraints on the positive and negative examples
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对正例和负例的数学约束
- en: What is the margin and how to maximize the margin
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是边际以及如何最大化边际
- en: Role of Lagrange multipliers in maximizing the margin
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉格朗日乘子在最大化边际中的作用
- en: How to determine the separating hyperplane for the separable case
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何确定可分离情况下的分离超平面
- en: Let’s get started.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: '[![](../Images/6f93676eab3bacf5325f9f146d8e677b.png)](https://machinelearningmastery.com/wp-content/uploads/2021/11/IMG_9900-scaled.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/6f93676eab3bacf5325f9f146d8e677b.png)](https://machinelearningmastery.com/wp-content/uploads/2021/11/IMG_9900-scaled.jpg)'
- en: 'Method Of Lagrange Multipliers: The Theory Behind Support Vector Machines (Part
    1: The separable case)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 拉格朗日乘子法：支持向量机背后的理论（第1部分：可分离情况）
- en: Photo by Mehreen Saeed, some rights reserved.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由Mehreen Saeed拍摄，部分权利保留。
- en: 'This tutorial is divided into three parts; they are:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程分为三个部分，它们是：
- en: Formulation of the mathematical model of SVM
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SVM的数学模型的公式化
- en: Solution of finding the maximum margin hyperplane via the method of Lagrange
    multipliers
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过拉格朗日乘子法寻找最大边际超平面的解
- en: Solved example to demonstrate all concepts
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解决的示例以演示所有概念
- en: Notations Used In This Tutorial
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本教程中使用的符号
- en: '$m$: Total training points.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $m$：总训练点数。
- en: '$n$: Total features or the dimensionality of all data points'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $n$：所有数据点的特征总数或维度
- en: '$x$: Data point, which is an n-dimensional vector.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $x$：数据点，是一个n维向量。
- en: '$x^+$: Data point labelled as +1.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $x^+$：标记为+1的数据点。
- en: '$x^-$: Data point labelled as -1.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $x^-$：标记为-1的数据点。
- en: '$i$: Subscript used to index the training points. $0 \leq i < m$'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $i$：用于索引训练点的下标。 $0 \leq i < m$
- en: '$j$: Subscript used to index the individual dimension of a data point. $1 \leq
    j \leq n$'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $j$：用于索引数据点的单独维度的下标。 $1 \leq j \leq n$
- en: '$t$: Label of a data point.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $t$：数据点的标签。
- en: 'T: Transpose operator.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T：转置算子。
- en: '$w$: Weight vector denoting the coefficients of the hyperplane. It is also
    an n-dimensional vector.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $w$：表示超平面系数的权重向量。它也是一个n维向量。
- en: '$\alpha$: Lagrange multipliers, one per each training point. This is an m-dimensional
    vector.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\alpha$：拉格朗日乘子，每个训练点一个。这是一个m维向量。
- en: '$d$: Perpendicular distance of a data point from the decision boundary.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $d$：数据点到决策边界的垂直距离。
- en: The Hyperplane As The Decision Boundary
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超平面作为决策边界
- en: '[![](../Images/7db5f534c187624cfb49d8de0e2da574.png)](https://machinelearningmastery.com/wp-content/uploads/2021/11/intro1.png)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/7db5f534c187624cfb49d8de0e2da574.png)](https://machinelearningmastery.com/wp-content/uploads/2021/11/intro1.png)'
- en: The support vector machine is designed to discriminate data points belonging
    to two different classes. One set of points is labelled as +1 also called the
    positive class. The other set of points is labeled as -1 also called the negative
    class. For now, we’ll make a simplifying assumption that points from both classes
    can be discriminated via linear hyperplane.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机旨在区分属于两个不同类别的数据点。一组点标记为+1，也称为正类。另一组点标记为-1，也称为负类。现在，我们将做一个简化假设，假设两个类别的点可以通过线性超平面进行区分。
- en: The SVM assumes a linear decision boundary between the two classes and the goal
    is to find a hyperplane that gives the maximum separation between the two classes.
    For this reason, the alternate term `maximum margin classifier` is also sometimes
    used to refer to an SVM. The perpendicular distance between the closest data point
    and the decision boundary is referred to as the `margin`. As the margin completely
    separates the positive and negative examples and does not tolerate any errors,
    it is also called the `hard margin`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: SVM假设两个类别之间有一个线性的决策边界，目标是找到一个超平面，使两个类别之间的分离最大。因此，有时使用术语`最大边界分类器`来指代SVM。最近的数据点与决策边界之间的垂直距离被称为`边界`。由于边界完全分隔了正负示例，并且不容忍任何错误，因此也被称为`硬边界`。
- en: 'The mathematical expression for a hyperplane is given below with \(w_j\) being
    the coefficients and \(w_0\) being the arbitrary constant that determines the
    distance of the hyperplane from the origin:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 超平面的数学表达式如下，其中\(w_j\)是系数，\(w_0\)是决定超平面距离原点的任意常数：
- en: $$
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: w^T x_i + w_0 = 0
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: w^T x_i + w_0 = 0
- en: $$
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: 'For the ith 2-dimensional point $(x_{i1}, x_{i2})$ the above expression is
    reduced to:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第$i$个二维点$(x_{i1}, x_{i2})$，上述表达式简化为：
- en: $$
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: w_1x_{i1} + w_2 x_{i2} + w_0 = 0
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: w_1x_{i1} + w_2 x_{i2} + w_0 = 0
- en: $$
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: Mathematical Constraints On Positive and Negative Data Points
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对正负数据点的数学约束
- en: 'As we are looking to maximize the margin between positive and negative data
    points, we would like the positive data points to satisfy the following constraint:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望最大化正负数据点之间的边界，因此我们希望正数据点满足以下约束：
- en: $$
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: w^T x_i^+ + w_0 \geq +1
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: w^T x_i^+ + w_0 \geq +1
- en: $$
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: 'Similarly, the negative data points should satisfy:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，负数据点应满足：
- en: $$
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: w^T x_i^- + w_0 \leq -1
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: w^T x_i^- + w_0 \leq -1
- en: $$
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: 'We can use a neat trick to write a uniform equation for both set of points
    by using $t_i \in \{-1,+1\}$ to denote the class label of data point $x_i$:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用$t_i \in \{-1,+1\}$来表示数据点$x_i$的类别标签，使用一个整齐的技巧来写出两个点集的统一方程：
- en: $$
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: t_i(w^T x_i + w_0) \geq +1
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: t_i(w^T x_i + w_0) \geq +1
- en: $$
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: The Maximum Margin Hyperplane
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大边界超平面
- en: 'The perpendicular distance $d_i$ of a data point $x_i$ from the margin is given
    by:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 数据点$x_i$到边界的垂直距离$d_i$由以下公式给出：
- en: $$
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: d_i = \frac{|w^T x_i + w_0|}{||w||}
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: d_i = \frac{|w^T x_i + w_0|}{||w||}
- en: $$
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: 'To maximize this distance, we can minimize the square of the denominator to
    give us a quadratic programming problem given by:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化这个距离，我们可以最小化分母的平方，从而得到一个二次规划问题：
- en: $$
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: \min \frac{1}{2}||w||^2 \;\text{ subject to } t_i(w^Tx_i+w_0) \geq +1, \forall
    i
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: \min \frac{1}{2}||w||^2 \;\text{ subject to } t_i(w^Tx_i+w_0) \geq +1, \forall
    i
- en: $$
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: Solution Via The Method Of Lagrange Multipliers
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过拉格朗日乘子法的解
- en: 'To solve the above quadratic programming problem with inequality constraints,
    we can use the method of Lagrange multipliers. The Lagrange function is therefore:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述带有不等式约束的二次规划问题，我们可以使用拉格朗日乘子法。因此，拉格朗日函数为：
- en: $$
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: L(w, w_0, \alpha) = \frac{1}{2}||w||^2 + \sum_i \alpha_i\big(t_i(w^Tx_i+w_0)
    – 1\big)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: L(w, w_0, \alpha) = \frac{1}{2}||w||^2 + \sum_i \alpha_i\big(t_i(w^Tx_i+w_0)
    – 1\big)
- en: $$
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: 'To solve the above, we set the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述问题，我们设置如下：
- en: \begin{equation}
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: \begin{equation}
- en: \frac{\partial L}{ \partial w} = 0, \\
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: \frac{\partial L}{ \partial w} = 0, \\
- en: \frac{\partial L}{ \partial \alpha} = 0, \\
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: \frac{\partial L}{ \partial \alpha} = 0, \\
- en: \frac{\partial L}{ \partial w_0} = 0 \\
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: \frac{\partial L}{ \partial w_0} = 0 \\
- en: \end{equation}
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: \end{equation}
- en: 'Plugging above in the Lagrange function gives us the following optimization
    problem, also called the dual:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 将上述内容代入拉格朗日函数得到如下优化问题，也称为对偶问题：
- en: $$
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: L_d = -\frac{1}{2} \sum_i \sum_k \alpha_i \alpha_k t_i t_k (x_i)^T (x_k) + \sum_i
    \alpha_i
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: L_d = -\frac{1}{2} \sum_i \sum_k \alpha_i \alpha_k t_i t_k (x_i)^T (x_k) + \sum_i
    \alpha_i
- en: $$
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: 'We have to maximize the above subject to the following:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须在以下条件下最大化上述目标：
- en: $$
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: w = \sum_i \alpha_i t_i x_i
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: w = \sum_i \alpha_i t_i x_i
- en: $$
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: and
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: $$
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: 0=\sum_i \alpha_i t_i
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 0=\sum_i \alpha_i t_i
- en: $$
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: The nice thing about the above is that we have an expression for \(w\) in terms
    of Lagrange multipliers. The objective function involves no $w$ term. There is
    a Lagrange multiplier associated with each data point. The computation of $w_0$
    is also explained later.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法的好处在于，我们有一个关于 \(w\) 的表达式，涉及到拉格朗日乘子。目标函数中没有 $w$ 项。每个数据点都有一个相关的拉格朗日乘子。$w_0$
    的计算也在后面解释。
- en: Deciding The Classification of a Test Point
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决定测试点的分类
- en: 'The classification of any test point $x$ can be determined using this expression:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 任何测试点 $x$ 的分类可以使用这个表达式来确定：
- en: $$
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: y(x) = \sum_i \alpha_i t_i x^T x_i + w_0
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: y(x) = \sum_i \alpha_i t_i x^T x_i + w_0
- en: $$
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: A positive value of $y(x)$ implies $x\in+1$ and a negative value means $x\in-1$
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: $y(x)$ 的正值意味着 $x\in+1$，负值则意味着 $x\in-1$
- en: Want to Get Started With Calculus for Machine Learning?
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 想要开始学习机器学习中的微积分？
- en: Take my free 7-day email crash course now (with sample code).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 立即获取我的免费7天邮件速成课程（包含示例代码）。
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 点击注册并获得课程的免费PDF电子书版本。
- en: Karush-Kuhn-Tucker Conditions
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Karush-Kuhn-Tucker 条件
- en: 'Also, Karush-Kuhn-Tucker (KKT) conditions are satisfied by the above constrained
    optimization problem as given by:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Karush-Kuhn-Tucker (KKT) 条件满足上述约束优化问题，如下所示：
- en: \begin{eqnarray}
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: \begin{eqnarray}
- en: \alpha_i &\geq& 0 \\
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: \alpha_i &\geq& 0 \\
- en: t_i y(x_i) -1 &\geq& 0 \\
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: t_i y(x_i) -1 &\geq& 0 \\
- en: \alpha_i(t_i y(x_i) -1) &=& 0
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: \alpha_i(t_i y(x_i) -1) &=& 0
- en: \end{eqnarray}
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: \end{eqnarray}
- en: Interpretation Of KKT Conditions
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: KKT 条件的解释
- en: 'The KKT conditions dictate that for each data point one of the following is
    true:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: KKT 条件规定，对于每个数据点，以下之一是正确的：
- en: The Lagrange multiplier is zero, i.e., \(\alpha_i=0\). This point, therefore,
    plays no role in classification
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉格朗日乘子为零，即 \(\alpha_i=0\)。因此，这一点在分类中没有作用。
- en: OR
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '$ t_i y(x_i) = 1$ and $\alpha_i > 0$: In this case, the data point has a role
    in deciding the value of $w$. Such a point is called a support vector.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $ t_i y(x_i) = 1$ 和 $\alpha_i > 0$：在这种情况下，数据点在决定 $w$ 的值时起作用。这样的点被称为支持向量。
- en: Computing w_0
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算 $w_0$
- en: For $w_0$, we can select any support vector $x_s$ and solve
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $w_0$，我们可以选择任何支持向量 $x_s$ 并求解
- en: $$
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: t_s y(x_s) = 1
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: t_s y(x_s) = 1
- en: $$
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: 'giving us:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 给出：
- en: $$
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: t_s(\sum_i \alpha_i t_i x_s^T x_i + w_0) = 1
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: t_s(\sum_i \alpha_i t_i x_s^T x_i + w_0) = 1
- en: $$
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: A Solved Example
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个已解决的示例
- en: To help you understand the above concepts, here is a simple arbitrarily solved
    example. Of course, for a large number of points you would use an optimization
    software to solve this. Also, this is one possible solution that satisfies all
    the constraints. The objective function can be maximized further but the slope
    of the hyperplane will remain the same for an optimal solution. Also, for this
    example, $w_0$ was computed by taking the average of $w_0$ from all three support
    vectors.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助你理解上述概念，这里有一个简单的任意求解示例。当然，对于大量点，你会使用优化软件来解决这个问题。此外，这只是满足所有约束条件的一个可能解决方案。目标函数可以进一步最大化，但超平面的斜率对于最优解将保持不变。此外，对于这个例子，$w_0$
    是通过取所有三个支持向量的 $w_0$ 的平均值来计算的。
- en: This example will show you that the model is not as complex as it looks.[![](../Images/8c090023e87641c9d86abd51936e9465.png)](https://machinelearningmastery.com/wp-content/uploads/2021/11/intro2-1.png)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例将向你展示模型并不像它看起来那么复杂。[![](../Images/8c090023e87641c9d86abd51936e9465.png)](https://machinelearningmastery.com/wp-content/uploads/2021/11/intro2-1.png)
- en: For the above set of points, we can see that (1,2), (2,1) and (0,0) are points
    closest to the separating hyperplane and hence, act as support vectors. Points
    far away from the boundary (e.g. (-3,1)) do not play any role in determining the
    classification of the points.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上述点集，我们可以看到 (1,2)、(2,1) 和 (0,0) 是离分隔超平面最近的点，因此，作为支持向量。远离边界的点（例如 (-3,1)）在确定点的分类时没有任何作用。
- en: Further Reading
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: This section provides more resources on the topic if you are looking to go deeper.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供更多有关该主题的资源，如果你想深入了解。
- en: Books
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 书籍
- en: '[Pattern Recognition and Machine Learning](https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738)
    by Christopher M. Bishop'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[模式识别与机器学习](https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738)
    由 Christopher M. Bishop 编著'
- en: '[Thomas’ Calculus](https://amzn.to/35Yeolv), 14th edition, 2017  (based on
    the original works of George B. Thomas, revised by Joel Hass, Christopher Heil,
    Maurice Weir)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[托马斯微积分](https://amzn.to/35Yeolv)，第14版，2017年（基于 George B. Thomas 的原著，由 Joel
    Hass、Christopher Heil 和 Maurice Weir 修订）'
- en: Articles
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文章
- en: '[Support Vector Machines for Machine Learning](https://machinelearningmastery.com/support-vector-machines-for-machine-learning/)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[支持向量机在机器学习中的应用](https://machinelearningmastery.com/support-vector-machines-for-machine-learning/)'
- en: '[A Tutorial on Support Vector Machines for Pattern Recognition](https://www.di.ens.fr/~mallat/papiers/svmtutorial.pdf)
    by Christopher J.C. Burges'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[支持向量机模式识别教程](https://www.di.ens.fr/~mallat/papiers/svmtutorial.pdf) 作者：克里斯托弗·J.C.·伯吉斯'
- en: Summary
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this tutorial, you discovered how to use the method of Lagrange multipliers
    to solve the problem of maximizing the margin via a quadratic programming problem
    with inequality constraints.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你了解了如何使用拉格朗日乘子法解决通过具有不等式约束的二次规划问题来最大化间隔的问题。
- en: 'Specifically, you learned:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，你学到了：
- en: The mathematical expression for a separating linear hyperplane
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分离线性超平面的数学表达式
- en: The maximum margin as a solution of a quadratic programming problem with inequality
    constraint
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大间隔作为具有不等式约束的二次规划问题的解
- en: How to find a linear hyperplane between positive and negative examples using
    the method of Lagrange multipliers
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用拉格朗日乘子法找到正负样本之间的线性超平面
- en: Do you have any questions about the SVM discussed in this post? Ask your questions
    in the comments below and I will do my best to answer.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这篇文章中讨论的支持向量机，你有任何问题吗？请在下面的评论中提出你的问题，我会尽力回答。
