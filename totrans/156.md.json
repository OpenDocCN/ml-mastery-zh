["```py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n```", "```py\n# Creating our dataset class\nclass Build_Data(Dataset):    \n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-5, 5, 0.1).view(-1, 1)\n        self.func = -5 * self.x + 1\n        self.y = self.func + 0.4 * torch.randn(self.x.size())\n        self.len = self.x.shape[0]        \n    # Getting the data\n    def __getitem__(self, index):    \n        return self.x[index], self.y[index]    \n    # Getting length of the data\n    def __len__(self):\n        return self.len\n```", "```py\n# Create dataset object\ndata_set = Build_Data()\n\n# Plot and visualizing the data points\nplt.plot(data_set.x.numpy(), data_set.y.numpy(), 'b+', label = 'y')\nplt.plot(data_set.x.numpy(), data_set.func.numpy(), 'r', label = 'func')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.grid('True', color='y')\nplt.show()\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# Creating our dataset class\nclass Build_Data(Dataset):    \n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-5, 5, 0.1).view(-1, 1)\n        self.func = -5 * self.x + 1\n        self.y = self.func + 0.4 * torch.randn(self.x.size())\n        self.len = self.x.shape[0]        \n    # Getting the data\n    def __getitem__(self, index):    \n        return self.x[index], self.y[index]    \n    # Getting length of the data\n    def __len__(self):\n        return self.len\n\n# Create dataset object\ndata_set = Build_Data()\n\n# Plot and visualizing the data points\nplt.plot(data_set.x.numpy(), data_set.y.numpy(), 'b+', label = 'y')\nplt.plot(data_set.x.numpy(), data_set.func.numpy(), 'r', label = 'func')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.grid('True', color='y')\nplt.show()\n```", "```py\nmodel = torch.nn.Linear(1, 1)\ncriterion = torch.nn.MSELoss()\n```", "```py\n...\nprint(list(model.parameters()))\n```", "```py\n[Parameter containing:\ntensor([[-5.2178]], requires_grad=True), Parameter containing:\ntensor([-5.5367], requires_grad=True)]\n```", "```py\n...\nmodel.state_dict()['weight'][0] = -10\nmodel.state_dict()['bias'][0] = -20\n```", "```py\n...\n# Creating Dataloader object\ntrainloader = DataLoader(dataset = data_set, batch_size=1)\n```", "```py\n...\n# define optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n```", "```py\n...\nloss_SGD = []\nn_iter = 20\n\nfor i in range(n_iter):\n    for x, y in trainloader:\n        # making a pridiction in forward pass\n        y_hat = model(x)\n        # calculating the loss between original and predicted data points\n        loss = criterion(y_hat, y)\n        # store loss into list\n        loss_SGD.append(loss.item())\n        # zeroing gradients after each iteration\n        optimizer.zero_grad()\n        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n        loss.backward()\n        # updateing the parameters after each iteration\n        optimizer.step()\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# Creating our dataset class\nclass Build_Data(Dataset):    \n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-5, 5, 0.1).view(-1, 1)\n        self.func = -5 * self.x + 1\n        self.y = self.func + 0.4 * torch.randn(self.x.size())\n        self.len = self.x.shape[0]        \n    # Getting the data\n    def __getitem__(self, index):    \n        return self.x[index], self.y[index]    \n    # Getting length of the data\n    def __len__(self):\n        return self.len\n\n# Create dataset object\ndata_set = Build_Data()\n\nmodel = torch.nn.Linear(1, 1)\ncriterion = torch.nn.MSELoss()\n\n# Creating Dataloader object\ntrainloader = DataLoader(dataset = data_set, batch_size=1)\n\n# define optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\nloss_SGD = []\nn_iter = 20\n\nfor i in range(n_iter):\n    for x, y in trainloader:\n        # making a pridiction in forward pass\n        y_hat = model(x)\n        # calculating the loss between original and predicted data points\n        loss = criterion(y_hat, y)\n        # store loss into list\n        loss_SGD.append(loss.item())\n        # zeroing gradients after each iteration\n        optimizer.zero_grad()\n        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n        loss.backward()\n        # updateing the parameters after each iteration\n        optimizer.step()\n```", "```py\n...\n# define optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n```", "```py\n...\nloss_Adam = []\nn_iter = 20\n\nfor i in range(n_iter):\n    for x, y in trainloader:\n        # making a pridiction in forward pass\n        y_hat = model(x)\n        # calculating the loss between original and predicted data points\n        loss = criterion(y_hat, y)\n        # store loss into list\n        loss_Adam.append(loss.item())\n        # zeroing gradients after each iteration\n        optimizer.zero_grad()\n        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n        loss.backward()\n        # updateing the parameters after each iteration\n        optimizer.step()\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# Creating our dataset class\nclass Build_Data(Dataset):\n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-5, 5, 0.1).view(-1, 1)\n        self.func = -5 * self.x + 1\n        self.y = self.func + 0.4 * torch.randn(self.x.size())\n        self.len = self.x.shape[0]        \n    # Getting the data\n    def __getitem__(self, index):    \n        return self.x[index], self.y[index]    \n    # Getting length of the data\n    def __len__(self):\n        return self.len\n\n# Create dataset object\ndata_set = Build_Data()\n\nmodel = torch.nn.Linear(1, 1)\ncriterion = torch.nn.MSELoss()\n\n# Creating Dataloader object\ntrainloader = DataLoader(dataset = data_set, batch_size=1)\n\n# define optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nloss_Adam = []\nn_iter = 20\n\nfor i in range(n_iter):\n    for x, y in trainloader:\n        # making a pridiction in forward pass\n        y_hat = model(x)\n        # calculating the loss between original and predicted data points\n        loss = criterion(y_hat, y)\n        # store loss into list\n        loss_Adam.append(loss.item())\n        # zeroing gradients after each iteration\n        optimizer.zero_grad()\n        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n        loss.backward()\n        # updateing the parameters after each iteration\n        optimizer.step()\n```", "```py\n...\nplt.plot(loss_SGD,label = \"Stochastic Gradient Descent\")\nplt.plot(loss_Adam,label = \"Adam Optimizer\")\nplt.xlabel('epoch')\nplt.ylabel('Cost/ total loss')\nplt.legend()\nplt.show()\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# Creating our dataset class\nclass Build_Data(Dataset):\n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-5, 5, 0.1).view(-1, 1)\n        self.func = -5 * self.x + 1\n        self.y = self.func + 0.4 * torch.randn(self.x.size())\n        self.len = self.x.shape[0]\n    # Getting the data\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n    # Getting length of the data\n    def __len__(self):\n        return self.len\n\n# Create dataset object\ndata_set = Build_Data()\n\nmodel = torch.nn.Linear(1, 1)\ncriterion = torch.nn.MSELoss()\n\n# Creating Dataloader object\ntrainloader = DataLoader(dataset = data_set, batch_size=1)\n\n# define optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nloss_SGD = []\nn_iter = 20\n\nfor i in range(n_iter):\n    for x, y in trainloader:\n        # making a prediction in forward pass\n        y_hat = model(x)\n        # calculating the loss between original and predicted data points\n        loss = criterion(y_hat, y)\n        # store loss into list\n        loss_SGD.append(loss.item())\n        # zeroing gradients after each iteration\n        optimizer.zero_grad()\n        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n        loss.backward()\n        # updating the parameters after each iteration\n        optimizer.step()\n\nmodel = torch.nn.Linear(1, 1)\nloss_Adam = []\nfor i in range(n_iter):\n    for x, y in trainloader:\n        # making a prediction in forward pass\n        y_hat = model(x)\n        # calculating the loss between original and predicted data points\n        loss = criterion(y_hat, y)\n        # store loss into list\n        loss_Adam.append(loss.item())\n        # zeroing gradients after each iteration\n        optimizer.zero_grad()\n        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n        loss.backward()\n        # updating the parameters after each iteration\n        optimizer.step()\n\nplt.plot(loss_SGD,label = \"Stochastic Gradient Descent\")\nplt.plot(loss_Adam,label = \"Adam Optimizer\")\nplt.xlabel('epoch')\nplt.ylabel('Cost/ total loss')\nplt.legend()\nplt.show()\n```"]