- en: How to Implement Multi-Head Attention from Scratch in TensorFlow and Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras/](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We have already familiarized ourselves with the theory behind the [Transformer
    model](https://machinelearningmastery.com/the-transformer-model/) and its [attention
    mechanism](https://machinelearningmastery.com/the-transformer-attention-mechanism/).
    We have already started our journey of implementing a complete model by seeing
    how to [implement the scaled-dot product attention](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras).
    We shall now progress one step further into our journey by encapsulating the scaled-dot
    product attention into a multi-head attention mechanism, which is a core component.
    Our end goal remains to apply the complete model to Natural Language Processing
    (NLP).
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, you will discover how to implement multi-head attention from
    scratch in TensorFlow and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'After completing this tutorial, you will know:'
  prefs: []
  type: TYPE_NORMAL
- en: The layers that form part of the multi-head attention mechanism.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to implement the multi-head attention mechanism from scratch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kick-start your project** with my book [Building Transformer Models with
    Attention](https://machinelearningmastery.com/transformer-models-with-attention/).
    It provides **self-study tutorials** with **working code** to guide you into building
    a fully-working transformer model that can'
  prefs: []
  type: TYPE_NORMAL
- en: '*translate sentences from one language to another*...'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/19463d287d4ba96be0d0b7013886bc1c.png)](https://machinelearningmastery.com/wp-content/uploads/2022/03/multihead_cover-scaled.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: How to implement multi-head attention from scratch in TensorFlow and Keras
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Everaldo Coelho](https://unsplash.com/photos/YfldCpQuKt4), some rights
    reserved.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tutorial Overview**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This tutorial is divided into three parts; they are:'
  prefs: []
  type: TYPE_NORMAL
- en: Recap of the Transformer Architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Transformer Multi-Head Attention
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Multi-Head Attention From Scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing Out the Code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prerequisites**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this tutorial, we assume that you are already familiar with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[The concept of attention](https://machinelearningmastery.com/what-is-attention/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Transfomer attention mechanism](https://machinelearningmastery.com/the-transformer-attention-mechanism)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Transformer model](https://machinelearningmastery.com/the-transformer-model/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The scaled dot-product attention](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recap of the Transformer Architecture**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Recall](https://machinelearningmastery.com/the-transformer-model/) having
    seen that the Transformer architecture follows an encoder-decoder structure. The
    encoder, on the left-hand side, is tasked with mapping an input sequence to a
    sequence of continuous representations; the decoder, on the right-hand side, receives
    the output of the encoder together with the decoder output at the previous time
    step to generate an output sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/5cc2fa00063bfd70298252dce57dbdcd.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The encoder-decoder structure of the Transformer architecture
  prefs: []
  type: TYPE_NORMAL
- en: Taken from “[Attention Is All You Need](https://arxiv.org/abs/1706.03762)“
  prefs: []
  type: TYPE_NORMAL
- en: In generating an output sequence, the Transformer does not rely on recurrence
    and convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: You have seen that the decoder part of the Transformer shares many similarities
    in its architecture with the encoder. One of the core mechanisms that both the
    encoder and decoder share is the *multi-head attention* mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Transformer Multi-Head Attention**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Each multi-head attention block is made up of four consecutive levels:'
  prefs: []
  type: TYPE_NORMAL
- en: On the first level, three linear (dense) layers that each receive the queries,
    keys, or values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the second level, a scaled dot-product attention function. The operations
    performed on both the first and second levels are repeated *h* times and performed
    in parallel, according to the number of heads composing the multi-head attention
    block.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the third level, a concatenation operation that joins the outputs of the
    different heads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the fourth level, a final linear (dense) layer that produces the output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](../Images/49d312ed799331ac86c88962132369f2.png)](https://machinelearningmastery.com/wp-content/uploads/2021/09/tour_4.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-head attention
  prefs: []
  type: TYPE_NORMAL
- en: Taken from “[Attention Is All You Need](https://arxiv.org/abs/1706.03762)“
  prefs: []
  type: TYPE_NORMAL
- en: '[Recall](https://machinelearningmastery.com/the-transformer-attention-mechanism/)
    as well the important components that will serve as building blocks for your implementation
    of the multi-head attention:'
  prefs: []
  type: TYPE_NORMAL
- en: The **queries**, **keys,** and **values**: These are the inputs to each multi-head
    attention block. In the encoder stage, they each carry the same input sequence
    after this has been embedded and augmented by positional information. Similarly,
    on the decoder side, the queries, keys, and values fed into the first attention
    block represent the same target sequence after this would have also been embedded
    and augmented by positional information. The second attention block of the decoder
    receives the encoder output in the form of keys and values, and the normalized
    output of the first decoder attention block as the queries. The dimensionality
    of the queries and keys is denoted by $d_k$, whereas the dimensionality of the
    values is denoted by $d_v$.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **projection matrices**: When applied to the queries, keys, and values,
    these projection matrices generate different subspace representations of each.
    Each attention *head* then works on one of these projected versions of the queries,
    keys, and values. An additional projection matrix is also applied to the output
    of the multi-head attention block after the outputs of each individual head would
    have been concatenated together. The projection matrices are learned during training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now see how to implement the multi-head attention from scratch in TensorFlow
    and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: '**Implementing Multi-Head Attention from Scratch**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by creating the class, `MultiHeadAttention`, which inherits from
    the `Layer` base class in Keras and initialize several instance attributes that
    you shall be working with (attribute descriptions may be found in the comments):'
  prefs: []
  type: TYPE_NORMAL
- en: Python
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here note that an instance of the `DotProductAttention` class that was implemented
    earlier has been created, and its output was assigned to the variable `attention`.
    [Recall](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras)
    that you implemented the `DotProductAttention` class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Python
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, you will be reshaping the *linearly projected* queries, keys, and values
    in such a manner as to allow the attention heads to be computed in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: The queries, keys, and values will be fed as input into the multi-head attention
    block having a shape of (*batch size*, *sequence length*, *model dimensionality*),
    where the *batch size* is a hyperparameter of the training process, the *sequence
    length* defines the maximum length of the input/output phrases, and the *model
    dimensionality* is the dimensionality of the outputs produced by all sub-layers
    of the model. They are then passed through the respective dense layer to be linearly
    projected to a shape of (*batch size*, *sequence length*, *queries*/*keys*/*values
    dimensionality*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The linearly projected queries, keys, and values will be rearranged into (*batch
    size*, *number of heads*, *sequence length*, *depth*), by first reshaping them
    into (*batch size*, *sequence length*, *number of heads*, *depth*) and then transposing
    the second and third dimensions. For this purpose, you will create the class method,
    `reshape_tensor`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Python
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `reshape_tensor` method receives the linearly projected queries, keys, or
    values as input (while setting the flag to `True`) to be rearranged as previously
    explained. Once the multi-head attention output has been generated, this is also
    fed into the same function (this time setting the flag to `False`) to perform
    a reverse operation, effectively concatenating the results of all heads together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, the next step is to feed the linearly projected queries, keys, and values
    into the `reshape_tensor` method to be rearranged, then feed them into the scaled
    dot-product attention function. In order to do so, let’s create another class
    method, `call`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Python
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `reshape_tensor` method can also receive a mask (whose value defaults
    to `None`) as input, in addition to the queries, keys, and values.
  prefs: []
  type: TYPE_NORMAL
- en: '[Recall](https://machinelearningmastery.com/the-transformer-model/) that the
    Transformer model introduces a *look-ahead mask* to prevent the decoder from attending
    to succeeding words, such that the prediction for a particular word can only depend
    on known outputs for the words that come before it. Furthermore, since the word
    embeddings are zero-padded to a specific sequence length, a *padding mask* also
    needs to be introduced to prevent the zero values from being processed along with
    the input. These look-ahead and padding masks can be passed on to the scaled-dot
    product attention through the `mask` argument.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you have generated the multi-head attention output from all the attention
    heads, the final steps are to concatenate back all outputs together into a tensor
    of shape (*batch size*, *sequence length*, *values dimensionality*) and passing
    the result through one final dense layer. For this purpose, you will add the next
    two lines of code to the `call` method.
  prefs: []
  type: TYPE_NORMAL
- en: Python
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Putting everything together, you have the following implementation of the multi-head
    attention:'
  prefs: []
  type: TYPE_NORMAL
- en: Python
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Want to Get Started With Building Transformer Models with Attention?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Take my free 12-day email crash course now (with sample code).
  prefs: []
  type: TYPE_NORMAL
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  prefs: []
  type: TYPE_NORMAL
- en: '**Testing Out the Code**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You will be working with the parameter values specified in the paper, [Attention
    Is All You Need](https://arxiv.org/abs/1706.03762), by Vaswani et al. (2017):'
  prefs: []
  type: TYPE_NORMAL
- en: Python
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As for the sequence length and the queries, keys, and values, you will be working
    with dummy data for the time being until you arrive at the stage of [training
    the complete Transformer model](https://machinelearningmastery.com/training-the-transformer-model)
    in a separate tutorial, at which point you will be using actual sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: Python
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In the complete Transformer model, values for the sequence length and the queries,
    keys, and values will be obtained through a process of word tokenization and embedding.
    We will be covering this in a separate tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning to the testing procedure, the next step is to create a new instance
    of the `MultiHeadAttention` class, assigning its output to the `multihead_attention`
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: Python
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the `MultiHeadAttention` class inherits from the `Layer` base class,
    the `call()` method of the former will be automatically invoked by the magic `__call()__`
    method of the latter. The final step is to pass in the input arguments and print
    the result:'
  prefs: []
  type: TYPE_NORMAL
- en: Python
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Tying everything together produces the following code listing:'
  prefs: []
  type: TYPE_NORMAL
- en: Python
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Running this code produces an output of shape (*batch size*, *sequence length*,
    *model dimensionality*). Note that you will likely see a different output due
    to the random initialization of the queries, keys, and values and the parameter
    values of the dense layers.
  prefs: []
  type: TYPE_NORMAL
- en: Python
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**Further Reading**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section provides more resources on the topic if you are looking to go deeper.
  prefs: []
  type: TYPE_NORMAL
- en: '**Books**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Advanced Deep Learning with Python](https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X),
    2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Transformers for Natural Language Processing](https://www.amazon.com/Transformers-Natural-Language-Processing-architectures/dp/1800565798),
    2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Papers**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Attention Is All You Need](https://arxiv.org/abs/1706.03762), 2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this tutorial, you discovered how to implement multi-head attention from
    scratch in TensorFlow and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, you learned:'
  prefs: []
  type: TYPE_NORMAL
- en: The layers that form part of the multi-head attention mechanism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to implement the multi-head attention mechanism from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you have any questions?
  prefs: []
  type: TYPE_NORMAL
- en: Ask your questions in the comments below, and I will do my best to answer.
  prefs: []
  type: TYPE_NORMAL
