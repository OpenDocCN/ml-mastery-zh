["```py\n*** START OF THIS PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***\n```", "```py\nTHE END\n```", "```py\n['\\n', '\\r', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', ':', ';', '?', '[', ']',\n'_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xbb', '\\xbf', '\\xef']\n```", "```py\nimport numpy as np\n\n# load ascii text and covert to lowercase\nfilename = \"wonderland.txt\"\nraw_text = open(filename, 'r', encoding='utf-8').read()\nraw_text = raw_text.lower()\n\n# create mapping of unique chars to integers\nchars = sorted(list(set(raw_text)))\nchar_to_int = dict((c, i) for i, c in enumerate(chars))\n\n# summarize the loaded data\nn_chars = len(raw_text)\nn_vocab = len(chars)\nprint(\"Total Characters: \", n_chars)\nprint(\"Total Vocab: \", n_vocab)\n```", "```py\nTotal Characters: 144574\nTotal Vocab: 50\n```", "```py\nchapt -> e\nhapte -> r\n```", "```py\n# prepare the dataset of input to output pairs encoded as integers\nseq_length = 100\ndataX = []\ndataY = []\nfor i in range(0, n_chars - seq_length, 1):\n    seq_in = raw_text[i:i + seq_length]\n    seq_out = raw_text[i + seq_length]\n    dataX.append([char_to_int[char] for char in seq_in])\n    dataY.append(char_to_int[seq_out])\nn_patterns = len(dataX)\nprint(\"Total Patterns: \", n_patterns)\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# reshape X to be [samples, time steps, features]\nX = torch.tensor(dataX, dtype=torch.float32).reshape(n_patterns, seq_length, 1)\nX = X / float(n_vocab)\ny = torch.tensor(dataY)\nprint(X.shape, y.shape)\n```", "```py\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\n\nclass CharModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=1, batch_first=True)\n        self.dropout = nn.Dropout(0.2)\n        self.linear = nn.Linear(256, n_vocab)\n    def forward(self, x):\n        x, _ = self.lstm(x)\n        # take only the last output\n        x = x[:, -1, :]\n        # produce output\n        x = self.linear(self.dropout(x))\n        return x\n```", "```py\nn_epochs = 40\nbatch_size = 128\nmodel = CharModel()\n\noptimizer = optim.Adam(model.parameters())\nloss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\nloader = data.DataLoader(data.TensorDataset(X, y), shuffle=True, batch_size=batch_size)\n\nbest_model = None\nbest_loss = np.inf\nfor epoch in range(n_epochs):\n    model.train()\n    for X_batch, y_batch in loader:\n        y_pred = model(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    # Validation\n    model.eval()\n    loss = 0\n    with torch.no_grad():\n        for X_batch, y_batch in loader:\n            y_pred = model(X_batch)\n            loss += loss_fn(y_pred, y_batch)\n        if loss < best_loss:\n            best_loss = loss\n            best_model = model.state_dict()\n        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n\ntorch.save([best_model, char_to_dict], \"single-char.pth\")\n```", "```py\n...\nEpoch 35: Cross-entropy: 245745.2500\nEpoch 36: Cross-entropy: 243908.7031\nEpoch 37: Cross-entropy: 238833.5000\nEpoch 38: Cross-entropy: 239069.0000\nEpoch 39: Cross-entropy: 234176.2812\n```", "```py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\n\n# load ascii text and covert to lowercase\nfilename = \"wonderland.txt\"\nraw_text = open(filename, 'r', encoding='utf-8').read()\nraw_text = raw_text.lower()\n\n# create mapping of unique chars to integers\nchars = sorted(list(set(raw_text)))\nchar_to_int = dict((c, i) for i, c in enumerate(chars))\n\n# summarize the loaded data\nn_chars = len(raw_text)\nn_vocab = len(chars)\nprint(\"Total Characters: \", n_chars)\nprint(\"Total Vocab: \", n_vocab)\n\n# prepare the dataset of input to output pairs encoded as integers\nseq_length = 100\ndataX = []\ndataY = []\nfor i in range(0, n_chars - seq_length, 1):\n    seq_in = raw_text[i:i + seq_length]\n    seq_out = raw_text[i + seq_length]\n    dataX.append([char_to_int[char] for char in seq_in])\n    dataY.append(char_to_int[seq_out])\nn_patterns = len(dataX)\nprint(\"Total Patterns: \", n_patterns)\n\n# reshape X to be [samples, time steps, features]\nX = torch.tensor(dataX, dtype=torch.float32).reshape(n_patterns, seq_length, 1)\nX = X / float(n_vocab)\ny = torch.tensor(dataY)\n\nclass CharModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=1, batch_first=True)\n        self.dropout = nn.Dropout(0.2)\n        self.linear = nn.Linear(256, n_vocab)\n    def forward(self, x):\n        x, _ = self.lstm(x)\n        # take only the last output\n        x = x[:, -1, :]\n        # produce output\n        x = self.linear(self.dropout(x))\n        return x\n\nn_epochs = 40\nbatch_size = 128\nmodel = CharModel()\n\noptimizer = optim.Adam(model.parameters())\nloss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\nloader = data.DataLoader(data.TensorDataset(X, y), shuffle=True, batch_size=batch_size)\n\nbest_model = None\nbest_loss = np.inf\nfor epoch in range(n_epochs):\n    model.train()\n    for X_batch, y_batch in loader:\n        y_pred = model(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    # Validation\n    model.eval()\n    loss = 0\n    with torch.no_grad():\n        for X_batch, y_batch in loader:\n            y_pred = model(X_batch)\n            loss += loss_fn(y_pred, y_batch)\n        if loss < best_loss:\n            best_loss = loss\n            best_model = model.state_dict()\n        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n\ntorch.save([best_model, char_to_int], \"single-char.pth\")\n```", "```py\nseq_length = 100\nstart = np.random.randint(0, len(raw_text)-seq_length)\nprompt = raw_text[start:start+seq_length]\n```", "```py\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nbest_model, char_to_int = torch.load(\"single-char.pth\")\nn_vocab = len(char_to_int)\nint_to_char = dict((i, c) for c, i in char_to_int.items())\n\n# reload the model\nclass CharModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=1, batch_first=True)\n        self.dropout = nn.Dropout(0.2)\n        self.linear = nn.Linear(256, n_vocab)\n    def forward(self, x):\n        x, _ = self.lstm(x)\n        # take only the last output\n        x = x[:, -1, :]\n        # produce output\n        x = self.linear(self.dropout(x))\n        return x\nmodel = CharModel()\nmodel.load_state_dict(best_model)\n\n# randomly generate a prompt\nfilename = \"wonderland.txt\"\nseq_length = 100\nraw_text = open(filename, 'r', encoding='utf-8').read()\nraw_text = raw_text.lower()\nstart = np.random.randint(0, len(raw_text)-seq_length)\nprompt = raw_text[start:start+seq_length]\npattern = [char_to_int[c] for c in prompt]\n\nmodel.eval()\nprint('Prompt: \"%s\"' % prompt)\nwith torch.no_grad():\n    for i in range(1000):\n        # format input array of int into PyTorch tensor\n        x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n        x = torch.tensor(x, dtype=torch.float32)\n        # generate logits as output from the model\n        prediction = model(x)\n        # convert logits into one character\n        index = int(prediction.argmax())\n        result = int_to_char[index]\n        print(result, end=\"\")\n        # append the new character into the prompt for the next iteration\n        pattern.append(index)\n        pattern = pattern[1:]\nprint()\nprint(\"Done.\")\n```", "```py\nPrompt: \"nother rush at the stick, and tumbled head\nover heels in its hurry to get hold of it; then alice, th\"\n```", "```py\ne was qot a litule soteet of thet was sh the thiee harden an the courd, and was tuitk a little toaee th thite ththe and said to the suher, and the whrtght the pacbit sese tha woode of the soeee, and the white rabbit ses ani thr gort to the thite rabbit, and then she was aoiinnene th the three baaed of the sueen and saed “ota turpe ”hun mot,”\n\n“i don’t know the ter ano _enend to mere,” said the maccht ar a sore of great roaee. “ie you don’t teink if thet soued to soeed to the boeie the mooer, io you bane thing it wo\ntou het bn the crur,\n“h whsh you cen not,” said the manch hare.\n\n“wes, it aadi,” said the manch hare.\n\n“weat you tail to merer ae in an a gens if gre” ”he were thing,” said the maccht ar a sore of geeaghen asd tothe to the thieg harden an the could.\n“h dan tor toe taie thing,” said the manch hare.\n\n“wes, it aadi,” said the manch hare.\n\n“weat you tail to merer ae in an a gens if gre” ”he were thing,” said the maccht ar a sore of geeaghen asd tothe to the thieg harden an t\n```", "```py\nclass CharModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=2, batch_first=True, dropout=0.2)\n        self.dropout = nn.Dropout(0.2)\n        self.linear = nn.Linear(256, n_vocab)\n    def forward(self, x):\n        x, _ = self.lstm(x)\n        # take only the last output\n        x = x[:, -1, :]\n        # produce output\n        x = self.linear(self.dropout(x))\n        return x\n```", "```py\n...\nEpoch 34: Cross-entropy: 203763.0312\nEpoch 35: Cross-entropy: 204002.5938\nEpoch 36: Cross-entropy: 210636.5625\nEpoch 37: Cross-entropy: 199619.6875\nEpoch 38: Cross-entropy: 199240.2969\nEpoch 39: Cross-entropy: 196966.1250\n```", "```py\nPrompt: \"ll\nsay that ‘i see what i eat’ is the same thing as ‘i eat what i see’!”\n\n“you might just as well sa\"\ny it to sea,” she katter said to the jury. and the thoee hardeners vhine she was seady to alice the was a long tay of the sooe of the court, and she was seady to and taid to the coor and the court.\n“well you see what you see, the mookee of the soog of the season of the shase of the court!”\n\n“i don’t know the rame thing is it?” said the caterpillar.\n\n“the cormous was it makes he it was it taie the reason of the shall bbout it, you know.”\n\n“i don’t know the rame thing i can’t gelp the sea,” the hatter went on, “i don’t know the peally was in the shall sereat it would be a teally.\nthe mookee of the court ”\n\n“i don’t know the rame thing is it?” said the caterpillar.\n\n“the cormous was it makes he it was it taie the reason of the shall bbout it, you know.”\n\n“i don’t know the rame thing i can’t gelp the sea,” the hatter went on, “i don’t know the peally was in the shall sereat it would be a teally.\nthe mookee of the court ”\n\n“i don’t know the rame thing is it?” said the caterpillar.\n\n“the\nDone.\n```", "```py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\n\n# load ascii text and covert to lowercase\nfilename = \"wonderland.txt\"\nraw_text = open(filename, 'r', encoding='utf-8').read()\nraw_text = raw_text.lower()\n\n# create mapping of unique chars to integers\nchars = sorted(list(set(raw_text)))\nchar_to_int = dict((c, i) for i, c in enumerate(chars))\n\n# summarize the loaded data\nn_chars = len(raw_text)\nn_vocab = len(chars)\nprint(\"Total Characters: \", n_chars)\nprint(\"Total Vocab: \", n_vocab)\n\n# prepare the dataset of input to output pairs encoded as integers\nseq_length = 100\ndataX = []\ndataY = []\nfor i in range(0, n_chars - seq_length, 1):\n    seq_in = raw_text[i:i + seq_length]\n    seq_out = raw_text[i + seq_length]\n    dataX.append([char_to_int[char] for char in seq_in])\n    dataY.append(char_to_int[seq_out])\nn_patterns = len(dataX)\nprint(\"Total Patterns: \", n_patterns)\n\n# reshape X to be [samples, time steps, features]\nX = torch.tensor(dataX, dtype=torch.float32).reshape(n_patterns, seq_length, 1)\nX = X / float(n_vocab)\ny = torch.tensor(dataY)\n\nclass CharModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=2, batch_first=True, dropout=0.2)\n        self.dropout = nn.Dropout(0.2)\n        self.linear = nn.Linear(256, n_vocab)\n    def forward(self, x):\n        x, _ = self.lstm(x)\n        # take only the last output\n        x = x[:, -1, :]\n        # produce output\n        x = self.linear(self.dropout(x))\n        return x\n\nn_epochs = 40\nbatch_size = 128\nmodel = CharModel()\n\noptimizer = optim.Adam(model.parameters())\nloss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\nloader = data.DataLoader(data.TensorDataset(X, y), shuffle=True, batch_size=batch_size)\n\nbest_model = None\nbest_loss = np.inf\nfor epoch in range(n_epochs):\n    model.train()\n    for X_batch, y_batch in loader:\n        y_pred = model(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    # Validation\n    model.eval()\n    loss = 0\n    with torch.no_grad():\n        for X_batch, y_batch in loader:\n            y_pred = model(X_batch)\n            loss += loss_fn(y_pred, y_batch)\n        if loss < best_loss:\n            best_loss = loss\n            best_model = model.state_dict()\n        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n\ntorch.save([best_model, char_to_int], \"single-char.pth\")\n\n# Generation using the trained model\nbest_model, char_to_int = torch.load(\"single-char.pth\")\nn_vocab = len(char_to_int)\nint_to_char = dict((i, c) for c, i in char_to_int.items())\nmodel.load_state_dict(best_model)\n\n# randomly generate a prompt\nfilename = \"wonderland.txt\"\nseq_length = 100\nraw_text = open(filename, 'r', encoding='utf-8').read()\nraw_text = raw_text.lower()\nstart = np.random.randint(0, len(raw_text)-seq_length)\nprompt = raw_text[start:start+seq_length]\npattern = [char_to_int[c] for c in prompt]\n\nmodel.eval()\nprint('Prompt: \"%s\"' % prompt)\nwith torch.no_grad():\n    for i in range(1000):\n        # format input array of int into PyTorch tensor\n        x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n        x = torch.tensor(x, dtype=torch.float32)\n        # generate logits as output from the model\n        prediction = model(x)\n        # convert logits into one character\n        index = int(prediction.argmax())\n        result = int_to_char[index]\n        print(result, end=\"\")\n        # append the new character into the prompt for the next iteration\n        pattern.append(index)\n        pattern = pattern[1:]\nprint()\nprint(\"Done.\")\n```", "```py\ntorch.cuda.is_available()\n```", "```py\ndevice = torch.device(\"cuda:0\")\nmodel.to(device)\n```", "```py\ny_pred = model(X_batch.to(device))\n```", "```py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\n\n# load ascii text and covert to lowercase\nfilename = \"wonderland.txt\"\nraw_text = open(filename, 'r', encoding='utf-8').read()\nraw_text = raw_text.lower()\n\n# create mapping of unique chars to integers\nchars = sorted(list(set(raw_text)))\nchar_to_int = dict((c, i) for i, c in enumerate(chars))\n\n# summarize the loaded data\nn_chars = len(raw_text)\nn_vocab = len(chars)\nprint(\"Total Characters: \", n_chars)\nprint(\"Total Vocab: \", n_vocab)\n\n# prepare the dataset of input to output pairs encoded as integers\nseq_length = 100\ndataX = []\ndataY = []\nfor i in range(0, n_chars - seq_length, 1):\n    seq_in = raw_text[i:i + seq_length]\n    seq_out = raw_text[i + seq_length]\n    dataX.append([char_to_int[char] for char in seq_in])\n    dataY.append(char_to_int[seq_out])\nn_patterns = len(dataX)\nprint(\"Total Patterns: \", n_patterns)\n\n# reshape X to be [samples, time steps, features]\nX = torch.tensor(dataX, dtype=torch.float32).reshape(n_patterns, seq_length, 1)\nX = X / float(n_vocab)\ny = torch.tensor(dataY)\n\nclass CharModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=2, batch_first=True, dropout=0.2)\n        self.dropout = nn.Dropout(0.2)\n        self.linear = nn.Linear(256, n_vocab)\n    def forward(self, x):\n        x, _ = self.lstm(x)\n        # take only the last output\n        x = x[:, -1, :]\n        # produce output\n        x = self.linear(self.dropout(x))\n        return x\n\nn_epochs = 40\nbatch_size = 128\nmodel = CharModel()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\noptimizer = optim.Adam(model.parameters())\nloss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\nloader = data.DataLoader(data.TensorDataset(X, y), shuffle=True, batch_size=batch_size)\n\nbest_model = None\nbest_loss = np.inf\nfor epoch in range(n_epochs):\n    model.train()\n    for X_batch, y_batch in loader:\n        y_pred = model(X_batch.to(device))\n        loss = loss_fn(y_pred, y_batch.to(device))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    # Validation\n    model.eval()\n    loss = 0\n    with torch.no_grad():\n        for X_batch, y_batch in loader:\n            y_pred = model(X_batch.to(device))\n            loss += loss_fn(y_pred, y_batch.to(device))\n        if loss < best_loss:\n            best_loss = loss\n            best_model = model.state_dict()\n        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n\ntorch.save([best_model, char_to_int], \"single-char.pth\")\n\n# Generation using the trained model\nbest_model, char_to_int = torch.load(\"single-char.pth\")\nn_vocab = len(char_to_int)\nint_to_char = dict((i, c) for c, i in char_to_int.items())\nmodel.load_state_dict(best_model)\n\n# randomly generate a prompt\nfilename = \"wonderland.txt\"\nseq_length = 100\nraw_text = open(filename, 'r', encoding='utf-8').read()\nraw_text = raw_text.lower()\nstart = np.random.randint(0, len(raw_text)-seq_length)\nprompt = raw_text[start:start+seq_length]\npattern = [char_to_int[c] for c in prompt]\n\nmodel.eval()\nprint('Prompt: \"%s\"' % prompt)\nwith torch.no_grad():\n    for i in range(1000):\n        # format input array of int into PyTorch tensor\n        x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n        x = torch.tensor(x, dtype=torch.float32)\n        # generate logits as output from the model\n        prediction = model(x.to(device))\n        # convert logits into one character\n        index = int(prediction.argmax())\n        result = int_to_char[index]\n        print(result, end=\"\")\n        # append the new character into the prompt for the next iteration\n        pattern.append(index)\n        pattern = pattern[1:]\nprint()\nprint(\"Done.\")\n```", "```py\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n```"]