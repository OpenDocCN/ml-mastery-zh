["```py\nself.val_split = 0.1  # Ratio of the validation data split\n```", "```py\nval = dataset[int(self.n_sentences * self.train_split):int(self.n_sentences * (1-self.val_split))]\ntest = dataset[int(self.n_sentences * (1 - self.val_split)):]\n```", "```py\nvalX = self.encode_pad(val[:, 0], enc_tokenizer, enc_seq_length)\nvalY = self.encode_pad(val[:, 1], dec_tokenizer, dec_seq_length)\n```", "```py\nself.save_tokenizer(enc_tokenizer, 'enc')\nself.save_tokenizer(dec_tokenizer, 'dec')\nsavetxt('test_dataset.txt', test, fmt='%s')\n```", "```py\nfrom pickle import load, dump, HIGHEST_PROTOCOL\nfrom numpy.random import shuffle\nfrom numpy import savetxt\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import convert_to_tensor, int64\n\nclass PrepareDataset:\n    def __init__(self, **kwargs):\n        super(PrepareDataset, self).__init__(**kwargs)\n        self.n_sentences = 15000  # Number of sentences to include in the dataset\n        self.train_split = 0.8  # Ratio of the training data split\n        self.val_split = 0.1  # Ratio of the validation data split\n\n    # Fit a tokenizer\n    def create_tokenizer(self, dataset):\n        tokenizer = Tokenizer()\n        tokenizer.fit_on_texts(dataset)\n\n        return tokenizer\n\n    def find_seq_length(self, dataset):\n        return max(len(seq.split()) for seq in dataset)\n\n    def find_vocab_size(self, tokenizer, dataset):\n        tokenizer.fit_on_texts(dataset)\n\n        return len(tokenizer.word_index) + 1\n\n    # Encode and pad the input sequences\n    def encode_pad(self, dataset, tokenizer, seq_length):\n        x = tokenizer.texts_to_sequences(dataset)\n        x = pad_sequences(x, maxlen=seq_length, padding='post')\n        x = convert_to_tensor(x, dtype=int64)\n\n        return x\n\n    def save_tokenizer(self, tokenizer, name):\n        with open(name + '_tokenizer.pkl', 'wb') as handle:\n            dump(tokenizer, handle, protocol=HIGHEST_PROTOCOL)\n\n    def __call__(self, filename, **kwargs):\n        # Load a clean dataset\n        clean_dataset = load(open(filename, 'rb'))\n\n        # Reduce dataset size\n        dataset = clean_dataset[:self.n_sentences, :]\n\n        # Include start and end of string tokens\n        for i in range(dataset[:, 0].size):\n            dataset[i, 0] = \"<START> \" + dataset[i, 0] + \" <EOS>\"\n            dataset[i, 1] = \"<START> \" + dataset[i, 1] + \" <EOS>\"\n\n        # Random shuffle the dataset\n        shuffle(dataset)\n\n        # Split the dataset in training, validation and test sets\n        train = dataset[:int(self.n_sentences * self.train_split)]\n        val = dataset[int(self.n_sentences * self.train_split):int(self.n_sentences * (1-self.val_split))]\n        test = dataset[int(self.n_sentences * (1 - self.val_split)):]\n\n        # Prepare tokenizer for the encoder input\n        enc_tokenizer = self.create_tokenizer(dataset[:, 0])\n        enc_seq_length = self.find_seq_length(dataset[:, 0])\n        enc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n\n        # Prepare tokenizer for the decoder input\n        dec_tokenizer = self.create_tokenizer(dataset[:, 1])\n        dec_seq_length = self.find_seq_length(dataset[:, 1])\n        dec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n\n        # Encode and pad the training input\n        trainX = self.encode_pad(train[:, 0], enc_tokenizer, enc_seq_length)\n        trainY = self.encode_pad(train[:, 1], dec_tokenizer, dec_seq_length)\n\n        # Encode and pad the validation input\n        valX = self.encode_pad(val[:, 0], enc_tokenizer, enc_seq_length)\n        valY = self.encode_pad(val[:, 1], dec_tokenizer, dec_seq_length)\n\n        # Save the encoder tokenizer\n        self.save_tokenizer(enc_tokenizer, 'enc')\n\n        # Save the decoder tokenizer\n        self.save_tokenizer(dec_tokenizer, 'dec')\n\n        # Save the testing dataset into a text file\n        savetxt('test_dataset.txt', test, fmt='%s')\n\n        return trainX, trainY, valX, valY, train, val, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size\n```", "```py\nval_dataset = data.Dataset.from_tensor_slices((valX, valY))\nval_dataset = val_dataset.batch(batch_size)\n```", "```py\nval_loss = Mean(name='val_loss')\n```", "```py\ntrain_loss_dict = {}\nval_loss_dict = {}\n\ntrain_loss_dict[epoch] = train_loss.result()\nval_loss_dict[epoch] = val_loss.result()\n```", "```py\nloss = loss_fcn(decoder_output, prediction)\nval_loss(loss)\n```", "```py\n# Save the trained model weights\ntraining_model.save_weights(\"weights/wghts\" + str(epoch + 1) + \".ckpt\")\n```", "```py\nwith open('./train_loss.pkl', 'wb') as file:\n    dump(train_loss_dict, file)\n\nwith open('./val_loss.pkl', 'wb') as file:\n    dump(val_loss_dict, file)\n```", "```py\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import LearningRateSchedule\nfrom tensorflow.keras.metrics import Mean\nfrom tensorflow import data, train, math, reduce_sum, cast, equal, argmax, float32, GradientTape, function\nfrom keras.losses import sparse_categorical_crossentropy\nfrom model import TransformerModel\nfrom prepare_dataset import PrepareDataset\nfrom time import time\nfrom pickle import dump\n\n# Define the model parameters\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_model = 512  # Dimensionality of model layers' outputs\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nn = 6  # Number of layers in the encoder stack\n\n# Define the training parameters\nepochs = 20\nbatch_size = 64\nbeta_1 = 0.9\nbeta_2 = 0.98\nepsilon = 1e-9\ndropout_rate = 0.1\n\n# Implementing a learning rate scheduler\nclass LRScheduler(LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n        super(LRScheduler, self).__init__(**kwargs)\n\n        self.d_model = cast(d_model, float32)\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step_num):\n\n        # Linearly increasing the learning rate for the first warmup_steps, and decreasing it thereafter\n        arg1 = step_num ** -0.5\n        arg2 = step_num * (self.warmup_steps ** -1.5)\n\n        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)\n\n# Instantiate an Adam optimizer\noptimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)\n\n# Prepare the training dataset\ndataset = PrepareDataset()\ntrainX, trainY, valX, valY, train_orig, val_orig, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german.pkl')\n\nprint(enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size)\n\n# Prepare the training dataset batches\ntrain_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\ntrain_dataset = train_dataset.batch(batch_size)\n\n# Prepare the validation dataset batches\nval_dataset = data.Dataset.from_tensor_slices((valX, valY))\nval_dataset = val_dataset.batch(batch_size)\n\n# Create model\ntraining_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n\n# Defining the loss function\ndef loss_fcn(target, prediction):\n    # Create mask so that the zero padding values are not included in the computation of loss\n    padding_mask = math.logical_not(equal(target, 0))\n    padding_mask = cast(padding_mask, float32)\n\n    # Compute a sparse categorical cross-entropy loss on the unmasked values\n    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * padding_mask\n\n    # Compute the mean loss over the unmasked values\n    return reduce_sum(loss) / reduce_sum(padding_mask)\n\n# Defining the accuracy function\ndef accuracy_fcn(target, prediction):\n    # Create mask so that the zero padding values are not included in the computation of accuracy\n    padding_mask = math.logical_not(equal(target, 0))\n\n    # Find equal prediction and target values, and apply the padding mask\n    accuracy = equal(target, argmax(prediction, axis=2))\n    accuracy = math.logical_and(padding_mask, accuracy)\n\n    # Cast the True/False values to 32-bit-precision floating-point numbers\n    padding_mask = cast(padding_mask, float32)\n    accuracy = cast(accuracy, float32)\n\n    # Compute the mean accuracy over the unmasked values\n    return reduce_sum(accuracy) / reduce_sum(padding_mask)\n\n# Include metrics monitoring\ntrain_loss = Mean(name='train_loss')\ntrain_accuracy = Mean(name='train_accuracy')\nval_loss = Mean(name='val_loss')\n\n# Create a checkpoint object and manager to manage multiple checkpoints\nckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\nckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=None)\n\n# Initialise dictionaries to store the training and validation losses\ntrain_loss_dict = {}\nval_loss_dict = {}\n\n# Speeding up the training process\n@function\ndef train_step(encoder_input, decoder_input, decoder_output):\n    with GradientTape() as tape:\n\n        # Run the forward pass of the model to generate a prediction\n        prediction = training_model(encoder_input, decoder_input, training=True)\n\n        # Compute the training loss\n        loss = loss_fcn(decoder_output, prediction)\n\n        # Compute the training accuracy\n        accuracy = accuracy_fcn(decoder_output, prediction)\n\n    # Retrieve gradients of the trainable variables with respect to the training loss\n    gradients = tape.gradient(loss, training_model.trainable_weights)\n\n    # Update the values of the trainable variables by gradient descent\n    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n\n    train_loss(loss)\n    train_accuracy(accuracy)\n\nfor epoch in range(epochs):\n\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n    val_loss.reset_states()\n\n    print(\"\\nStart of epoch %d\" % (epoch + 1))\n\n    start_time = time()\n\n    # Iterate over the dataset batches\n    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n\n        # Define the encoder and decoder inputs, and the decoder output\n        encoder_input = train_batchX[:, 1:]\n        decoder_input = train_batchY[:, :-1]\n        decoder_output = train_batchY[:, 1:]\n\n        train_step(encoder_input, decoder_input, decoder_output)\n\n        if step % 50 == 0:\n            print(f'Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n\n    # Run a validation step after every epoch of training\n    for val_batchX, val_batchY in val_dataset:\n\n        # Define the encoder and decoder inputs, and the decoder output\n        encoder_input = val_batchX[:, 1:]\n        decoder_input = val_batchY[:, :-1]\n        decoder_output = val_batchY[:, 1:]\n\n        # Generate a prediction\n        prediction = training_model(encoder_input, decoder_input, training=False)\n\n        # Compute the validation loss\n        loss = loss_fcn(decoder_output, prediction)\n        val_loss(loss)\n\n    # Print epoch number and accuracy and loss values at the end of every epoch\n    print(\"Epoch %d: Training Loss %.4f, Training Accuracy %.4f, Validation Loss %.4f\" % (epoch + 1, train_loss.result(), train_accuracy.result(), val_loss.result()))\n\n    # Save a checkpoint after every epoch\n    if (epoch + 1) % 1 == 0:\n\n        save_path = ckpt_manager.save()\n        print(\"Saved checkpoint at epoch %d\" % (epoch + 1))\n\n        # Save the trained model weights\n        training_model.save_weights(\"weights/wghts\" + str(epoch + 1) + \".ckpt\")\n\n        train_loss_dict[epoch] = train_loss.result()\n        val_loss_dict[epoch] = val_loss.result()\n\n# Save the training loss values\nwith open('./train_loss.pkl', 'wb') as file:\n    dump(train_loss_dict, file)\n\n# Save the validation loss values\nwith open('./val_loss.pkl', 'wb') as file:\n    dump(val_loss_dict, file)\n\nprint(\"Total time taken: %.2fs\" % (time() - start_time))\n```", "```py\nfrom pickle import load\nfrom matplotlib.pylab import plt\nfrom numpy import arange\n\n# Load the training and validation loss dictionaries\ntrain_loss = load(open('train_loss.pkl', 'rb'))\nval_loss = load(open('val_loss.pkl', 'rb'))\n\n# Retrieve each dictionary's values\ntrain_values = train_loss.values()\nval_values = val_loss.values()\n\n# Generate a sequence of integers to represent the epoch numbers\nepochs = range(1, 21)\n\n# Plot and label the training and validation loss values\nplt.plot(epochs, train_values, label='Training Loss')\nplt.plot(epochs, val_values, label='Validation Loss')\n\n# Add in a title and axes labels\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n\n# Set the tick locations\nplt.xticks(arange(0, 21, 2))\n\n# Display the plot\nplt.legend(loc='best')\nplt.show()\n```"]