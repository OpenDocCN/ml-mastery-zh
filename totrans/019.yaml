- en: What are Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是大型语言模型
- en: 原文：[https://machinelearningmastery.com/what-are-large-language-models/](https://machinelearningmastery.com/what-are-large-language-models/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/what-are-large-language-models/](https://machinelearningmastery.com/what-are-large-language-models/)
- en: Large language models (LLMs) are recent advances in deep learning models to
    work on human languages. Some great use case of LLMs has been demonstrated. A
    large language model is a trained deep-learning model that understands and generates
    text in a human-like fashion. Behind the scene, it is a large transformer model
    that does all the magic.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）是深度学习模型在处理人类语言方面的最新进展。一些出色的 LLM 使用案例已经得到展示。大型语言模型是一个经过训练的深度学习模型，以类似人类的方式理解和生成文本。其背后是一个大型变压器模型，完成所有的魔法。
- en: 'In this post, you will learn about the structure of large language models and
    how it works. In particular, you will know:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，你将了解大型语言模型的结构及其工作原理。特别是，你将了解到：
- en: What is a transformer model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是变压器模型
- en: How a transformer model reads text and generates output
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变压器模型如何读取文本并生成输出
- en: How a large language model can produce text in a human-like fashion.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型如何以类似人类的方式生成文本。
- en: '![](../Images/d1eca61cfb81041e4b4b4a858fb09227.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1eca61cfb81041e4b4b4a858fb09227.png)'
- en: What are Large Language Models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是大型语言模型。
- en: Picture generated by author using Stable Diffusion. Some rights reserved.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者使用稳定扩散生成的图像。保留部分权利。
- en: '**Get started and apply ChatGPT** with my book [Maximizing Productivity with
    ChatGPT](https://machinelearningmastery.com/productivity-with-chatgpt/). It provides
    **real-world use cases** and **prompt examples** designed to get you using ChatGPT
    quickly.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**开始并应用 ChatGPT**，请参阅我的书籍[用 ChatGPT 最大化生产力](https://machinelearningmastery.com/productivity-with-chatgpt/)。它提供了**真实的使用案例**和**提示示例**，旨在帮助你快速使用
    ChatGPT。'
- en: Let’s get started.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Overview
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'This post is divided into three parts; they are:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文分为三个部分；它们是：
- en: From Transformer Model to Large Language Model
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从变压器模型到大型语言模型
- en: Why Transformer Can Predict Text?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么变压器能够预测文本？
- en: How a Large Language Model Is Built?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型是如何构建的？
- en: From Transformer Model to Large Language Model
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从变压器模型到大型语言模型
- en: As humans, we perceive text as a collection of words. Sentences are sequences
    of words. Documents are sequences of chapters, sections, and paragraphs. However,
    for computers, text is merely a sequence of characters. To enable machines to
    comprehend text, [a model based on recurrent neural networks](https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/)
    can be built. This model processes one word or character at a time and provides
    an output once the entire input text has been consumed. This model works pretty
    well, except it sometimes “forgets” what happened at the beginning of the sequence
    when the end is reached.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们人类来说，我们将文本视为单词的集合。句子是单词的序列。文档是章节、部分和段落的序列。然而，对计算机而言，文本仅仅是一串字符。为了让机器理解文本，可以构建一个[基于递归神经网络的模型](https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/)。该模型一次处理一个词或字符，并在整个输入文本被处理完后提供输出。这个模型效果相当好，只是当序列的结尾到达时，它有时会“忘记”序列开头发生了什么。
- en: In 2017, Vaswani et al. published a paper, “Attention is All You Need,” to establish
    a [transformer model](https://machinelearningmastery.com/building-transformer-models-with-attention-crash-course-build-a-neural-machine-translator-in-12-days/).
    It is based on the attention mechanism. Contrary to recurrent neural networks,
    the attention mechanism allows you to see the entire sentence (or even the paragraph)
    at once rather than one word at a time. This allows the transformer model to understand
    the context of a word better. Many state-of-the-art language processing models
    are based on transformers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，Vaswani等人发表了一篇论文，《Attention is All You Need》，以建立一个[变压器模型](https://machinelearningmastery.com/building-transformer-models-with-attention-crash-course-build-a-neural-machine-translator-in-12-days/)。它基于注意力机制。与递归神经网络相反，注意力机制允许你一次性看到整个句子（甚至是段落），而不是逐词处理。这使得变压器模型能够更好地理解一个词的上下文。许多最先进的语言处理模型都基于变压器。
- en: To process a text input with a transformer model, you first need to tokenize
    it into a sequence of words. These tokens are then encoded as numbers and converted
    into embeddings, which are vector-space representations of the tokens that preserve
    their meaning. Next, the encoder in the transformer transforms the embeddings
    of all the tokens into a context vector.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用变换器模型处理文本输入，你首先需要将其标记化为一系列词语。这些标记随后被编码为数字，并转换为嵌入，这些嵌入是保留其含义的标记的向量空间表示。接下来，变换器中的编码器将所有标记的嵌入转换为一个上下文向量。
- en: Below is an example of a text string, its tokenization, and the vector embedding.
    Note that the tokenization can be subwords, such as the word “nosegay” in the
    text is tokenized into “nose” and “gay”.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个文本字符串、其标记化及向量嵌入的示例。请注意，标记化可以是子词，例如文本中的“nosegay”被标记化为“nose”和“gay”。
- en: Example of input text
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 输入文本示例
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Tokenized Text
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化文本
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Embedding of the Above Text
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 上述文本的嵌入
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The context vector is like the essence of the entire input. Using this vector,
    the transformer decoder generates output based on clues. For instance, you can
    provide the original input as a clue and let the transformer decoder produce the
    subsequent word that naturally follows. Then, you can reuse the same decoder,
    but this time the clue will be the previously produced next-word. This process
    can be repeated to create an entire paragraph, starting from a leading sentence.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文向量就像整个输入的精髓。利用这个向量，变换器解码器基于线索生成输出。例如，你可以提供原始输入作为线索，让变换器解码器生成自然跟随的下一个词。然后，你可以重复使用相同的解码器，但这次的线索将是之前生成的下一个词。这个过程可以重复，以从一个引导句开始创建整个段落。
- en: '![](../Images/fc1e4d7b9ab803c6ae6c1c8da35d116b.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc1e4d7b9ab803c6ae6c1c8da35d116b.png)'
- en: Transformer Architecture
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器架构
- en: This process is called autoregressive generation. This is how a large language
    model works, except such a model is a transformer model that can take very long
    input text, the context vector is large so it can handle very complex concepts,
    and with many layers in its encoder and decoder.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程被称为自回归生成。这就是大型语言模型的工作方式，除了这种模型是一个变换器模型，可以处理非常长的输入文本，其上下文向量非常大，因此能够处理非常复杂的概念，并且其编码器和解码器具有许多层。
- en: Why Transformer Can Predict Text?
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么变换器可以预测文本？
- en: In his blog post “[Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)”,
    Andrej Karpathy demonstrated that recurrent neural networks can predict the next
    word of a text reasonably well. Not only because there are rules in human language
    (i.e., grammar) that limited the use of words in different places in a sentence,
    but also because there is redundancy in language.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的博客文章“[递归神经网络的非凡有效性](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)”中，Andrej
    Karpathy 证明了递归神经网络可以相当好地预测文本中的下一个词。这不仅仅是因为人类语言中存在限制词语在句子中不同位置使用的规则（即语法），还因为语言中存在冗余。
- en: According to Claude Shannon’s influential paper, “Prediction and Entropy of
    Printed English,” the English language has an entropy of 2.1 bits per letter,
    despite having 27 letters (including spaces). If letters were used randomly, the
    entropy would be 4.8 bits, making it easier to predict what comes next in a human
    language text. Machine learning models, and especially transformer models, are
    adept at making such predictions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Claude Shannon 的影响力论文“《印刷英语的预测与熵》”，尽管英语有27个字母（包括空格），但其熵为每个字母2.1比特。如果字母是随机使用的，那么熵将为4.8比特，这使得在一种人类语言文本中预测下一个词变得更容易。机器学习模型，尤其是变换器模型，擅长做出这样的预测。
- en: By repeating this process, a transformer model can generate the entire passage
    word by word. However, what is grammar as seen by a transformer model? Essentially,
    grammar denotes how words are utilized in language, categorizing them into various
    parts of speech and requiring a specific order within a sentence. Despite this,
    it is challenging to enumerate all the rules of grammar. In reality, the transformer
    model doesn’t explicitly store these rules, instead acquiring them implicitly
    through examples. It’s possible that the model can learn beyond just grammar rules,
    extending to ideas presented in those examples, but the transformer model must
    be large enough.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重复这一过程，变压器模型能够逐字生成整个段落。然而，对于变压器模型来说，什么是语法？本质上，语法表示词汇在语言中的使用方式，将其分类为不同的词性，并要求在句子中按照特定的顺序排列。尽管如此，列举所有语法规则仍然具有挑战性。实际上，变压器模型并不会明确地存储这些规则，而是通过示例隐式地获得它们。模型可能会学习超越语法规则的内容，扩展到这些示例中呈现的思想，但变压器模型必须足够大。
- en: How a Large Language Model Is Built?
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型是如何构建的？
- en: A large language model is a transformer model on a large scale. It is so large
    that usually cannot be run on a single computer. Hence it is naturally a service
    provided over API or a web interface. As you can expect, such large model is learned
    from a vast amount of text before it can remember the patterns and structures
    of language.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型是大规模的变压器模型。它大到通常无法在单台计算机上运行。因此，它自然是通过API或Web界面提供的服务。正如你所预期的，这种大型模型在记住语言的模式和结构之前，必须从大量文本中学习。
- en: For example, the GPT-3 model that is backing the ChatGPT service was trained
    on massive amounts of text data from the internet. This includes books, articles,
    websites, and various other sources. During the training process, the model learns
    the statistical relationships between words, phrases, and sentences, allowing
    it to generate coherent and contextually relevant responses when given a prompt
    or query.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，支撑ChatGPT服务的GPT-3模型是在来自互联网的大量文本数据上进行训练的。这包括书籍、文章、网站以及各种其他来源。在训练过程中，模型学习词汇、短语和句子之间的统计关系，从而能够在给定提示或查询时生成连贯且上下文相关的回应。
- en: Distilling from this vast amount of text, the GPT-3 model can therefore understand
    multiple languages and possess knowledge of various topics. That’s why it can
    produce text in different style. While you may be amazed that large language model
    can perform translation, text summarization, and question answering, it is not
    surprised if you consider these are special “grammars” that match the leading
    text, a.k.a. prompts.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从大量文本中提炼出知识，GPT-3模型因此能够理解多种语言，并且掌握各种主题的知识。这也是为什么它能够生成不同风格的文本。虽然你可能会惊讶于大型语言模型能够进行翻译、文本摘要和问答，但如果你考虑到这些实际上是匹配主文本的特殊“语法”，即提示（prompts），那么这并不令人惊讶。
- en: Summary
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'There are multiple large language models developed. Examples include the GPT-3
    and GPT-4 from OpenAI, LLaMA from Meta, and PaLM2 from Google. These are models
    that can understand language and can generate text. In this post, you learned
    that:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 已经开发了多个大型语言模型。示例包括来自OpenAI的GPT-3和GPT-4、来自Meta的LLaMA以及来自Google的PaLM2。这些模型能够理解语言并生成文本。在这篇文章中，你了解到：
- en: The large language model is based on transformer architecture
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型基于变压器架构。
- en: The attention mechanism allows LLMs to capture long-range dependencies between
    words, hence the model can understand context
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意机制使得LLMs能够捕捉词语之间的长程依赖关系，因此模型能够理解上下文。
- en: Large language model generates text autoregressively based on previously generated
    tokens
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型基于先前生成的标记以自回归的方式生成文本。
