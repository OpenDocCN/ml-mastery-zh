- en: What are Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://machinelearningmastery.com/what-are-large-language-models/](https://machinelearningmastery.com/what-are-large-language-models/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Large language models (LLMs) are recent advances in deep learning models to
    work on human languages. Some great use case of LLMs has been demonstrated. A
    large language model is a trained deep-learning model that understands and generates
    text in a human-like fashion. Behind the scene, it is a large transformer model
    that does all the magic.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post, you will learn about the structure of large language models and
    how it works. In particular, you will know:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a transformer model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How a transformer model reads text and generates output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How a large language model can produce text in a human-like fashion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/d1eca61cfb81041e4b4b4a858fb09227.png)'
  prefs: []
  type: TYPE_IMG
- en: What are Large Language Models.
  prefs: []
  type: TYPE_NORMAL
- en: Picture generated by author using Stable Diffusion. Some rights reserved.
  prefs: []
  type: TYPE_NORMAL
- en: '**Get started and apply ChatGPT** with my book [Maximizing Productivity with
    ChatGPT](https://machinelearningmastery.com/productivity-with-chatgpt/). It provides
    **real-world use cases** and **prompt examples** designed to get you using ChatGPT
    quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This post is divided into three parts; they are:'
  prefs: []
  type: TYPE_NORMAL
- en: From Transformer Model to Large Language Model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why Transformer Can Predict Text?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How a Large Language Model Is Built?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From Transformer Model to Large Language Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As humans, we perceive text as a collection of words. Sentences are sequences
    of words. Documents are sequences of chapters, sections, and paragraphs. However,
    for computers, text is merely a sequence of characters. To enable machines to
    comprehend text, [a model based on recurrent neural networks](https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/)
    can be built. This model processes one word or character at a time and provides
    an output once the entire input text has been consumed. This model works pretty
    well, except it sometimes “forgets” what happened at the beginning of the sequence
    when the end is reached.
  prefs: []
  type: TYPE_NORMAL
- en: In 2017, Vaswani et al. published a paper, “Attention is All You Need,” to establish
    a [transformer model](https://machinelearningmastery.com/building-transformer-models-with-attention-crash-course-build-a-neural-machine-translator-in-12-days/).
    It is based on the attention mechanism. Contrary to recurrent neural networks,
    the attention mechanism allows you to see the entire sentence (or even the paragraph)
    at once rather than one word at a time. This allows the transformer model to understand
    the context of a word better. Many state-of-the-art language processing models
    are based on transformers.
  prefs: []
  type: TYPE_NORMAL
- en: To process a text input with a transformer model, you first need to tokenize
    it into a sequence of words. These tokens are then encoded as numbers and converted
    into embeddings, which are vector-space representations of the tokens that preserve
    their meaning. Next, the encoder in the transformer transforms the embeddings
    of all the tokens into a context vector.
  prefs: []
  type: TYPE_NORMAL
- en: Below is an example of a text string, its tokenization, and the vector embedding.
    Note that the tokenization can be subwords, such as the word “nosegay” in the
    text is tokenized into “nose” and “gay”.
  prefs: []
  type: TYPE_NORMAL
- en: Example of input text
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Tokenized Text
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Embedding of the Above Text
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The context vector is like the essence of the entire input. Using this vector,
    the transformer decoder generates output based on clues. For instance, you can
    provide the original input as a clue and let the transformer decoder produce the
    subsequent word that naturally follows. Then, you can reuse the same decoder,
    but this time the clue will be the previously produced next-word. This process
    can be repeated to create an entire paragraph, starting from a leading sentence.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc1e4d7b9ab803c6ae6c1c8da35d116b.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformer Architecture
  prefs: []
  type: TYPE_NORMAL
- en: This process is called autoregressive generation. This is how a large language
    model works, except such a model is a transformer model that can take very long
    input text, the context vector is large so it can handle very complex concepts,
    and with many layers in its encoder and decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Why Transformer Can Predict Text?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In his blog post “[Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)”,
    Andrej Karpathy demonstrated that recurrent neural networks can predict the next
    word of a text reasonably well. Not only because there are rules in human language
    (i.e., grammar) that limited the use of words in different places in a sentence,
    but also because there is redundancy in language.
  prefs: []
  type: TYPE_NORMAL
- en: According to Claude Shannon’s influential paper, “Prediction and Entropy of
    Printed English,” the English language has an entropy of 2.1 bits per letter,
    despite having 27 letters (including spaces). If letters were used randomly, the
    entropy would be 4.8 bits, making it easier to predict what comes next in a human
    language text. Machine learning models, and especially transformer models, are
    adept at making such predictions.
  prefs: []
  type: TYPE_NORMAL
- en: By repeating this process, a transformer model can generate the entire passage
    word by word. However, what is grammar as seen by a transformer model? Essentially,
    grammar denotes how words are utilized in language, categorizing them into various
    parts of speech and requiring a specific order within a sentence. Despite this,
    it is challenging to enumerate all the rules of grammar. In reality, the transformer
    model doesn’t explicitly store these rules, instead acquiring them implicitly
    through examples. It’s possible that the model can learn beyond just grammar rules,
    extending to ideas presented in those examples, but the transformer model must
    be large enough.
  prefs: []
  type: TYPE_NORMAL
- en: How a Large Language Model Is Built?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A large language model is a transformer model on a large scale. It is so large
    that usually cannot be run on a single computer. Hence it is naturally a service
    provided over API or a web interface. As you can expect, such large model is learned
    from a vast amount of text before it can remember the patterns and structures
    of language.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the GPT-3 model that is backing the ChatGPT service was trained
    on massive amounts of text data from the internet. This includes books, articles,
    websites, and various other sources. During the training process, the model learns
    the statistical relationships between words, phrases, and sentences, allowing
    it to generate coherent and contextually relevant responses when given a prompt
    or query.
  prefs: []
  type: TYPE_NORMAL
- en: Distilling from this vast amount of text, the GPT-3 model can therefore understand
    multiple languages and possess knowledge of various topics. That’s why it can
    produce text in different style. While you may be amazed that large language model
    can perform translation, text summarization, and question answering, it is not
    surprised if you consider these are special “grammars” that match the leading
    text, a.k.a. prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are multiple large language models developed. Examples include the GPT-3
    and GPT-4 from OpenAI, LLaMA from Meta, and PaLM2 from Google. These are models
    that can understand language and can generate text. In this post, you learned
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: The large language model is based on transformer architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The attention mechanism allows LLMs to capture long-range dependencies between
    words, hence the model can understand context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large language model generates text autoregressively based on previously generated
    tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
