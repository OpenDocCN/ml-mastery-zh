- en: Implementing the Transformer Encoder from Scratch in TensorFlow and Keras
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 TensorFlow 和 Keras 中从头开始实现 Transformer 编码器
- en: 原文：[https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras/](https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras/](https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras/)
- en: Having seen how to implement the [scaled dot-product attention](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras) and
    integrate it within the [multi-head attention](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras)
    of the Transformer model, let’s progress one step further toward implementing
    a complete Transformer model by applying its encoder. Our end goal remains to
    apply the complete model to Natural Language Processing (NLP).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在看完如何实现 [缩放点积注意力](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras)
    并将其集成到 Transformer 模型的 [多头注意力](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras)
    后，让我们进一步实现完整的 Transformer 模型，通过应用其编码器来达到我们的最终目标，即将该模型应用于自然语言处理（NLP）。
- en: In this tutorial, you will discover how to implement the Transformer encoder
    from scratch in TensorFlow and Keras.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您将学习如何在 TensorFlow 和 Keras 中从头开始实现 Transformer 编码器。
- en: 'After completing this tutorial, you will know:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本教程后，您将了解：
- en: The layers that form part of the Transformer encoder.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组成 Transformer 编码器的层。
- en: How to implement the Transformer encoder from scratch.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何从头开始实现 Transformer 编码器。
- en: '**Kick-start your project** with my book [Building Transformer Models with
    Attention](https://machinelearningmastery.com/transformer-models-with-attention/).
    It provides **self-study tutorials** with **working code** to guide you into building
    a fully-working transformer model that can'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 用我的书 [使用注意力构建 Transformer 模型](https://machinelearningmastery.com/transformer-models-with-attention/)
    来**启动您的项目**。它提供了**自学教程**和**可工作的代码**，帮助您构建一个完全工作的 Transformer 模型，能够
- en: '*translate sentences from one language to another*...'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*将句子从一种语言翻译到另一种语言*...'
- en: Let’s get started.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: '[![](../Images/7e8cc4e1ead28e3b9459b2fd8a3a5837.png)](https://machinelearningmastery.com/wp-content/uploads/2022/03/encoder_cover-scaled.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/7e8cc4e1ead28e3b9459b2fd8a3a5837.png)](https://machinelearningmastery.com/wp-content/uploads/2022/03/encoder_cover-scaled.jpg)'
- en: Implementing the transformer encoder from scratch in TensorFlow and Keras
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 和 Keras 中从头开始实现 Transformer 编码器
- en: Photo by [ian dooley](https://unsplash.com/photos/DuBNA1QMpPA), some rights
    reserved.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [ian dooley](https://unsplash.com/photos/DuBNA1QMpPA) 提供，部分权利保留。
- en: '**Tutorial Overview**'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**教程概述**'
- en: 'This tutorial is divided into three parts; they are:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程分为三个部分；它们是：
- en: Recap of the Transformer Architecture
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer 架构总结
- en: The Transformer Encoder
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer 编码器
- en: Implementing the Transformer Encoder From Scratch
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始实现 Transformer 编码器
- en: The Fully Connected Feed-Forward Neural Network and Layer Normalization
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接前馈神经网络和层归一化
- en: The Encoder Layer
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器层
- en: The Transformer Encoder
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer 编码器
- en: Testing Out the Code
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试代码
- en: '**Prerequisites**'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**先决条件**'
- en: 'For this tutorial, we assume that you are already familiar with:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本教程，我们假设您已经熟悉以下内容：
- en: '[The Transformer model](https://machinelearningmastery.com/the-transformer-model/)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Transformer 模型](https://machinelearningmastery.com/the-transformer-model/)'
- en: '[The scaled dot-product attention](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[缩放点积注意力](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras)'
- en: '[The multi-head attention](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多头注意力](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras)'
- en: '[The Transformer positional encoding](https://machinelearningmastery.com/the-transformer-positional-encoding-layer-in-keras-part-2/)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Transformer 位置编码](https://machinelearningmastery.com/the-transformer-positional-encoding-layer-in-keras-part-2/)'
- en: '**Recap of the Transformer Architecture**'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Transformer 架构总结**'
- en: '[Recall](https://machinelearningmastery.com/the-transformer-model/) having
    seen that the Transformer architecture follows an encoder-decoder structure. The
    encoder, on the left-hand side, is tasked with mapping an input sequence to a
    sequence of continuous representations; the decoder, on the right-hand side, receives
    the output of the encoder together with the decoder output at the previous time
    step to generate an output sequence.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[回顾](https://machinelearningmastery.com/the-transformer-model/) 已经看到Transformer架构遵循编码器-解码器结构。左侧的编码器负责将输入序列映射到连续表示的序列；右侧的解码器接收编码器的输出以及前一个时间步的解码器输出以生成输出序列。'
- en: '[![](../Images/5cc2fa00063bfd70298252dce57dbdcd.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/5cc2fa00063bfd70298252dce57dbdcd.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png)'
- en: The encoder-decoder structure of the Transformer architecture
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构的编码器-解码器结构
- en: Taken from “[Attention Is All You Need](https://arxiv.org/abs/1706.03762)“
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 摘自 “[Attention Is All You Need](https://arxiv.org/abs/1706.03762)“
- en: In generating an output sequence, the Transformer does not rely on recurrence
    and convolutions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成输出序列时，Transformer 不依赖于递归和卷积。
- en: You have seen that the decoder part of the Transformer shares many similarities
    in its architecture with the encoder. In this tutorial, you will focus on the
    components that form part of the Transformer encoder.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到Transformer的解码器部分在其架构上与编码器有许多相似之处。在本教程中，你将重点关注组成Transformer编码器的组件。
- en: '**The Transformer Encoder**'
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**Transformer编码器**'
- en: 'The Transformer encoder consists of a stack of $N$ identical layers, where
    each layer further consists of two main sub-layers:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer编码器由 $N$ 个相同的层堆叠而成，每层进一步包含两个主要子层：
- en: The first sub-layer comprises a multi-head attention mechanism that receives
    the queries, keys, and values as inputs.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个子层包括一个多头注意力机制，该机制将查询、键和值作为输入。
- en: A second sub-layer comprises a fully-connected feed-forward network.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个子层包括一个全连接前馈网络。
- en: '[![](../Images/64c247dcde7ce423e196af0e42321858.png)](https://machinelearningmastery.com/wp-content/uploads/2021/10/transformer_1.png)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/64c247dcde7ce423e196af0e42321858.png)](https://machinelearningmastery.com/wp-content/uploads/2021/10/transformer_1.png)'
- en: The encoder block of the Transformer architecture
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构的编码器模块
- en: Taken from “[Attention Is All You Need](https://arxiv.org/abs/1706.03762)“
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 摘自 “[Attention Is All You Need](https://arxiv.org/abs/1706.03762)“
- en: 'Following each of these two sub-layers is layer normalization, into which the
    sub-layer input (through a residual connection) and output are fed. The output
    of each layer normalization step is the following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 每个这些两个子层后面都有层归一化，其中将子层输入（通过残差连接）和输出送入。每一步层归一化的输出如下：
- en: LayerNorm(Sublayer Input + Sublayer Output)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: LayerNorm（子层输入 + 子层输出）
- en: In order to facilitate such an operation, which involves an addition between
    the sublayer input and output, Vaswani et al. designed all sub-layers and embedding
    layers in the model to produce outputs of dimension, $d_{\text{model}}$ = 512.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便这种操作——涉及子层输入和输出之间的加法，Vaswani等人设计了模型中的所有子层和嵌入层以产生维度为 $d_{\text{model}}$ =
    512 的输出。
- en: Also, [recall](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras)
    the queries, keys, and values as the inputs to the Transformer encoder.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，[回顾](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras)
    将查询、键和值作为Transformer编码器的输入。
- en: Here, the queries, keys, and values carry the same input sequence after this
    has been embedded and augmented by positional information, where the queries and
    keys are of dimensionality, $d_k$, and the dimensionality of the values is $d_v$.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，查询、键和值在经过嵌入和位置编码增强后，携带相同的输入序列，其中查询和键的维度为 $d_k$，而值的维度为 $d_v$。
- en: Furthermore, Vaswani et al. also introduce regularization into the model by
    applying a dropout to the output of each sub-layer (before the layer normalization
    step), as well as to the positional encodings before these are fed into the encoder.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Vaswani等人还通过在每个子层的输出（在层归一化步骤之前）以及位置编码输入编码器之前应用dropout来引入正则化。
- en: Let’s now see how to implement the Transformer encoder from scratch in TensorFlow
    and Keras.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何从头开始在TensorFlow和Keras中实现Transformer编码器。
- en: Want to Get Started With Building Transformer Models with Attention?
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 想要开始构建具有注意力机制的Transformer模型吗？
- en: Take my free 12-day email crash course now (with sample code).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在就可以立即领取我的免费12天电子邮件速成课程（包括示例代码）。
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 点击注册并获得免费的PDF电子书版课程。
- en: '**Implementing the Transformer Encoder from Scratch**'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**从零开始实现Transformer编码器**'
- en: '**The Fully Connected Feed-Forward Neural Network and Layer Normalization**'
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**全连接前馈神经网络和层归一化**'
- en: Let’s begin by creating classes for the *Feed Forward* and *Add & Norm* layers
    that are shown in the diagram above.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建如上图所示的*Feed Forward*和*Add & Norm*层的类。
- en: Vaswani et al. tell us that the fully connected feed-forward network consists
    of two linear transformations with a ReLU activation in between. The first linear
    transformation produces an output of dimensionality, $d_{ff}$ = 2048, while the
    second linear transformation produces an output of dimensionality, $d_{\text{model}}$
    = 512.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Vaswani等人告诉我们，全连接前馈网络由两个线性变换组成，中间夹有一个ReLU激活。第一个线性变换产生维度为$d_{ff}$ = 2048的输出，而第二个线性变换产生维度为$d_{\text{model}}$
    = 512的输出。
- en: 'For this purpose, let’s first create the class `FeedForward` that inherits
    from the `Layer` base class in Keras and initialize the dense layers and the ReLU
    activation:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们首先创建一个名为`FeedForward`的类，它继承自Keras中的`Layer`基类，并初始化稠密层和ReLU激活：
- en: Python
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will add to it the class method, `call()`, that receives an input and passes
    it through the two fully connected layers with ReLU activation, returning an output
    of dimensionality equal to 512:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将向其中添加一个类方法`call()`，它接收一个输入，并通过两个具有ReLU激活的全连接层，返回一个维度为512的输出：
- en: Python
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The next step is to create another class, `AddNormalization`, that also inherits
    from the `Layer` base class in Keras and initialize a Layer normalization layer:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建另一个类`AddNormalization`，它同样继承自Keras中的`Layer`基类，并初始化一个层归一化层：
- en: Python
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In it, include the following class method that sums its sub-layer’s input and
    output, which it receives as inputs, and applies layer normalization to the result:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在其中，包含以下类方法，它将其子层的输入和输出进行求和，然后对结果应用层归一化：
- en: Python
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**The Encoder Layer**'
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**编码器层**'
- en: Next, you will implement the encoder layer, which the Transformer encoder will
    replicate identically $N$ times.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将实现编码器层，Transformer编码器将完全复制这个层$N$次。
- en: 'For this purpose, let’s create the class, `EncoderLayer`, and initialize all
    the sub-layers that it consists of:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们首先创建一个名为`EncoderLayer`的类，并初始化它所包含的所有子层：
- en: Python
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, you may notice that you have initialized instances of the `FeedForward`
    and `AddNormalization` classes, which you just created in the previous section,
    and assigned their output to the respective variables, `feed_forward` and `add_norm`
    (1 and 2). The `Dropout` layer is self-explanatory, where the `rate` defines the
    frequency at which the input units are set to 0\. You created the `MultiHeadAttention`
    class in a [previous tutorial](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras),
    and if you saved the code into a separate Python script, then do not forget to
    `import` it. I saved mine in a Python script named *multihead_attention.py*, and
    for this reason, I need to include the line of code *from multihead_attention
    import MultiHeadAttention.*
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可能会注意到你已经初始化了之前创建的`FeedForward`和`AddNormalization`类的实例，并将它们的输出分配给各自的变量`feed_forward`和`add_norm`（1和2）。`Dropout`层是不言自明的，其中`rate`定义了输入单元被设为0的频率。你在[上一篇教程](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras)中创建了`MultiHeadAttention`类，如果你将代码保存到了一个单独的Python脚本中，请不要忘记`import`它。我将我的代码保存到名为*multihead_attention.py*的Python脚本中，因此我需要包括代码行*from
    multihead_attention import MultiHeadAttention.*。
- en: 'Let’s now proceed to create the class method, `call()`, that implements all
    the encoder sub-layers:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续创建实现所有编码器子层的类方法`call()`：
- en: Python
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In addition to the input data, the `call()` method can also receive a padding
    mask. As a brief reminder of what was said in a [previous tutorial](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras),
    the *padding* mask is necessary to suppress the zero padding in the input sequence
    from being processed along with the actual input values.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 除了输入数据之外，`call()`方法还可以接收填充掩码。作为之前教程中提到的简要提醒，*填充*掩码是必要的，以抑制输入序列中的零填充与实际输入值一起处理。
- en: The same class method can receive a `training` flag which, when set to `True`,
    will only apply the Dropout layers during training.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 同一个类方法可以接收一个`training`标志，当设置为`True`时，仅在训练期间应用Dropout层。
- en: '**The Transformer Encoder**'
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**Transformer编码器**'
- en: 'The last step is to create a class for the Transformer encoder, which should
    be named `Encoder`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是创建一个名为`Encoder`的Transformer编码器类：
- en: Python
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The Transformer encoder receives an input sequence after this would have undergone
    a process of word embedding and positional encoding. In order to compute the positional
    encoding, let’s make use of the `PositionEmbeddingFixedWeights` class described
    by Mehreen Saeed in [this tutorial](https://machinelearningmastery.com/the-transformer-positional-encoding-layer-in-keras-part-2/).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer编码器在此之后会接收一个经过单词嵌入和位置编码处理的输入序列。为了计算位置编码，让我们使用Mehreen Saeed在[本教程](https://machinelearningmastery.com/the-transformer-positional-encoding-layer-in-keras-part-2/)中描述的`PositionEmbeddingFixedWeights`类。
- en: 'As you have similarly done in the previous sections, here, you will also create
    a class method, `call()`, that applies word embedding and positional encoding
    to the input sequence and feeds the result to $N$ encoder layers:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 就像您在前面的部分中所做的那样，在这里，您还将创建一个名为`call()`的类方法，该方法将单词嵌入和位置编码应用于输入序列，并将结果馈送到$N$个编码器层：
- en: Python
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The code listing for the full Transformer encoder is the following:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的Transformer编码器的代码清单如下：
- en: Python
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Testing Out the Code**'
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**测试代码**'
- en: 'You will work with the parameter values specified in the paper, [Attention
    Is All You Need](https://arxiv.org/abs/1706.03762), by Vaswani et al. (2017):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用Vaswani等人（2017年）在论文[注意力机制全是你需要的](https://arxiv.org/abs/1706.03762)中指定的参数值进行工作：
- en: Python
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As for the input sequence, you will work with dummy data for the time being
    until you arrive at the stage of [training the complete Transformer model](https://machinelearningmastery.com/training-the-transformer-model)
    in a separate tutorial, at which point you will be using actual sentences:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 至于输入序列，暂时您将使用虚拟数据，直到在单独的教程中[训练完整的Transformer模型](https://machinelearningmastery.com/training-the-transformer-model)时，您将使用实际句子：
- en: Python
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, you will create a new instance of the `Encoder` class, assigning its
    output to the `encoder` variable,  subsequently feeding in the input arguments,
    and printing the result. You will set the padding mask argument to `None` for
    the time being, but you will return to this when you implement the complete Transformer
    model:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将创建`Encoder`类的一个新实例，将其输出分配给`encoder`变量，随后输入参数，并打印结果。暂时将填充掩码参数设置为`None`，但在实现完整的Transformer模型时会回到这里：
- en: Python
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Tying everything together produces the following code listing:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容联系在一起得到以下代码清单：
- en: Python
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Running this code produces an output of shape (*batch size*, *sequence length*,
    *model dimensionality*). Note that you will likely see a different output due
    to the random initialization of the input sequence and the parameter values of
    the Dense layers.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码会生成形状为（*批量大小*，*序列长度*，*模型维度*）的输出。请注意，由于输入序列的随机初始化和密集层参数值的不同，您可能会看到不同的输出。
- en: Python
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Further Reading**'
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: This section provides more resources on the topic if you are looking to go deeper.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望深入了解此主题，本节提供了更多资源。
- en: '**Books**'
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**书籍**'
- en: '[Advanced Deep Learning with Python](https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X),
    2019'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python高级深度学习](https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X)，2019'
- en: '[Transformers for Natural Language Processing](https://www.amazon.com/Transformers-Natural-Language-Processing-architectures/dp/1800565798),
    2021'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理的Transformer](https://www.amazon.com/Transformers-Natural-Language-Processing-architectures/dp/1800565798)，2021'
- en: '**Papers**'
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**论文**'
- en: '[Attention Is All You Need](https://arxiv.org/abs/1706.03762), 2017'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[注意力机制全是你需要的](https://arxiv.org/abs/1706.03762)，2017'
- en: '**Summary**'
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this tutorial, you discovered how to implement the Transformer encoder from
    scratch in TensorFlow and Keras.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你学会了如何从零开始在 TensorFlow 和 Keras 中实现 Transformer 编码器。
- en: 'Specifically, you learned:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，你学到了：
- en: The layers that form part of the Transformer encoder
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 形成 Transformer 编码器的一部分的层
- en: How to implement the Transformer encoder from scratch
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何从零开始实现 Transformer 编码器
- en: Do you have any questions?
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你有什么问题吗？
- en: Ask your questions in the comments below, and I will do my best to answer.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的评论中提出你的问题，我会尽力回答。
