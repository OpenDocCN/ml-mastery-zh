["```py\nimport tensorflow as tf\nfrom tensorflow import convert_to_tensor, string\nfrom tensorflow.keras.layers import TextVectorization, Embedding, Layer\nfrom tensorflow.data import Dataset\nimport numpy as np\nimport matplotlib.pyplot as plt\n```", "```py\noutput_sequence_length = 5\nvocab_size = 10\nsentences = [[\"I am a robot\"], [\"you too robot\"]]\nsentence_data = Dataset.from_tensor_slices(sentences)\n# Create the TextVectorization layer\nvectorize_layer = TextVectorization(\n                  output_sequence_length=output_sequence_length,\n                  max_tokens=vocab_size)\n# Train the layer to create a dictionary\nvectorize_layer.adapt(sentence_data)\n# Convert all sentences to tensors\nword_tensors = convert_to_tensor(sentences, dtype=tf.string)\n# Use the word tensors to get vectorized phrases\nvectorized_words = vectorize_layer(word_tensors)\nprint(\"Vocabulary: \", vectorize_layer.get_vocabulary())\nprint(\"Vectorized words: \", vectorized_words)\n```", "```py\nVocabulary:  ['', '[UNK]', 'robot', 'you', 'too', 'i', 'am', 'a']\nVectorized words:  tf.Tensor(\n[[5 6 7 2 0]\n [3 4 2 0 0]], shape=(2, 5), dtype=int64)\n```", "```py\noutput_length = 6\nword_embedding_layer = Embedding(vocab_size, output_length)\nembedded_words = word_embedding_layer(vectorized_words)\nprint(embedded_words)\n```", "```py\nposition_embedding_layer = Embedding(output_sequence_length, output_length)\nposition_indices = tf.range(output_sequence_length)\nembedded_indices = position_embedding_layer(position_indices)\nprint(embedded_indices)\n```", "```py\nfinal_output_embedding = embedded_words + embedded_indices\nprint(\"Final output: \", final_output_embedding)\n```", "```py\nclass PositionEmbeddingLayer(Layer):\n    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n        super(PositionEmbeddingLayer, self).__init__(**kwargs)\n        self.word_embedding_layer = Embedding(\n            input_dim=vocab_size, output_dim=output_dim\n        )\n        self.position_embedding_layer = Embedding(\n            input_dim=sequence_length, output_dim=output_dim\n        )\n\n    def call(self, inputs):        \n        position_indices = tf.range(tf.shape(inputs)[-1])\n        embedded_words = self.word_embedding_layer(inputs)\n        embedded_indices = self.position_embedding_layer(position_indices)\n        return embedded_words + embedded_indices\n```", "```py\nmy_embedding_layer = PositionEmbeddingLayer(output_sequence_length,\n                                            vocab_size, output_length)\nembedded_layer_output = my_embedding_layer(vectorized_words)\nprint(\"Output from my_embedded_layer: \", embedded_layer_output)\n```", "```py\nOutput from my_embedded_layer:  tf.Tensor(\n[[[ 0.06798736 -0.02821309  0.00571618  0.00314623 -0.03060734\n    0.01111387]\n  [-0.06097465  0.03966043 -0.05164248  0.06578685  0.03638128\n   -0.03397174]\n  [ 0.06715029 -0.02453769  0.02205854  0.01110986  0.02345785\n    0.05879898]\n  [-0.04625867  0.07500569 -0.05690887 -0.07615659  0.01962536\n    0.00035865]\n  [ 0.01423577 -0.03938593 -0.08625181  0.04841495  0.06951572\n    0.08811047]]\n\n [[ 0.0163899   0.06895607 -0.01131684  0.01810524 -0.05857501\n    0.01811318]\n  [ 0.01915303 -0.0163289  -0.04133433  0.06810946  0.03736673\n    0.04218033]\n  [ 0.00795418 -0.00143972 -0.01627307 -0.00300788 -0.02759011\n    0.09251165]\n  [ 0.0028762   0.04526488 -0.05222676 -0.02007698  0.07879823\n    0.00541583]\n  [ 0.01423577 -0.03938593 -0.08625181  0.04841495  0.06951572\n    0.08811047]]], shape=(2, 5, 6), dtype=float32)\n```", "```py\nclass PositionEmbeddingFixedWeights(Layer):\n    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)   \n        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)                                          \n        self.word_embedding_layer = Embedding(\n            input_dim=vocab_size, output_dim=output_dim,\n            weights=[word_embedding_matrix],\n            trainable=False\n        )\n        self.position_embedding_layer = Embedding(\n            input_dim=sequence_length, output_dim=output_dim,\n            weights=[position_embedding_matrix],\n            trainable=False\n        )\n\n    def get_position_encoding(self, seq_len, d, n=10000):\n        P = np.zeros((seq_len, d))\n        for k in range(seq_len):\n            for i in np.arange(int(d/2)):\n                denominator = np.power(n, 2*i/d)\n                P[k, 2*i] = np.sin(k/denominator)\n                P[k, 2*i+1] = np.cos(k/denominator)\n        return P\n\n    def call(self, inputs):        \n        position_indices = tf.range(tf.shape(inputs)[-1])\n        embedded_words = self.word_embedding_layer(inputs)\n        embedded_indices = self.position_embedding_layer(position_indices)\n        return embedded_words + embedded_indices\n```", "```py\nattnisallyouneed_embedding = PositionEmbeddingFixedWeights(output_sequence_length,\n                                            vocab_size, output_length)\nattnisallyouneed_output = attnisallyouneed_embedding(vectorized_words)\nprint(\"Output from my_embedded_layer: \", attnisallyouneed_output)\n```", "```py\nOutput from my_embedded_layer:  tf.Tensor(\n[[[-0.9589243   1.2836622   0.23000172  1.9731903   0.01077196\n    1.9999421 ]\n  [ 0.56205547  1.5004725   0.3213085   1.9603932   0.01508068\n    1.9999142 ]\n  [ 1.566284    0.3377554   0.41192317  1.9433732   0.01938933\n    1.999877  ]\n  [ 1.0504174  -1.4061394   0.2314966   1.9860148   0.01077211\n    1.9999698 ]\n  [-0.7568025   0.3463564   0.18459873  1.982814    0.00861763\n    1.9999628 ]]\n\n [[ 0.14112     0.0100075   0.1387981   1.9903207   0.00646326\n    1.9999791 ]\n  [ 0.08466846 -0.11334133  0.23099795  1.9817369   0.01077207\n    1.9999605 ]\n  [ 1.8185948  -0.8322937   0.185397    1.9913884   0.00861771\n    1.9999814 ]\n  [ 0.14112     0.0100075   0.1387981   1.9903207   0.00646326\n    1.9999791 ]\n  [-0.7568025   0.3463564   0.18459873  1.982814    0.00861763\n    1.9999628 ]]], shape=(2, 5, 6), dtype=float32)\n```", "```py\ntechnical_phrase = \"to understand machine learning algorithms you need\" +\\\n                   \" to understand concepts such as gradient of a function \"+\\\n                   \"Hessians of a matrix and optimization etc\"\nwise_phrase = \"patrick henry said give me liberty or give me death \"+\\\n              \"when he addressed the second virginia convention in march\"\n\ntotal_vocabulary = 200\nsequence_length = 20\nfinal_output_len = 50\nphrase_vectorization_layer = TextVectorization(\n                  output_sequence_length=sequence_length,\n                  max_tokens=total_vocabulary)\n# Learn the dictionary\nphrase_vectorization_layer.adapt([technical_phrase, wise_phrase])\n# Convert all sentences to tensors\nphrase_tensors = convert_to_tensor([technical_phrase, wise_phrase], \n                                   dtype=tf.string)\n# Use the word tensors to get vectorized phrases\nvectorized_phrases = phrase_vectorization_layer(phrase_tensors)\n\nrandom_weights_embedding_layer = PositionEmbeddingLayer(sequence_length, \n                                                        total_vocabulary,\n                                                        final_output_len)\nfixed_weights_embedding_layer = PositionEmbeddingFixedWeights(sequence_length, \n                                                        total_vocabulary,\n                                                        final_output_len)\nrandom_embedding = random_weights_embedding_layer(vectorized_phrases)\nfixed_embedding = fixed_weights_embedding_layer(vectorized_phrases)\n```", "```py\nfig = plt.figure(figsize=(15, 5))    \ntitle = [\"Tech Phrase\", \"Wise Phrase\"]\nfor i in range(2):\n    ax = plt.subplot(1, 2, 1+i)\n    matrix = tf.reshape(random_embedding[i, :, :], (sequence_length, final_output_len))\n    cax = ax.matshow(matrix)\n    plt.gcf().colorbar(cax)   \n    plt.title(title[i], y=1.2)\nfig.suptitle(\"Random Embedding\")\nplt.show()\n```", "```py\nfig = plt.figure(figsize=(15, 5))    \ntitle = [\"Tech Phrase\", \"Wise Phrase\"]\nfor i in range(2):\n    ax = plt.subplot(1, 2, 1+i)\n    matrix = tf.reshape(fixed_embedding[i, :, :], (sequence_length, final_output_len))\n    cax = ax.matshow(matrix)\n    plt.gcf().colorbar(cax)   \n    plt.title(title[i], y=1.2)\nfig.suptitle(\"Fixed Weight Embedding from Attention is All You Need\")\nplt.show()\n```"]