["```py\nlongstr = \"\"\nfor x in range(1000):\n  longstr += str(x)\n```", "```py\nlongstr = \"\".join([str(x) for x in range(1000)])\n```", "```py\npython -m timeit 'longstr=\"\"' 'for x in range(1000): longstr += str(x)'\npython -m timeit '\"\".join([str(x) for x in range(1000)])'\n```", "```py\n1000 loops, best of 5: 265 usec per loop\n2000 loops, best of 5: 160 usec per loop\n```", "```py\npython -m timeit 'longstr=\"\"' 'for x in range(1000):' ' longstr += str(x)'\n```", "```py\npython -m timeit -s 'strings = [str(x) for x in range(1000)]' 'longstr=\"\"' 'for x in strings:' ' longstr += str(x)'\npython -m timeit -s 'strings = [str(x) for x in range(1000)]' '\"\".join(strings)'\n```", "```py\n2000 loops, best of 5: 173 usec per loop\n50000 loops, best of 5: 6.91 usec per loop\n```", "```py\npython -m timeit '[x**0.5 for x in range(1000)]'\npython -m timeit -s 'from math import sqrt' '[sqrt(x) for x in range(1000)]'\npython -m timeit -s 'from numpy import sqrt' '[sqrt(x) for x in range(1000)]'\n```", "```py\n5000 loops, best of 5: 93.2 usec per loop\n5000 loops, best of 5: 72.3 usec per loop\n200 loops, best of 5: 974 usec per loop\n```", "```py\npython -m timeit -s 'import numpy as np; x=np.array(range(1000))' 'np.sqrt(x)'\n```", "```py\n100000 loops, best of 5: 2.08 usec per loop\n```", "```py\nimport timeit\nmeasurements = timeit.repeat('[x**0.5 for x in range(1000)]', number=10000)\nprint(measurements)\n```", "```py\n[1.0888952040000106, 0.9799715450000122, 1.0921516899999801, 1.0946189250000202, 1.2792069260000005]\n```", "```py\n# manually search perceptron hyperparameters for binary classification\nfrom numpy import mean\nfrom numpy.random import randn\nfrom numpy.random import rand\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.linear_model import Perceptron\n\n# objective function\ndef objective(X, y, cfg):\n\t# unpack config\n\teta, alpha = cfg\n\t# define model\n\tmodel = Perceptron(penalty='elasticnet', alpha=alpha, eta0=eta)\n\t# define evaluation procedure\n\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\t# evaluate model\n\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n\t# calculate mean accuracy\n\tresult = mean(scores)\n\treturn result\n\n# take a step in the search space\ndef step(cfg, step_size):\n\t# unpack the configuration\n\teta, alpha = cfg\n\t# step eta\n\tnew_eta = eta + randn() * step_size\n\t# check the bounds of eta\n\tif new_eta <= 0.0:\n\t\tnew_eta = 1e-8\n\tif new_eta > 1.0:\n\t\tnew_eta = 1.0\n\t# step alpha\n\tnew_alpha = alpha + randn() * step_size\n\t# check the bounds of alpha\n\tif new_alpha < 0.0:\n\t\tnew_alpha = 0.0\n\t# return the new configuration\n\treturn [new_eta, new_alpha]\n\n# hill climbing local search algorithm\ndef hillclimbing(X, y, objective, n_iter, step_size):\n\t# starting point for the search\n\tsolution = [rand(), rand()]\n\t# evaluate the initial point\n\tsolution_eval = objective(X, y, solution)\n\t# run the hill climb\n\tfor i in range(n_iter):\n\t\t# take a step\n\t\tcandidate = step(solution, step_size)\n\t\t# evaluate candidate point\n\t\tcandidate_eval = objective(X, y, candidate)\n\t\t# check if we should keep the new point\n\t\tif candidate_eval >= solution_eval:\n\t\t\t# store the new point\n\t\t\tsolution, solution_eval = candidate, candidate_eval\n\t\t\t# report progress\n\t\t\tprint('>%d, cfg=%s %.5f' % (i, solution, solution_eval))\n\treturn [solution, solution_eval]\n\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=5, n_informative=2, n_redundant=1, random_state=1)\n# define the total iterations\nn_iter = 100\n# step size in the search space\nstep_size = 0.1\n# perform the hill climbing search\ncfg, score = hillclimbing(X, y, objective, n_iter, step_size)\nprint('Done!')\nprint('cfg=%s: Mean Accuracy: %f' % (cfg, score))\n```", "```py\npython -m cProfile hillclimb.py\n```", "```py\n>10, cfg=[0.3792455490265847, 0.21589566352848377] 0.78400\n>17, cfg=[0.49105438202347707, 0.1342150084854657] 0.79833\n>26, cfg=[0.5737524712834843, 0.016749795596210315] 0.80033\n>47, cfg=[0.5067828976025809, 0.05280380038497864] 0.80133\n>48, cfg=[0.5427345321546029, 0.0049895870979695875] 0.81167\nDone!\ncfg=[0.5427345321546029, 0.0049895870979695875]: Mean Accuracy: 0.811667\n         2686451 function calls (2638255 primitive calls) in 5.500 seconds\n\n   Ordered by: standard name\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      101    0.001    0.000    4.892    0.048 hillclimb.py:11(objective)\n        1    0.000    0.000    5.501    5.501 hillclimb.py:2(<module>)\n      100    0.000    0.000    0.001    0.000 hillclimb.py:25(step)\n        1    0.001    0.001    4.894    4.894 hillclimb.py:44(hillclimbing)\n        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(<module>)\n      303    0.000    0.000    0.008    0.000 <__array_function__ internals>:2(all)\n      303    0.000    0.000    0.005    0.000 <__array_function__ internals>:2(amin)\n        2    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(any)\n        4    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(atleast_1d)\n     3333    0.003    0.000    0.018    0.000 <__array_function__ internals>:2(bincount)\n      103    0.000    0.000    0.001    0.000 <__array_function__ internals>:2(concatenate)\n        3    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(copyto)\n      606    0.001    0.000    0.010    0.000 <__array_function__ internals>:2(cumsum)\n        6    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(dot)\n        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(empty_like)\n        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(inv)\n        2    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(linspace)\n        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(lstsq)\n      101    0.000    0.000    0.005    0.000 <__array_function__ internals>:2(mean)\n        2    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(ndim)\n        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(outer)\n        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(polyfit)\n        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(polyval)\n        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(prod)\n      303    0.000    0.000    0.002    0.000 <__array_function__ internals>:2(ravel)\n        2    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(result_type)\n      303    0.001    0.000    0.001    0.000 <__array_function__ internals>:2(shape)\n      303    0.000    0.000    0.035    0.000 <__array_function__ internals>:2(sort)\n        4    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(trim_zeros)\n     1617    0.002    0.000    0.112    0.000 <__array_function__ internals>:2(unique)\n...\n```", "```py\npython -m cProfile -s ncalls hillclimb.py\n```", "```py\n         2685349 function calls (2637153 primitive calls) in 5.609 seconds\n\n   Ordered by: call count\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n   247588    0.029    0.000    0.029    0.000 {method 'get' of 'dict' objects}\n   246196    0.028    0.000    0.028    0.000 inspect.py:2548(name)\n   168057    0.018    0.000    0.018    0.000 {method 'append' of 'list' objects}\n   161738    0.018    0.000    0.018    0.000 inspect.py:2560(kind)\n   144431    0.021    0.000    0.029    0.000 {built-in method builtins.isinstance}\n   142213    0.030    0.000    0.031    0.000 {built-in method builtins.getattr}\n...\n```", "```py\npython -m cProfile -o hillclimb.stats hillclimb.py\n```", "```py\npython -m pstats hillclimb.stats\n```", "```py\nWelcome to the profile statistics browser.\nhillclimb.stat% help\n\nDocumented commands (type help <topic>):\n========================================\nEOF  add  callees  callers  help  quit  read  reverse  sort  stats  strip\n\nhillclimb.stat% sort ncall\nhillclimb.stat% stats hillclimb\nThu Jan 13 16:44:10 2022    hillclimb.stat\n\n         2686227 function calls (2638031 primitive calls) in 5.582 seconds\n\n   Ordered by: call count\n   List reduced from 3456 to 4 due to restriction <'hillclimb'>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      101    0.001    0.000    4.951    0.049 hillclimb.py:11(objective)\n      100    0.000    0.000    0.001    0.000 hillclimb.py:25(step)\n        1    0.000    0.000    5.583    5.583 hillclimb.py:2(<module>)\n        1    0.000    0.000    4.952    4.952 hillclimb.py:44(hillclimbing)\n\nhillclimb.stat%\n```", "```py\nhillclimb.stat% callers objective\n   Ordered by: call count\n   List reduced from 3456 to 1 due to restriction <'objective'>\n\nFunction                    was called by...\n                                ncalls  tottime  cumtime\nhillclimb.py:11(objective)  <-     101    0.001    4.951  hillclimb.py:44(hillclimbing)\n\nhillclimb.stat% callees hillclimbing\n   Ordered by: call count\n   List reduced from 3456 to 1 due to restriction <'hillclimbing'>\n\nFunction                       called...\n                                   ncalls  tottime  cumtime\nhillclimb.py:44(hillclimbing)  ->     101    0.001    4.951  hillclimb.py:11(objective)\n                                      100    0.000    0.001  hillclimb.py:25(step)\n                                        4    0.000    0.000  {built-in method builtins.print}\n                                        2    0.000    0.000  {method 'rand' of 'numpy.random.mtrand.RandomState' objects}\n\nhillclimb.stat%\n```", "```py\n# manually search perceptron hyperparameters for binary classification\nimport cProfile as profile\nimport pstats\nfrom numpy import mean\nfrom numpy.random import randn\nfrom numpy.random import rand\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.linear_model import Perceptron\n\n# objective function\ndef objective(X, y, cfg):\n\t# unpack config\n\teta, alpha = cfg\n\t# define model\n\tmodel = Perceptron(penalty='elasticnet', alpha=alpha, eta0=eta)\n\t# define evaluation procedure\n\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\t# evaluate model\n\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n\t# calculate mean accuracy\n\tresult = mean(scores)\n\treturn result\n\n# take a step in the search space\ndef step(cfg, step_size):\n\t# unpack the configuration\n\teta, alpha = cfg\n\t# step eta\n\tnew_eta = eta + randn() * step_size\n\t# check the bounds of eta\n\tif new_eta <= 0.0:\n\t\tnew_eta = 1e-8\n\tif new_eta > 1.0:\n\t\tnew_eta = 1.0\n\t# step alpha\n\tnew_alpha = alpha + randn() * step_size\n\t# check the bounds of alpha\n\tif new_alpha < 0.0:\n\t\tnew_alpha = 0.0\n\t# return the new configuration\n\treturn [new_eta, new_alpha]\n\n# hill climbing local search algorithm\ndef hillclimbing(X, y, objective, n_iter, step_size):\n\t# starting point for the search\n\tsolution = [rand(), rand()]\n\t# evaluate the initial point\n\tsolution_eval = objective(X, y, solution)\n\t# run the hill climb\n\tfor i in range(n_iter):\n\t\t# take a step\n\t\tcandidate = step(solution, step_size)\n\t\t# evaluate candidate point\n\t\tcandidate_eval = objective(X, y, candidate)\n\t\t# check if we should keep the new point\n\t\tif candidate_eval >= solution_eval:\n\t\t\t# store the new point\n\t\t\tsolution, solution_eval = candidate, candidate_eval\n\t\t\t# report progress\n\t\t\tprint('>%d, cfg=%s %.5f' % (i, solution, solution_eval))\n\treturn [solution, solution_eval]\n\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=5, n_informative=2, n_redundant=1, random_state=1)\n# define the total iterations\nn_iter = 100\n# step size in the search space\nstep_size = 0.1\n# perform the hill climbing search with profiling\nprof = profile.Profile()\nprof.enable()\ncfg, score = hillclimbing(X, y, objective, n_iter, step_size)\nprof.disable()\n# print program output\nprint('Done!')\nprint('cfg=%s: Mean Accuracy: %f' % (cfg, score))\n# print profiling output\nstats = pstats.Stats(prof).strip_dirs().sort_stats(\"cumtime\")\nstats.print_stats(10) # top 10 rows\n```", "```py\n>0, cfg=[0.3776271076534661, 0.2308364063203663] 0.75700\n>3, cfg=[0.35803234662466354, 0.03204434939660264] 0.77567\n>8, cfg=[0.3001050823005957, 0.0] 0.78633\n>10, cfg=[0.39518618870158934, 0.0] 0.78633\n>12, cfg=[0.4291267905390187, 0.0] 0.78633\n>13, cfg=[0.4403131521968569, 0.0] 0.78633\n>16, cfg=[0.38865272555918756, 0.0] 0.78633\n>17, cfg=[0.38871654921891885, 0.0] 0.78633\n>18, cfg=[0.4542440671724224, 0.0] 0.78633\n>19, cfg=[0.44899743344802734, 0.0] 0.78633\n>20, cfg=[0.5855375509507891, 0.0] 0.78633\n>21, cfg=[0.5935318064858227, 0.0] 0.78633\n>23, cfg=[0.7606367310048543, 0.0] 0.78633\n>24, cfg=[0.855444293727846, 0.0] 0.78633\n>25, cfg=[0.9505501566826242, 0.0] 0.78633\n>26, cfg=[1.0, 0.0244821888204496] 0.79800\nDone!\ncfg=[1.0, 0.0244821888204496]: Mean Accuracy: 0.798000\n         2179559 function calls (2140124 primitive calls) in 4.941 seconds\n\n   Ordered by: cumulative time\n   List reduced from 581 to 10 due to restriction <10>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    0.001    0.001    4.941    4.941 hillclimb.py:46(hillclimbing)\n      101    0.001    0.000    4.939    0.049 hillclimb.py:13(objective)\n      101    0.001    0.000    4.931    0.049 _validation.py:375(cross_val_score)\n      101    0.002    0.000    4.930    0.049 _validation.py:48(cross_validate)\n      101    0.005    0.000    4.903    0.049 parallel.py:960(__call__)\n      101    0.235    0.002    3.089    0.031 parallel.py:920(retrieve)\n     3030    0.004    0.000    2.849    0.001 _parallel_backends.py:537(wrap_future_result)\n     3030    0.020    0.000    2.845    0.001 _base.py:417(result)\n     2602    0.016    0.000    2.819    0.001 threading.py:280(wait)\n    12447    2.796    0.000    2.796    0.000 {method 'acquire' of '_thread.lock' objects}\n```", "```py\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dense, AveragePooling2D, Flatten\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Load and reshape data to shape of (n_sample, height, width, n_channel)\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = np.expand_dims(X_train, axis=3).astype('float32')\nX_test = np.expand_dims(X_test, axis=3).astype('float32')\n\n# One-hot encode the output\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\n# LeNet5 model\nmodel = Sequential([\n    Conv2D(6, (5,5), input_shape=(28,28,1), padding=\"same\", activation=\"tanh\"),\n    AveragePooling2D((2,2), strides=2),\n    Conv2D(16, (5,5), activation=\"tanh\"),\n    AveragePooling2D((2,2), strides=2),\n    Conv2D(120, (5,5), activation=\"tanh\"),\n    Flatten(),\n    Dense(84, activation=\"tanh\"),\n    Dense(10, activation=\"softmax\")\n])\nmodel.summary(line_length=100)\n\n# Training\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nearlystopping = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32, callbacks=[earlystopping])\n\n# Evaluate\nprint(model.evaluate(X_test, y_test, verbose=0))\n```", "```py\n         5962698 function calls (5728324 primitive calls) in 39.674 seconds\n\n   Ordered by: cumulative time\n   List reduced from 12295 to 15 due to restriction <15>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n   3212/1    0.013    0.000   39.699   39.699 {built-in method builtins.exec}\n        1    0.003    0.003   39.699   39.699 mnist.py:4(<module>)\n     52/4    0.005    0.000   35.470    8.868 /usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:58(error_handler)\n        1    0.089    0.089   34.334   34.334 /usr/local/lib/python3.9/site-packages/keras/engine/training.py:901(fit)\n11075/9531    0.032    0.000   33.406    0.004 /usr/local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:138(error_handler)\n     4689    0.089    0.000   33.017    0.007 /usr/local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:882(__call__)\n     4689    0.023    0.000   32.771    0.007 /usr/local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:929(_call)\n     4688    0.042    0.000   32.134    0.007 /usr/local/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3125(__call__)\n     4689    0.075    0.000   30.941    0.007 /usr/local/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1888(_call_flat)\n     4689    0.158    0.000   30.472    0.006 /usr/local/lib/python3.9/site-packages/tensorflow/python/eager/function.py:553(call)\n     4689    0.034    0.000   30.152    0.006 /usr/local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:33(quick_execute)\n     4689   30.105    0.006   30.105    0.006 {built-in method tensorflow.python._pywrap_tfe.TFE_Py_Execute}\n  3185/24    0.021    0.000    3.902    0.163 <frozen importlib._bootstrap>:1002(_find_and_load)\n  3169/10    0.014    0.000    3.901    0.390 <frozen importlib._bootstrap>:967(_find_and_load_unlocked)\n  2885/12    0.009    0.000    3.901    0.325 <frozen importlib._bootstrap_external>:844(exec_module)\n```"]