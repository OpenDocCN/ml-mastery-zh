["```py\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n```", "```py\n...\n# Creating a function f(X) with a slope of -5\nX = torch.arange(-5, 5, 0.1).view(-1, 1)\nfunc = -5 * X\n```", "```py\n...\n# Plot the line in red with grids\nplt.plot(X.numpy(), func.numpy(), 'r', label='func')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.grid('True', color='y')\nplt.show()\n```", "```py\n...\n# Adding Gaussian noise to the function f(X) and saving it in Y\nY = func + 0.4 * torch.randn(X.size())\n```", "```py\n# Plot and visualizing the data points in blue\nplt.plot(X.numpy(), Y.numpy(), 'b+', label='Y')\nplt.plot(X.numpy(), func.numpy(), 'r', label='func')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.grid('True', color='y')\nplt.show()\n```", "```py\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Creating a function f(X) with a slope of -5\nX = torch.arange(-5, 5, 0.1).view(-1, 1)\nfunc = -5 * X\n\n# Adding Gaussian noise to the function f(X) and saving it in Y\nY = func + 0.4 * torch.randn(X.size())\n\n# Plot and visualizing the data points in blue\nplt.plot(X.numpy(), Y.numpy(), 'b+', label='Y')\nplt.plot(X.numpy(), func.numpy(), 'r', label='func')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.grid('True', color='y')\nplt.show()\n```", "```py\n# defining the function for forward pass for prediction\ndef forward(x):\n    return w * x\n```", "```py\n# evaluating data points with Mean Square Error.\ndef criterion(y_pred, y):\n    return torch.mean((y_pred - y) ** 2)\n```", "```py\nw = torch.tensor(-10.0, requires_grad=True)\n```", "```py\nstep_size = 0.1\nloss_list = []\niter = 20\n```", "```py\nfor i in range (iter):\n    # making predictions with forward pass\n    Y_pred = forward(X)\n    # calculating the loss between original and predicted data points\n    loss = criterion(Y_pred, Y)\n    # storing the calculated loss in a list\n    loss_list.append(loss.item())\n    # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n    loss.backward()\n    # updateing the parameters after each iteration\n    w.data = w.data - step_size * w.grad.data\n    # zeroing gradients after each iteration\n    w.grad.data.zero_()\n    # priting the values for understanding\n    print('{},\\t{},\\t{}'.format(i, loss.item(), w.item()))\n```", "```py\n0,\t207.40255737304688,\t-1.6875505447387695\n1,\t92.3563003540039,\t-7.231954097747803\n2,\t41.173553466796875,\t-3.5338361263275146\n3,\t18.402894973754883,\t-6.000481128692627\n4,\t8.272472381591797,\t-4.355228900909424\n5,\t3.7655599117279053,\t-5.452612400054932\n6,\t1.7604843378067017,\t-4.7206573486328125\n7,\t0.8684477210044861,\t-5.208871364593506\n8,\t0.471589595079422,\t-4.883232593536377\n9,\t0.2950323224067688,\t-5.100433826446533\n10,\t0.21648380160331726,\t-4.955560684204102\n11,\t0.1815381944179535,\t-5.052190780639648\n12,\t0.16599132120609283,\t-4.987738609313965\n13,\t0.15907476842403412,\t-5.030728340148926\n14,\t0.15599775314331055,\t-5.002054214477539\n15,\t0.15462875366210938,\t-5.021179676055908\n16,\t0.15401971340179443,\t-5.008423328399658\n17,\t0.15374873578548431,\t-5.016931533813477\n18,\t0.15362821519374847,\t-5.011256694793701\n19,\t0.15357455611228943,\t-5.015041828155518\n```", "```py\n# Plotting the loss after each iteration\nplt.plot(loss_list, 'r')\nplt.tight_layout()\nplt.grid('True', color='y')\nplt.xlabel(\"Epochs/Iterations\")\nplt.ylabel(\"Loss\")\nplt.show()\n```", "```py\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX = torch.arange(-5, 5, 0.1).view(-1, 1)\nfunc = -5 * X\nY = func + 0.4 * torch.randn(X.size())\n\n# defining the function for forward pass for prediction\ndef forward(x):\n    return w * x\n\n# evaluating data points with Mean Square Error\ndef criterion(y_pred, y):\n    return torch.mean((y_pred - y) ** 2)\n\nw = torch.tensor(-10.0, requires_grad=True)\n\nstep_size = 0.1\nloss_list = []\niter = 20\n\nfor i in range (iter):\n    # making predictions with forward pass\n    Y_pred = forward(X)\n    # calculating the loss between original and predicted data points\n    loss = criterion(Y_pred, Y)\n    # storing the calculated loss in a list\n    loss_list.append(loss.item())\n    # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n    loss.backward()\n    # updateing the parameters after each iteration\n    w.data = w.data - step_size * w.grad.data\n    # zeroing gradients after each iteration\n    w.grad.data.zero_()\n    # priting the values for understanding\n    print('{},\\t{},\\t{}'.format(i, loss.item(), w.item()))\n\n# Plotting the loss after each iteration\nplt.plot(loss_list, 'r')\nplt.tight_layout()\nplt.grid('True', color='y')\nplt.xlabel(\"Epochs/Iterations\")\nplt.ylabel(\"Loss\")\nplt.show()\n```", "```py\n# defining the function for forward pass for prediction\ndef forward(x):\n    return w * x + b\n```", "```py\nw = torch.tensor(-10.0, requires_grad = True)\nb = torch.tensor(-20.0, requires_grad = True)\n```", "```py\nstep_size = 0.1\nloss_list = []\niter = 20\n\nfor i in range (iter):    \n    # making predictions with forward pass\n    Y_pred = forward(X)\n    # calculating the loss between original and predicted data points\n    loss = criterion(Y_pred, Y)\n    # storing the calculated loss in a list\n    loss_list.append(loss.item())\n    # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n    loss.backward()\n    # updateing the parameters after each iteration\n    w.data = w.data - step_size * w.grad.data\n    b.data = b.data - step_size * b.grad.data\n    # zeroing gradients after each iteration\n    w.grad.data.zero_()\n    b.grad.data.zero_()\n    # priting the values for understanding\n    print('{}, \\t{}, \\t{}, \\t{}'.format(i, loss.item(), w.item(), b.item()))\n```", "```py\n0, \t598.0744018554688, \t-1.8875503540039062, \t-16.046640396118164\n1, \t344.6290283203125, \t-7.2590203285217285, \t-12.802828788757324\n2, \t203.6309051513672, \t-3.6438119411468506, \t-10.261493682861328\n3, \t122.82559204101562, \t-6.029742240905762, \t-8.19227409362793\n4, \t75.30597686767578, \t-4.4176344871521, \t-6.560757637023926\n5, \t46.759193420410156, \t-5.476595401763916, \t-5.2394232749938965\n6, \t29.318675994873047, \t-4.757054805755615, \t-4.19294548034668\n7, \t18.525297164916992, \t-5.2265238761901855, \t-3.3485677242279053\n8, \t11.781207084655762, \t-4.90494441986084, \t-2.677760124206543\n9, \t7.537606239318848, \t-5.112729549407959, \t-2.1378984451293945\n10, \t4.853880405426025, \t-4.968738555908203, \t-1.7080869674682617\n11, \t3.1505300998687744, \t-5.060482025146484, \t-1.3627978563308716\n12, \t2.0666630268096924, \t-4.99583625793457, \t-1.0874838829040527\n13, \t1.3757448196411133, \t-5.0362019538879395, \t-0.8665863275527954\n14, \t0.9347621202468872, \t-5.007069110870361, \t-0.6902718544006348\n15, \t0.6530535817146301, \t-5.024737358093262, \t-0.5489290356636047\n16, \t0.4729837477207184, \t-5.011539459228516, \t-0.43603143095970154\n17, \t0.3578317165374756, \t-5.0192131996154785, \t-0.34558138251304626\n18, \t0.28417202830314636, \t-5.013190746307373, \t-0.27329811453819275\n19, \t0.23704445362091064, \t-5.01648473739624, \t-0.2154112160205841\n```", "```py\n# Plotting the loss after each iteration\nplt.plot(loss_list, 'r')\nplt.tight_layout()\nplt.grid('True', color='y')\nplt.xlabel(\"Epochs/Iterations\")\nplt.ylabel(\"Loss\")\nplt.show()\n```", "```py\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX = torch.arange(-5, 5, 0.1).view(-1, 1)\nfunc = -5 * X\nY = func + 0.4 * torch.randn(X.size())\n\n# defining the function for forward pass for prediction\ndef forward(x):\n    return w * x + b\n\n# evaluating data points with Mean Square Error.\ndef criterion(y_pred, y):\n    return torch.mean((y_pred - y) ** 2)\n\nw = torch.tensor(-10.0, requires_grad=True)\nb = torch.tensor(-20.0, requires_grad=True)\n\nstep_size = 0.1\nloss_list = []\niter = 20\n\nfor i in range (iter):    \n    # making predictions with forward pass\n    Y_pred = forward(X)\n    # calculating the loss between original and predicted data points\n    loss = criterion(Y_pred, Y)\n    # storing the calculated loss in a list\n    loss_list.append(loss.item())\n    # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n    loss.backward()\n    # updateing the parameters after each iteration\n    w.data = w.data - step_size * w.grad.data\n    b.data = b.data - step_size * b.grad.data\n    # zeroing gradients after each iteration\n    w.grad.data.zero_()\n    b.grad.data.zero_()\n    # priting the values for understanding\n    print('{}, \\t{}, \\t{}, \\t{}'.format(i, loss.item(), w.item(), b.item()))\n\n# Plotting the loss after each iteration\nplt.plot(loss_list, 'r')\nplt.tight_layout()\nplt.grid('True', color='y')\nplt.xlabel(\"Epochs/Iterations\")\nplt.ylabel(\"Loss\")\nplt.show()\n```"]