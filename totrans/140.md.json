["```py\nimport numpy as np\nimport torch\n\n# load the dataset\ndataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\nX = dataset[:,0:8]\ny = dataset[:,8]\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n\n# split the dataset into training and test sets\nXtrain = X[:700]\nytrain = y[:700]\nXtest = X[700:]\nytest = y[700:]\n```", "```py\nimport torch.nn as nn\nimport torch.optim as optim\n\nmodel = nn.Sequential(\n    nn.Linear(8, 12),\n    nn.ReLU(),\n    nn.Linear(12, 8),\n    nn.ReLU(),\n    nn.Linear(8, 1),\n    nn.Sigmoid()\n)\nprint(model)\n\n# loss function and optimizer\nloss_fn = nn.BCELoss()  # binary cross entropy\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n```", "```py\nn_epochs = 50    # number of epochs to run\nbatch_size = 10  # size of each batch\nbatches_per_epoch = len(Xtrain) // batch_size\n\nfor epoch in range(n_epochs):\n    for i in range(batches_per_epoch):\n        start = i * batch_size\n        # take a batch\n        Xbatch = Xtrain[start:start+batch_size]\n        ybatch = ytrain[start:start+batch_size]\n        # forward pass\n        y_pred = model(Xbatch)\n        loss = loss_fn(y_pred, ybatch)\n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        # update weights\n        optimizer.step()\n```", "```py\n...\n\n# evaluate trained model with test set\nwith torch.no_grad():\n    y_pred = model(X)\naccuracy = (y_pred.round() == y).float().mean()\nprint(\"Accuracy {:.2f}\".format(accuracy * 100))\n```", "```py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# load the dataset\ndataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\nX = dataset[:,0:8]\ny = dataset[:,8]\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n\n# split the dataset into training and test sets\nXtrain = X[:700]\nytrain = y[:700]\nXtest = X[700:]\nytest = y[700:]\n\nmodel = nn.Sequential(\n    nn.Linear(8, 12),\n    nn.ReLU(),\n    nn.Linear(12, 8),\n    nn.ReLU(),\n    nn.Linear(8, 1),\n    nn.Sigmoid()\n)\nprint(model)\n\n# loss function and optimizer\nloss_fn = nn.BCELoss()  # binary cross entropy\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nn_epochs = 50    # number of epochs to run\nbatch_size = 10  # size of each batch\nbatches_per_epoch = len(Xtrain) // batch_size\n\nfor epoch in range(n_epochs):\n    for i in range(batches_per_epoch):\n        start = i * batch_size\n        # take a batch\n        Xbatch = Xtrain[start:start+batch_size]\n        ybatch = ytrain[start:start+batch_size]\n        # forward pass\n        y_pred = model(Xbatch)\n        loss = loss_fn(y_pred, ybatch)\n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        # update weights\n        optimizer.step()\n\n# evaluate trained model with test set\nwith torch.no_grad():\n    y_pred = model(X)\naccuracy = (y_pred.round() == y).float().mean()\nprint(\"Accuracy {:.2f}\".format(accuracy * 100))\n```", "```py\nn_epochs = 50    # number of epochs to run\nbatch_size = 10  # size of each batch\nbatches_per_epoch = len(Xtrain) // batch_size\n\n# collect statistics\ntrain_loss = []\ntrain_acc = []\ntest_acc = []\n\nfor epoch in range(n_epochs):\n    for i in range(batches_per_epoch):\n        start = i * batch_size\n        # take a batch\n        Xbatch = Xtrain[start:start+batch_size]\n        ybatch = ytrain[start:start+batch_size]\n        # forward pass\n        y_pred = model(Xbatch)\n        loss = loss_fn(y_pred, ybatch)\n        acc = (y_pred.round() == ybatch).float().mean()\n        # store metrics\n        train_loss.append(float(loss))\n        train_acc.append(float(acc))\n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        # update weights\n        optimizer.step()\n        # print progress\n        print(f\"epoch {epoch} step {i} loss {loss} accuracy {acc}\")\n    # evaluate model at end of epoch\n    y_pred = model(Xtest)\n    acc = (y_pred.round() == ytest).float().mean()\n    test_acc.append(float(acc))\n    print(f\"End of {epoch}, accuracy {acc}\")\n```", "```py\nimport matplotlib.pyplot as plt\n\n# Plot the loss metrics, set the y-axis to start from 0\nplt.plot(train_loss)\nplt.xlabel(\"steps\")\nplt.ylabel(\"loss\")\nplt.ylim(0)\nplt.show()\n\n# plot the accuracy metrics\navg_train_acc = []\nfor i in range(n_epochs):\n    start = i * batch_size\n    average = sum(train_acc[start:start+batches_per_epoch]) / batches_per_epoch\n    avg_train_acc.append(average)\n\nplt.plot(avg_train_acc, label=\"train\")\nplt.plot(test_acc, label=\"test\")\nplt.xlabel(\"epochs\")\nplt.ylabel(\"accuracy\")\nplt.ylim(0)\nplt.show()\n```", "```py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# load the dataset\ndataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',') # split into input (X) and output (y) variables\nX = dataset[:,0:8]\ny = dataset[:,8]\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n\n# split the dataset into training and test sets\nXtrain = X[:700]\nytrain = y[:700]\nXtest = X[700:]\nytest = y[700:]\n\nmodel = nn.Sequential(\n    nn.Linear(8, 12),\n    nn.ReLU(),\n    nn.Linear(12, 8),\n    nn.ReLU(),\n    nn.Linear(8, 1),\n    nn.Sigmoid()\n)\nprint(model)\n\n# loss function and optimizer\nloss_fn = nn.BCELoss()  # binary cross entropy\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\nn_epochs = 50    # number of epochs to run\nbatch_size = 10  # size of each batch\nbatches_per_epoch = len(Xtrain) // batch_size\n\n# collect statistics\ntrain_loss = []\ntrain_acc = []\ntest_acc = []\n\nfor epoch in range(n_epochs):\n    for i in range(batches_per_epoch):\n        # take a batch\n        start = i * batch_size\n        Xbatch = Xtrain[start:start+batch_size]\n        ybatch = ytrain[start:start+batch_size]\n        # forward pass\n        y_pred = model(Xbatch)\n        loss = loss_fn(y_pred, ybatch)\n        acc = (y_pred.round() == ybatch).float().mean()\n        # store metrics\n        train_loss.append(float(loss))\n        train_acc.append(float(acc))\n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        # update weights\n        optimizer.step()\n        # print progress\n        print(f\"epoch {epoch} step {i} loss {loss} accuracy {acc}\")\n    # evaluate model at end of epoch\n    y_pred = model(Xtest)\n    acc = (y_pred.round() == ytest).float().mean()\n    test_acc.append(float(acc))\n    print(f\"End of {epoch}, accuracy {acc}\")\n\nimport matplotlib.pyplot as plt\n\n# Plot the loss metrics\nplt.plot(train_loss)\nplt.xlabel(\"steps\")\nplt.ylabel(\"loss\")\nplt.ylim(0)\nplt.show()\n\n# plot the accuracy metrics\navg_train_acc = []\nfor i in range(n_epochs):\n    start = i * batch_size\n    average = sum(train_acc[start:start+batches_per_epoch]) / batches_per_epoch\n    avg_train_acc.append(average)\n\nplt.plot(avg_train_acc, label=\"train\")\nplt.plot(test_acc, label=\"test\")\nplt.xlabel(\"epochs\")\nplt.ylabel(\"accuracy\")\nplt.ylim(0)\nplt.show()\n```", "```py\nfor epoch in range(n_epochs):\n    with tqdm.trange(batches_per_epoch, unit=\"batch\", mininterval=0) as bar:\n        bar.set_description(f\"Epoch {epoch}\")\n        for i in bar:\n            # take a batch\n            start = i * batch_size\n            Xbatch = Xtrain[start:start+batch_size]\n            ybatch = ytrain[start:start+batch_size]\n            # forward pass\n            y_pred = model(Xbatch)\n            loss = loss_fn(y_pred, ybatch)\n            acc = (y_pred.round() == ybatch).float().mean()\n            # store metrics\n            train_loss.append(float(loss))\n            train_acc.append(float(acc))\n            # backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            # update weights\n            optimizer.step()\n            # print progress\n            bar.set_postfix(\n                loss=float(loss),\n                acc=f\"{float(acc)*100:.2f}%\"\n            )\n    # evaluate model at end of epoch\n    y_pred = model(Xtest)\n    acc = (y_pred.round() == ytest).float().mean()\n    test_acc.append(float(acc))\n    print(f\"End of {epoch}, accuracy {acc}\")\n```", "```py\nstarts = [i*batch_size for i in range(batches_per_epoch)]\n\nfor epoch in range(n_epochs):\n    with tqdm.tqdm(starts, unit=\"batch\", mininterval=0) as bar:\n        bar.set_description(f\"Epoch {epoch}\")\n        for start in bar:\n            # take a batch\n            Xbatch = Xtrain[start:start+batch_size]\n            ybatch = ytrain[start:start+batch_size]\n            # forward pass\n            y_pred = model(Xbatch)\n            loss = loss_fn(y_pred, ybatch)\n            acc = (y_pred.round() == ybatch).float().mean()\n            # store metrics\n            train_loss.append(float(loss))\n            train_acc.append(float(acc))\n            # backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            # update weights\n            optimizer.step()\n            # print progress\n            bar.set_postfix(\n                loss=float(loss),\n                acc=f\"{float(acc)*100:.2f}%\"\n            )\n    # evaluate model at end of epoch\n    y_pred = model(Xtest)\n    acc = (y_pred.round() == ytest).float().mean()\n    test_acc.append(float(acc))\n    print(f\"End of {epoch}, accuracy {acc}\")\n```", "```py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport tqdm\n\n# load the dataset\ndataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',') # split into input (X) and output (y) variables\nX = dataset[:,0:8]\ny = dataset[:,8]\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n\n# split the dataset into training and test sets\nXtrain = X[:700]\nytrain = y[:700]\nXtest = X[700:]\nytest = y[700:]\n\nmodel = nn.Sequential(\n    nn.Linear(8, 12),\n    nn.ReLU(),\n    nn.Linear(12, 8),\n    nn.ReLU(),\n    nn.Linear(8, 1),\n    nn.Sigmoid()\n)\nprint(model)\n\n# loss function and optimizer\nloss_fn = nn.BCELoss()  # binary cross entropy\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\nn_epochs = 50    # number of epochs to run\nbatch_size = 10  # size of each batch\nbatches_per_epoch = len(Xtrain) // batch_size\n\n# collect statistics\ntrain_loss = []\ntrain_acc = []\ntest_acc = []\n\nfor epoch in range(n_epochs):\n    with tqdm.trange(batches_per_epoch, unit=\"batch\", mininterval=0) as bar:\n        bar.set_description(f\"Epoch {epoch}\")\n        for i in bar:\n            # take a batch\n            start = i * batch_size\n            Xbatch = Xtrain[start:start+batch_size]\n            ybatch = ytrain[start:start+batch_size]\n            # forward pass\n            y_pred = model(Xbatch)\n            loss = loss_fn(y_pred, ybatch)\n            acc = (y_pred.round() == ybatch).float().mean()\n            # store metrics\n            train_loss.append(float(loss))\n            train_acc.append(float(acc))\n            # backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            # update weights\n            optimizer.step()\n            # print progress\n            bar.set_postfix(\n                loss=float(loss),\n                acc=f\"{float(acc)*100:.2f}%\"\n            )\n    # evaluate model at end of epoch\n    y_pred = model(Xtest)\n    acc = (y_pred.round() == ytest).float().mean()\n    test_acc.append(float(acc))\n    print(f\"End of {epoch}, accuracy {acc}\")\n```"]