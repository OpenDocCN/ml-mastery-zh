["```py\nsudo pip install torch torchvision\n```", "```py\n# Example of PyTorch library\nimport torch\n# declare two symbolic floating-point scalars\na = torch.tensor(1.5)\nb = torch.tensor(2.5)\n# create a simple symbolic expression using the add function\nc = torch.add(a, b)\nprint(c)\n```", "```py\nimport torch\nprint(torch.__version__)\n```", "```py\nimport torch.nn as nn\n\nmodel = nn.Sequential(\n  nn.Linear(8, 12),\n  nn.ReLU(),\n  nn.Linear(12, 8),\n  nn.ReLU(),\n  nn.Linear(8, 1),\n  nn.Sigmoid()\n)\nprint(model)\n```", "```py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ndataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\nX = dataset[:,0:8]\ny = dataset[:,8]\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n\nloss_fn = nn.BCELoss() # binary cross-entropy\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nn_epochs = 100\nbatch_size = 10\nfor epoch in range(n_epochs):\n    for i in range(0, len(X), batch_size):\n        Xbatch = X[i:i+batch_size]\n        y_pred = model(Xbatch)\n        ybatch = y[i:i+batch_size]\n        loss = loss_fn(y_pred, ybatch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(f'Finished epoch {epoch}, latest loss {loss}')\n```", "```py\ni = 5\nX_sample = X[i:i+1]\ny_pred = model(X_sample)\nprint(f\"{X_sample[0]} -> {y_pred[0]}\")\n```", "```py\ni = 5\nX_sample = X[i:i+1]\nmodel.eval()\nwith torch.no_grad():\n    y_pred = model(X_sample)\nprint(f\"{X_sample[0]} -> {y_pred[0]}\")\n```", "```py\nmodel.eval()\nwith torch.no_grad():\n    y_pred = model(X)\naccuracy = (y_pred.round() == y).float().mean()\nprint(f\"Accuracy {accuracy}\")\n```", "```py\nimport matplotlib.pyplot as plt\nimport torchvision\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n\nfig, ax = plt.subplots(4, 6, sharex=True, sharey=True, figsize=(12,8))\nfor i in range(0, 24):\n    row, col = i//6, i%6\n    ax[row][col].imshow(trainset.data[i])\nplt.show()\n```", "```py\nimport matplotlib.pyplot as plt\nimport torchvision\nimport torch\nfrom torchvision.datasets import CIFAR10\n\ntransform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\ntrainset = CIFAR10(root='./data', train=True, download=True, transform=transform)\ntestset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n\nbatch_size = 24\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n\nfig, ax = plt.subplots(4, 6, sharex=True, sharey=True, figsize=(12,8))\nfor images, labels in trainloader:\n    for i in range(batch_size):\n        row, col = i//6, i%6\n        ax[row][col].imshow(images[i].numpy().transpose([1,2,0]))\n    break  # take only the first batch\nplt.show()\n```", "```py\nimport torch.nn as nn\n\nmodel = nn.Sequential(\n    nn.Conv2d(3, 32, kernel_size=(3,3), stride=1, padding=1),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Conv2d(32, 32, kernel_size=(3,3), stride=1, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=(2, 2)),\n    nn.Flatten(),\n    nn.Linear(8192, 512),\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(512, 10)\n)\nprint(model)\n```", "```py\nimport torch.nn as nn\nimport torch.optim as optim\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\nn_epochs = 20\nfor epoch in range(n_epochs):\n    model.train()\n    for inputs, labels in trainloader:\n        y_pred = model(inputs)\n        loss = loss_fn(y_pred, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    acc = 0\n    count = 0\n    model.eval()\n    with torch.no_grad():\n        for inputs, labels in testloader:\n            y_pred = model(inputs)\n            acc += (torch.argmax(y_pred, 1) == labels).float().sum()\n            count += len(labels)\n    acc /= count\n    print(\"Epoch %d: model accuracy %.2f%%\" % (epoch, acc*100))\n```", "```py\nimport torch.nn as nn\nimport torch.optim as optim\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\nn_epochs = 20\nfor epoch in range(n_epochs):\n    model.train()\n    for inputs, labels in trainloader:\n        y_pred = model(inputs.to(device))\n        loss = loss_fn(y_pred, labels.to(device))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    acc = 0\n    count = 0\n    model.eval()\n    with torch.no_grad():\n        for inputs, labels in testloader:\n            y_pred = model(inputs.to(device))\n            acc += (torch.argmax(y_pred, 1) == labels.to(device)).float().sum()\n            count += len(labels)\n    acc /= count\n    print(\"Epoch %d: model accuracy %.2f%%\" % (epoch, acc*100))\n```"]