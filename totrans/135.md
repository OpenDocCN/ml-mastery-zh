# 使用 scikit-learn 的 PyTorch 深度学习模型

> [原文](https://machinelearningmastery.com/use-pytorch-deep-learning-models-with-scikit-learn/)

在 Python 中最受欢迎的深度学习库是 TensorFlow/Keras 和 PyTorch，由于它们的简洁性。然而，scikit-learn 库仍然是 Python 中最受欢迎的通用机器学习库。在这篇文章中，你将发现如何将 PyTorch 的深度学习模型与 Python 中的 scikit-learn 库结合使用。这将使你能够利用 scikit-learn 库的强大功能进行模型评估和模型超参数优化。完成本课程后，你将知道：

+   如何包装 PyTorch 模型以便与 scikit-learn 机器学习库一起使用

+   如何使用 scikit-learn 中的交叉验证轻松评估 PyTorch 模型

+   如何使用 scikit-learn 中的网格搜索调整 PyTorch 模型的超参数

**快速启动你的项目**，请参考我的书籍 [《使用 PyTorch 的深度学习》](https://machinelearningmastery.com/deep-learning-with-pytorch/)。它提供了**自学教程**和**可运行的代码**。

让我们开始吧！[](../Images/a4f0998f7e876a482d3b30020553a429.png)

使用 scikit-learn 的 PyTorch 深度学习模型

照片由 [Priyanka Neve](https://unsplash.com/photos/puk-xEM9CyI) 提供。保留所有权利。

## 概述

本章分为四部分；它们是：

+   skorch 概述

+   评估深度学习模型的交叉验证

+   使用 scikit-learn 运行 k-折交叉验证

+   网格搜索深度学习模型参数

## skorch 概述

PyTorch 是一个在 Python 中用于深度学习的流行库，但该库的重点是深度学习，而不是所有机器学习。实际上，它追求简约，专注于快速而简单地定义和构建深度学习模型。Python 中的 scikit-learn 库建立在 SciPy 堆栈上，以实现高效的数值计算。它是一个功能全面的通用机器学习库，并提供许多有用的工具来开发深度学习模型。尤其包括：

+   使用如 k-折交叉验证等重采样方法评估模型

+   模型超参数的高效搜索和评估

+   将机器学习工作流程的多个步骤连接成一个管道

PyTorch 不能直接与 scikit-learn 一起使用。但由于 Python 语言的鸭子类型特性，适应 PyTorch 模型以与 scikit-learn 一起使用是很容易的。事实上，`skorch` 模块就是为此目的而构建的。使用 `skorch`，你可以让你的 PyTorch 模型像 scikit-learn 模型一样工作。你可能会觉得使用起来更方便。

在接下来的章节中，你将通过使用 `NeuralNetClassifier` 封装器来处理一个在 PyTorch 中创建并用于 scikit-learn 库的分类神经网络的示例。测试问题是 [Sonar 数据集](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks))。这是一个所有属性都是数值型的小型数据集，易于处理。

以下示例假设你已经成功安装了 PyTorch、skorch 和 scikit-learn。如果你使用 pip 安装 Python 模块，可以用以下命令安装它们：

```py
pip install torch skorch scikit-learn
```

## 使用交叉验证评估深度学习模型

`NeuralNet` 类，或更专业的 `NeuralNetClassifier`、`NeuralNetBinaryClassifier` 和 `NeuralNetRegressor` 类在 skorch 中是 PyTorch 模型的工厂封装器。它们接收一个参数 `model`，这个参数是一个类或一个函数，用于获取你的模型。作为回报，这些封装器类允许你指定损失函数和优化器，然后训练循环自动完成。这是与直接使用 PyTorch 相比的便利之处。

以下是一个在 Sonar 数据集上训练二分类器的简单示例：

```py
import copy

import numpy as np
from sklearn.model_selection import StratifiedKFold, train_test_split

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import LabelEncoder
from skorch import NeuralNetBinaryClassifier

# Read data
data = pd.read_csv("sonar.csv", header=None)
X = data.iloc[:, 0:60]
y = data.iloc[:, 60]

# Binary encoding of labels
encoder = LabelEncoder()
encoder.fit(y)
y = encoder.transform(y)

# Convert to 2D PyTorch tensors
X = torch.tensor(X.values, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32)

# Define the model
class SonarClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(60, 60)
        self.act1 = nn.ReLU()
        self.layer2 = nn.Linear(60, 60)
        self.act2 = nn.ReLU()
        self.layer3 = nn.Linear(60, 60)
        self.act3 = nn.ReLU()
        self.output = nn.Linear(60, 1)

    def forward(self, x):
        x = self.act1(self.layer1(x))
        x = self.act2(self.layer2(x))
        x = self.act3(self.layer3(x))
        x = self.output(x)
        return x

# create the skorch wrapper
model = NeuralNetBinaryClassifier(
    SonarClassifier,
    criterion=torch.nn.BCEWithLogitsLoss,
    optimizer=torch.optim.Adam,
    lr=0.0001,
    max_epochs=150,
    batch_size=10
)

# run
model.fit(X, y)
```

在这个模型中，你使用了 `torch.nn.BCEWithLogitsLoss` 作为损失函数（这确实是 `NeuralNetBinaryClassifier` 的默认设置）。它将 sigmoid 函数与二元交叉熵损失结合在一起，这样你就不需要在模型输出端使用 sigmoid 函数。它有时被偏好以提供更好的数值稳定性。

此外，你在 skorch 封装器中指定了训练参数，如训练轮数和批次大小。然后你只需调用 `fit()` 函数并提供输入特征和目标。封装器将帮助你初始化模型并训练它。

运行上述代码将产生以下结果：

```py
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        0.6952       0.5476        0.6921  0.0135
      2        0.6930       0.5476        0.6920  0.0114
      3        0.6925       0.5476        0.6919  0.0104
      4        0.6922       0.5238        0.6918  0.0118
      5        0.6919       0.5238        0.6917  0.0112
...
    146        0.2942       0.4524        0.9425  0.0115
    147        0.2920       0.4524        0.9465  0.0123
    148        0.2899       0.4524        0.9495  0.0112
    149        0.2879       0.4524        0.9544  0.0121
    150        0.2859       0.4524        0.9583  0.0118
```

请注意，skorch 被定位为适应 scikit-learn 接口的 PyTorch 模型封装器。因此，你应该将模型当作 scikit-learn 模型来使用。例如，要训练二分类模型，目标应该是一个向量而不是 $n\times 1$ 矩阵。并且在进行推断时，你应该使用 `model.predict(X)` 或 `model.predict_proba(X)`。这也是你应该使用 `NeuralNetBinaryClassifier` 的原因，这样分类相关的 scikit-learn 函数作为模型方法提供。

### 想要开始使用 PyTorch 深度学习吗？

立即参加我的免费电子邮件速成课程（包含示例代码）。

点击注册并获取课程的免费 PDF 电子书版本。

## 使用 scikit-learn 运行 k-Fold 交叉验证

使用 PyTorch 模型的封装器已经为你节省了大量构建自定义训练循环的样板代码。但来自 scikit-learn 的整个机器学习函数套件才是真正的生产力提升。

一个例子是使用 scikit-learn 的模型选择函数。假设你想用 k 折交叉验证评估这个模型设计。通常，这意味着将数据集分成 $k$ 部分，然后运行一个循环，将这些部分中的一个选作测试集，其余的作为训练集，从头开始训练模型并获得评估分数。这并不难，但你需要编写几行代码来实现这些功能。

确实，我们可以利用 scikit-learn 的 k 折交叉验证函数，如下：

```py
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score

model = NeuralNetBinaryClassifier(
    SonarClassifier,
    criterion=torch.nn.BCEWithLogitsLoss,
    optimizer=torch.optim.Adam,
    lr=0.0001,
    max_epochs=150,
    batch_size=10,
    verbose=False
)

kfold = StratifiedKFold(n_splits=5, shuffle=True)
results = cross_val_score(model, X, y, cv=kfold)
print(results)
```

`NeuralNetBinaryClassifier` 中的参数 `verbose=False` 是为了在模型训练时停止显示进度，因为进度很多。上述代码将打印验证分数，如下所示：

```py
[0.76190476 0.76190476 0.78571429 0.75609756 0.75609756]
```

这些是评估分数。因为这是一个二分类模型，所以它们是平均准确率。因为是从 $k=5$ 的 k 折交叉验证中获得的，所以有五个，每个对应一个不同的测试集。通常你会用交叉验证分数的均值和标准差来评估模型：

```py
print("mean = %.3f; std = %.3f" % (results.mean(), results.std()))
```

即

```py
mean = 0.764; std = 0.011
```

一个好的模型应该产生高分（在这种情况下，准确率接近 1）和低标准差。高标准差意味着模型在不同测试集上的一致性较差。

将所有内容整合在一起，以下是完整代码：

```py
import copy

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import LabelEncoder
from skorch import NeuralNetBinaryClassifier
from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score

# Read data
data = pd.read_csv("sonar.csv", header=None)
X = data.iloc[:, 0:60]
y = data.iloc[:, 60]

# Binary encoding of labels
encoder = LabelEncoder()
encoder.fit(y)
y = encoder.transform(y)

# Convert to 2D PyTorch tensors
X = torch.tensor(X.values, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32)

# Define the model
class SonarClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(60, 60)
        self.act1 = nn.ReLU()
        self.layer2 = nn.Linear(60, 60)
        self.act2 = nn.ReLU()
        self.layer3 = nn.Linear(60, 60)
        self.act3 = nn.ReLU()
        self.output = nn.Linear(60, 1)

    def forward(self, x):
        x = self.act1(self.layer1(x))
        x = self.act2(self.layer2(x))
        x = self.act3(self.layer3(x))
        x = self.output(x)
        return x

# create the skorch wrapper
model = NeuralNetBinaryClassifier(
    SonarClassifier,
    criterion=torch.nn.BCEWithLogitsLoss,
    optimizer=torch.optim.Adam,
    lr=0.0001,
    max_epochs=150,
    batch_size=10,
    verbose=False
)

# k-fold
kfold = StratifiedKFold(n_splits=5, shuffle=True)
results = cross_val_score(model, X, y, cv=kfold)
print("mean = %.3f; std = %.3f" % (results.mean(), results.std()))
```

相比之下，以下是使用 scikit-learn 实现的等效神经网络模型：

```py
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.neural_network import MLPClassifier
import numpy as np

# load dataset
data = pd.read_csv("sonar.csv", header=None)
# split into input (X) and output (Y) variables, in numpy arrays
X = data.iloc[:, 0:60].values
y = data.iloc[:, 60].values

# binary encoding of labels
encoder = LabelEncoder()
encoder.fit(y)
y = encoder.transform(y)

# create model
model = MLPClassifier(hidden_layer_sizes=(60,60,60), activation='relu',
                      max_iter=150, batch_size=10, verbose=False)

# evaluate using 10-fold cross validation
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)
results = cross_val_score(model, X, y, cv=kfold)
print("mean = %.3f; std = %.3f" % (results.mean(), results.std()))
```

你应该看到 skorch 如何使 PyTorch 模型可以替换 scikit-learn 模型。

## 网格搜索深度学习模型参数

前面的示例展示了如何轻松地将你的深度学习模型从 PyTorch 封装起来，并在 scikit-learn 库的函数中使用它。在这个示例中，你将更进一步。你在创建 `NeuralNetBinaryClassifier` 或 `NeuralNetClassifier` 包装器时指定给模型参数的函数可以接受许多参数。你可以使用这些参数进一步自定义模型的构建。此外，你也知道可以向 `fit()` 函数提供参数。

在这个示例中，你将使用网格搜索来评估神经网络模型的不同配置，并报告提供最佳估计性能的组合。为了增加趣味性，我们将修改 PyTorch 模型，使其接受一个参数来决定你希望模型有多深：

```py
class SonarClassifier(nn.Module):
    def __init__(self, n_layers=3):
        super().__init__()
        self.layers = []
        self.acts = []
        for i in range(n_layers):
            self.layers.append(nn.Linear(60, 60))
            self.acts.append(nn.ReLU())
            self.add_module(f"layer{i}", self.layers[-1])
            self.add_module(f"act{i}", self.acts[-1])
        self.output = nn.Linear(60, 1)

    def forward(self, x):
        for layer, act in zip(self.layers, self.acts):
            x = act(layer(x))
        x = self.output(x)
        return x
```

在这个设计中，我们将隐藏层及其激活函数保存在 Python 列表中。因为 PyTorch 组件不是类的直接属性，所以你不会在 `model.parameters()` 中看到它们。这在训练时会是个问题。这可以通过使用 `self.add_module()` 来注册组件来缓解。另一种方法是使用 `nn.ModuleList()` 代替 Python 列表，这样你就提供了足够的线索来告诉系统模型组件的位置。

skorch 封装器依旧保持不变。使用它，你可以获得一个兼容 scikit-learn 的模型。如你所见，封装器中有用于设置深度学习模型的参数，以及诸如学习率（`lr`）等训练参数，你可以有许多可能的变体。scikit-learn 的 `GridSearchCV` 函数提供网格搜索交叉验证。你可以为每个参数提供一个值列表，并要求 scikit-learn 尝试**所有组合**，并根据你指定的指标报告最佳参数集。示例如下：

```py
from sklearn.model_selection import GridSearchCV

model = NeuralNetBinaryClassifier(
    SonarClassifier,
    criterion=torch.nn.BCEWithLogitsLoss,
    optimizer=torch.optim.Adam,
    lr=0.0001,
    max_epochs=150,
    batch_size=10,
    verbose=False
)

param_grid = {
    'module__n_layers': [1, 3, 5], 
    'lr': [0.1, 0.01, 0.001, 0.0001],
    'max_epochs': [100, 150],
}

grid_search = GridSearchCV(model, param_grid, scoring='accuracy', verbose=1, cv=3)
result = grid_search.fit(X, y)
```

你将 `model` 传递给 `GridSearchCV()`，这是一个 skorch 封装器。你还传递了 `param_grid`，指定了要变化的参数：

+   PyTorch 模型中的 `n_layers` 参数（即 `SonarClassifier` 类），控制神经网络的深度。

+   封装器中的 `lr` 参数，控制优化器中的学习率。

+   封装器中的 `max_epochs` 参数，控制训练周期的数量。

注意使用双下划线来传递参数给 PyTorch 模型。实际上，这也允许你配置其他参数。例如，你可以设置 `optimizer__weight_decay` 来传递 `weight_decay` 参数给 Adam 优化器（用于设置 L2 正则化）。

运行这个可能需要一段时间，因为它尝试了所有组合，每个组合都经过 3 折交叉验证。你不希望频繁运行这个，但它对于设计模型是有用的。

网格搜索完成后，最佳模型的性能和配置组合将显示出来，随后是所有参数组合的性能，如下所示：

```py
print("Best: %f using %s" % (result.best_score_, result.best_params_))
means = result.cv_results_['mean_test_score']
stds = result.cv_results_['std_test_score']
params = result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
```

它给出的结果是：

```py
Best: 0.649551 using {'lr': 0.001, 'max_epochs': 150, 'module__n_layers': 1}
0.533678 (0.003611) with: {'lr': 0.1, 'max_epochs': 100, 'module__n_layers': 1}
0.533678 (0.003611) with: {'lr': 0.1, 'max_epochs': 100, 'module__n_layers': 3}
0.533678 (0.003611) with: {'lr': 0.1, 'max_epochs': 100, 'module__n_layers': 5}
0.533678 (0.003611) with: {'lr': 0.1, 'max_epochs': 150, 'module__n_layers': 1}
0.533678 (0.003611) with: {'lr': 0.1, 'max_epochs': 150, 'module__n_layers': 3}
0.533678 (0.003611) with: {'lr': 0.1, 'max_epochs': 150, 'module__n_layers': 5}
0.644651 (0.062160) with: {'lr': 0.01, 'max_epochs': 100, 'module__n_layers': 1}
0.567495 (0.049728) with: {'lr': 0.01, 'max_epochs': 100, 'module__n_layers': 3}
0.533678 (0.003611) with: {'lr': 0.01, 'max_epochs': 100, 'module__n_layers': 5}
0.615804 (0.061966) with: {'lr': 0.01, 'max_epochs': 150, 'module__n_layers': 1}
0.620290 (0.078243) with: {'lr': 0.01, 'max_epochs': 150, 'module__n_layers': 3}
0.533678 (0.003611) with: {'lr': 0.01, 'max_epochs': 150, 'module__n_layers': 5}
0.635335 (0.108412) with: {'lr': 0.001, 'max_epochs': 100, 'module__n_layers': 1}
0.582126 (0.058072) with: {'lr': 0.001, 'max_epochs': 100, 'module__n_layers': 3}
0.563423 (0.136916) with: {'lr': 0.001, 'max_epochs': 100, 'module__n_layers': 5}
0.649551 (0.075676) with: {'lr': 0.001, 'max_epochs': 150, 'module__n_layers': 1}
0.558178 (0.071443) with: {'lr': 0.001, 'max_epochs': 150, 'module__n_layers': 3}
0.567909 (0.088623) with: {'lr': 0.001, 'max_epochs': 150, 'module__n_layers': 5}
0.557971 (0.041416) with: {'lr': 0.0001, 'max_epochs': 100, 'module__n_layers': 1}
0.587026 (0.079951) with: {'lr': 0.0001, 'max_epochs': 100, 'module__n_layers': 3}
0.606349 (0.092394) with: {'lr': 0.0001, 'max_epochs': 100, 'module__n_layers': 5}
0.563147 (0.099652) with: {'lr': 0.0001, 'max_epochs': 150, 'module__n_layers': 1}
0.534023 (0.057187) with: {'lr': 0.0001, 'max_epochs': 150, 'module__n_layers': 3}
0.634921 (0.057235) with: {'lr': 0.0001, 'max_epochs': 150, 'module__n_layers': 5}
```

在你的工作站上执行这个操作可能需要大约 5 分钟（使用 CPU 而非 GPU）。运行示例后显示了以下结果。你可以看到，网格搜索发现使用 0.001 的学习率、150 个周期和只有一个隐藏层的组合，在这个问题上获得了大约 65% 的最佳交叉验证分数。

实际上，你可以先通过标准化输入特征来看看是否能改善结果。由于封装器允许你在 scikit-learn 中使用 PyTorch 模型，你也可以实时使用 scikit-learn 的标准化器，并创建一个机器学习管道：

```py
from sklearn.pipeline import Pipeline, FunctionTransformer
from sklearn.preprocessing import StandardScaler

pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('float32', FunctionTransformer(func=lambda X: torch.tensor(X, dtype=torch.float32),
                                    validate=False)),
    ('sonarmodel', model.initialize()),
])
```

你创建的新对象 `pipe` 是另一个 scikit-learn 模型，它的工作方式与 `model` 对象类似，只是数据在传递给神经网络之前应用了标准化器。因此，你可以在这个管道上运行网格搜索，只需稍微调整参数的指定方式：

```py
param_grid = {
    'sonarmodel__module__n_layers': [1, 3, 5], 
    'sonarmodel__lr': [0.1, 0.01, 0.001, 0.0001],
    'sonarmodel__max_epochs': [100, 150],
}

grid_search = GridSearchCV(pipe, param_grid, scoring='accuracy', verbose=1, cv=3)
result = grid_search.fit(X, y)
print("Best: %f using %s" % (result.best_score_, result.best_params_))
means = result.cv_results_['mean_test_score']
stds = result.cv_results_['std_test_score']
params = result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
```

这里要注意两点：由于 PyTorch 模型默认运行在 32 位浮点数上，但 NumPy 数组通常是 64 位浮点数。这些数据类型不对齐，但 scikit-learn 的缩放器总是返回一个 NumPy 数组。因此，你需要在管道中间进行类型转换，使用 `FunctionTransformer` 对象。

此外，在 scikit-learn 管道中，每个步骤都通过名称进行引用，例如 `scaler` 和 `sonarmodel`。因此，管道设置的参数也需要携带名称。在上述示例中，我们使用 `sonarmodel__module__n_layers` 作为网格搜索的参数。这指的是管道中的 `sonarmodel` 部分（即你的 skorch 封装器）、其中的 `module` 部分（即你的 PyTorch 模型）及其 `n_layers` 参数。注意使用双下划线进行层次分隔。

将所有内容整合在一起，以下是完整的代码：

```py
import copy

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, cross_val_score
from sklearn.pipeline import Pipeline, FunctionTransformer
from sklearn.preprocessing import StandardScaler, LabelEncoder
from skorch import NeuralNetBinaryClassifier

# Read data
data = pd.read_csv("sonar.csv", header=None)
X = data.iloc[:, 0:60]
y = data.iloc[:, 60]

# Binary encoding of labels
encoder = LabelEncoder()
encoder.fit(y)
y = encoder.transform(y)

# Convert to 2D PyTorch tensors
X = torch.tensor(X.values, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32)

class SonarClassifier(nn.Module):
    def __init__(self, n_layers=3):
        super().__init__()
        self.layers = []
        self.acts = []
        for i in range(n_layers):
            self.layers.append(nn.Linear(60, 60))
            self.acts.append(nn.ReLU())
            self.add_module(f"layer{i}", self.layers[-1])
            self.add_module(f"act{i}", self.acts[-1])
        self.output = nn.Linear(60, 1)

    def forward(self, x):
        for layer, act in zip(self.layers, self.acts):
            x = act(layer(x))
        x = self.output(x)
        return x

model = NeuralNetBinaryClassifier(
    SonarClassifier,
    criterion=torch.nn.BCEWithLogitsLoss,
    optimizer=torch.optim.Adam,
    lr=0.0001,
    max_epochs=150,
    batch_size=10,
    verbose=False
)

pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('float32', FunctionTransformer(func=lambda X: torch.tensor(X, dtype=torch.float32),
                                    validate=False)),
    ('sonarmodel', model.initialize()),
])

param_grid = {
    'sonarmodel__module__n_layers': [1, 3, 5], 
    'sonarmodel__lr': [0.1, 0.01, 0.001, 0.0001],
    'sonarmodel__max_epochs': [100, 150],
}

grid_search = GridSearchCV(pipe, param_grid, scoring='accuracy', verbose=1, cv=3)
result = grid_search.fit(X, y)
print("Best: %f using %s" % (result.best_score_, result.best_params_))
means = result.cv_results_['mean_test_score']
stds = result.cv_results_['std_test_score']
params = result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
```

## 深入阅读

本节提供了更多关于该主题的资源，如果你希望深入了解。

### 在线资源

+   [skorch 文档](https://skorch.readthedocs.io/en/latest/)

+   [分层 K 折交叉验证器](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)。scikit-learn 文档。

+   [网格搜索交叉验证器](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)。scikit-learn 文档。

+   [管道](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)。scikit-learn 文档

## 总结

在本章中，你了解了如何封装你的 PyTorch 深度学习模型并在 scikit-learn 通用机器学习库中使用它们。你学到了：

+   具体说明如何封装 PyTorch 模型，以便可以与 scikit-learn 机器学习库一起使用。

+   如何将封装的 PyTorch 模型作为评估模型性能的一部分在 scikit-learn 中使用。

+   如何使用封装的 PyTorch 模型在 scikit-learn 中进行超参数调整。

你可以看到，使用 scikit-learn 进行标准的机器学习操作，如模型评估和模型超参数优化，可以比自己实现这些方案节省大量时间。封装你的模型使你能够利用 scikit-learn 提供的强大工具，将你的深度学习模型融入到通用机器学习过程中。
