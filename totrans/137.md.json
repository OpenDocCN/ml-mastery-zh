["```py\nimport pandas as pd\n\n# Read data\ndata = pd.read_csv(\"sonar.csv\", header=None)\nX = data.iloc[:, 0:60]\ny = data.iloc[:, 60]\n```", "```py\nfrom sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\nencoder.fit(y)\ny = encoder.transform(y)\n```", "```py\nprint(encoder.classes_)\n```", "```py\n['M' 'R']\n```", "```py\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n```", "```py\nimport torch\n\nX = torch.tensor(X.values, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n```", "```py\nimport torch.nn as nn\n\nclass Wide(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.hidden = nn.Linear(60, 180)\n        self.relu = nn.ReLU()\n        self.output = nn.Linear(180, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.relu(self.hidden(x))\n        x = self.sigmoid(self.output(x))\n        return x\n```", "```py\nclass Deep(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(60, 60)\n        self.act1 = nn.ReLU()\n        self.layer2 = nn.Linear(60, 60)\n        self.act2 = nn.ReLU()\n        self.layer3 = nn.Linear(60, 60)\n        self.act3 = nn.ReLU()\n        self.output = nn.Linear(60, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.act1(self.layer1(x))\n        x = self.act2(self.layer2(x))\n        x = self.act3(self.layer3(x))\n        x = self.sigmoid(self.output(x))\n        return x\n```", "```py\n# Compare model sizes\nmodel1 = Wide()\nmodel2 = Deep()\nprint(sum([x.reshape(-1).shape[0] for x in model1.parameters()]))  # 11161\nprint(sum([x.reshape(-1).shape[0] for x in model2.parameters()]))  # 11041\n```", "```py\n# define 5-fold cross validation test harness\nkfold = StratifiedKFold(n_splits=5, shuffle=True)\ncv_scores = []\nfor train, test in kfold.split(X, y):\n    # create model, train, and get accuracy\n    model = Wide()\n    acc = model_train(model, X[train], y[train], X[test], y[test])\n    print(\"Accuracy (wide): %.2f\" % acc)\n    cv_scores.append(acc)\n\n# evaluate the model\nacc = np.mean(cv_scores)\nstd = np.std(cv_scores)\nprint(\"Model accuracy: %.2f%% (+/- %.2f%%)\" % (acc*100, std*100))\n```", "```py\nimport copy\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport tqdm\n\ndef model_train(model, X_train, y_train, X_val, y_val):\n    # loss function and optimizer\n    loss_fn = nn.BCELoss()  # binary cross entropy\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n    n_epochs = 250   # number of epochs to run\n    batch_size = 10  # size of each batch\n    batch_start = torch.arange(0, len(X_train), batch_size)\n\n    # Hold the best model\n    best_acc = - np.inf   # init to negative infinity\n    best_weights = None\n\n    for epoch in range(n_epochs):\n        model.train()\n        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n            bar.set_description(f\"Epoch {epoch}\")\n            for start in bar:\n                # take a batch\n                X_batch = X_train[start:start+batch_size]\n                y_batch = y_train[start:start+batch_size]\n                # forward pass\n                y_pred = model(X_batch)\n                loss = loss_fn(y_pred, y_batch)\n                # backward pass\n                optimizer.zero_grad()\n                loss.backward()\n                # update weights\n                optimizer.step()\n                # print progress\n                acc = (y_pred.round() == y_batch).float().mean()\n                bar.set_postfix(\n                    loss=float(loss),\n                    acc=float(acc)\n                )\n        # evaluate accuracy at end of each epoch\n        model.eval()\n        y_pred = model(X_val)\n        acc = (y_pred.round() == y_val).float().mean()\n        acc = float(acc)\n        if acc > best_acc:\n            best_acc = acc\n            best_weights = copy.deepcopy(model.state_dict())\n    # restore model and return best accuracy\n    model.load_state_dict(best_weights)\n    return best_acc\n```", "```py\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\n\n# train-test split: Hold out the test set for final model evaluation\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n\n# define 5-fold cross validation test harness\nkfold = StratifiedKFold(n_splits=5, shuffle=True)\ncv_scores_wide = []\nfor train, test in kfold.split(X_train, y_train):\n    # create model, train, and get accuracy\n    model = Wide()\n    acc = model_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n    print(\"Accuracy (wide): %.2f\" % acc)\n    cv_scores_wide.append(acc)\ncv_scores_deep = []\nfor train, test in kfold.split(X_train, y_train):\n    # create model, train, and get accuracy\n    model = Deep()\n    acc = model_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n    print(\"Accuracy (deep): %.2f\" % acc)\n    cv_scores_deep.append(acc)\n\n# evaluate the model\nwide_acc = np.mean(cv_scores_wide)\nwide_std = np.std(cv_scores_wide)\ndeep_acc = np.mean(cv_scores_deep)\ndeep_std = np.std(cv_scores_deep)\nprint(\"Wide: %.2f%% (+/- %.2f%%)\" % (wide_acc*100, wide_std*100))\nprint(\"Deep: %.2f%% (+/- %.2f%%)\" % (deep_acc*100, deep_std*100))\n```", "```py\nAccuracy (wide): 0.72\nAccuracy (wide): 0.66\nAccuracy (wide): 0.83\nAccuracy (wide): 0.76\nAccuracy (wide): 0.83\nAccuracy (deep): 0.90\nAccuracy (deep): 0.72\nAccuracy (deep): 0.93\nAccuracy (deep): 0.69\nAccuracy (deep): 0.76\nWide: 75.86% (+/- 6.54%)\nDeep: 80.00% (+/- 9.61%)\n```", "```py\n# rebuild model with full set of training data\nif wide_acc > deep_acc:\n    print(\"Retrain a wide model\")\n    model = Wide()\nelse:\n    print(\"Retrain a deep model\")\n    model = Deep()\nacc = model_train(model, X_train, y_train, X_test, y_test)\nprint(f\"Final model accuracy: {acc*100:.2f}%\")\n```", "```py\nmodel.eval()\nwith torch.no_grad():\n    # Test out inference with 5 samples\n    for i in range(5):\n        y_pred = model(X_test[i:i+1])\n        print(f\"{X_test[i].numpy()} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")\n```", "```py\n[0.0265 0.044  0.0137 0.0084 0.0305 0.0438 0.0341 0.078  0.0844 0.0779\n 0.0327 0.206  0.1908 0.1065 0.1457 0.2232 0.207  0.1105 0.1078 0.1165\n 0.2224 0.0689 0.206  0.2384 0.0904 0.2278 0.5872 0.8457 0.8467 0.7679\n 0.8055 0.626  0.6545 0.8747 0.9885 0.9348 0.696  0.5733 0.5872 0.6663\n 0.5651 0.5247 0.3684 0.1997 0.1512 0.0508 0.0931 0.0982 0.0524 0.0188\n 0.01   0.0038 0.0187 0.0156 0.0068 0.0097 0.0073 0.0081 0.0086 0.0095] -> [0.9583146] (expected [1.])\n...\n\n[0.034  0.0625 0.0381 0.0257 0.0441 0.1027 0.1287 0.185  0.2647 0.4117\n 0.5245 0.5341 0.5554 0.3915 0.295  0.3075 0.3021 0.2719 0.5443 0.7932\n 0.8751 0.8667 0.7107 0.6911 0.7287 0.8792 1\\.     0.9816 0.8984 0.6048\n 0.4934 0.5371 0.4586 0.2908 0.0774 0.2249 0.1602 0.3958 0.6117 0.5196\n 0.2321 0.437  0.3797 0.4322 0.4892 0.1901 0.094  0.1364 0.0906 0.0144\n 0.0329 0.0141 0.0019 0.0067 0.0099 0.0042 0.0057 0.0051 0.0033 0.0058] -> [0.01937182] (expected [0.])\n```", "```py\ny_pred = model(X_test[i:i+1])\ny_pred = y_pred.round() # 0 or 1\n```", "```py\nthreshold = 0.68\ny_pred = model(X_test[i:i+1])\ny_pred = (y_pred > threshold).float() # 0.0 or 1.0\n```", "```py\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n\nwith torch.no_grad():\n    # Plot the ROC curve\n    y_pred = model(X_test)\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n    plt.plot(fpr, tpr) # ROC curve = TPR vs FPR\n    plt.title(\"Receiver Operating Characteristics\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.show()\n```", "```py\nimport copy\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport tqdm\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Read data\ndata = pd.read_csv(\"sonar.csv\", header=None)\nX = data.iloc[:, 0:60]\ny = data.iloc[:, 60]\n\n# Binary encoding of labels\nencoder = LabelEncoder()\nencoder.fit(y)\ny = encoder.transform(y)\n\n# Convert to 2D PyTorch tensors\nX = torch.tensor(X.values, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n\n# Define two models\nclass Wide(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.hidden = nn.Linear(60, 180)\n        self.relu = nn.ReLU()\n        self.output = nn.Linear(180, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.relu(self.hidden(x))\n        x = self.sigmoid(self.output(x))\n        return x\n\nclass Deep(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(60, 60)\n        self.act1 = nn.ReLU()\n        self.layer2 = nn.Linear(60, 60)\n        self.act2 = nn.ReLU()\n        self.layer3 = nn.Linear(60, 60)\n        self.act3 = nn.ReLU()\n        self.output = nn.Linear(60, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.act1(self.layer1(x))\n        x = self.act2(self.layer2(x))\n        x = self.act3(self.layer3(x))\n        x = self.sigmoid(self.output(x))\n        return x\n\n# Compare model sizes\nmodel1 = Wide()\nmodel2 = Deep()\nprint(sum([x.reshape(-1).shape[0] for x in model1.parameters()]))  # 11161\nprint(sum([x.reshape(-1).shape[0] for x in model2.parameters()]))  # 11041\n\n# Helper function to train one model\ndef model_train(model, X_train, y_train, X_val, y_val):\n    # loss function and optimizer\n    loss_fn = nn.BCELoss()  # binary cross entropy\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n    n_epochs = 300   # number of epochs to run\n    batch_size = 10  # size of each batch\n    batch_start = torch.arange(0, len(X_train), batch_size)\n\n    # Hold the best model\n    best_acc = - np.inf   # init to negative infinity\n    best_weights = None\n\n    for epoch in range(n_epochs):\n        model.train()\n        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n            bar.set_description(f\"Epoch {epoch}\")\n            for start in bar:\n                # take a batch\n                X_batch = X_train[start:start+batch_size]\n                y_batch = y_train[start:start+batch_size]\n                # forward pass\n                y_pred = model(X_batch)\n                loss = loss_fn(y_pred, y_batch)\n                # backward pass\n                optimizer.zero_grad()\n                loss.backward()\n                # update weights\n                optimizer.step()\n                # print progress\n                acc = (y_pred.round() == y_batch).float().mean()\n                bar.set_postfix(\n                    loss=float(loss),\n                    acc=float(acc)\n                )\n        # evaluate accuracy at end of each epoch\n        model.eval()\n        y_pred = model(X_val)\n        acc = (y_pred.round() == y_val).float().mean()\n        acc = float(acc)\n        if acc > best_acc:\n            best_acc = acc\n            best_weights = copy.deepcopy(model.state_dict())\n    # restore model and return best accuracy\n    model.load_state_dict(best_weights)\n    return best_acc\n\n# train-test split: Hold out the test set for final model evaluation\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n\n# define 5-fold cross validation test harness\nkfold = StratifiedKFold(n_splits=5, shuffle=True)\ncv_scores_wide = []\nfor train, test in kfold.split(X_train, y_train):\n    # create model, train, and get accuracy\n    model = Wide()\n    acc = model_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n    print(\"Accuracy (wide): %.2f\" % acc)\n    cv_scores_wide.append(acc)\ncv_scores_deep = []\nfor train, test in kfold.split(X_train, y_train):\n    # create model, train, and get accuracy\n    model = Deep()\n    acc = model_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n    print(\"Accuracy (deep): %.2f\" % acc)\n    cv_scores_deep.append(acc)\n\n# evaluate the model\nwide_acc = np.mean(cv_scores_wide)\nwide_std = np.std(cv_scores_wide)\ndeep_acc = np.mean(cv_scores_deep)\ndeep_std = np.std(cv_scores_deep)\nprint(\"Wide: %.2f%% (+/- %.2f%%)\" % (wide_acc*100, wide_std*100))\nprint(\"Deep: %.2f%% (+/- %.2f%%)\" % (deep_acc*100, deep_std*100))\n\n# rebuild model with full set of training data\nif wide_acc > deep_acc:\n    print(\"Retrain a wide model\")\n    model = Wide()\nelse:\n    print(\"Retrain a deep model\")\n    model = Deep()\nacc = model_train(model, X_train, y_train, X_test, y_test)\nprint(f\"Final model accuracy: {acc*100:.2f}%\")\n\nmodel.eval()\nwith torch.no_grad():\n    # Test out inference with 5 samples\n    for i in range(5):\n        y_pred = model(X_test[i:i+1])\n        print(f\"{X_test[i].numpy()} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")\n\n    # Plot the ROC curve\n    y_pred = model(X_test)\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n    plt.plot(fpr, tpr) # ROC curve = TPR vs FPR\n    plt.title(\"Receiver Operating Characteristics\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.show()\n```"]