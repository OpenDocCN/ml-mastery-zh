["```py\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Creating a function f(X) with a slope of -5\nX = torch.arange(-5, 5, 0.1).view(-1, 1)\nfunc = -5 * X\n\n# Adding Gaussian noise to the function f(X) and saving it in Y\nY = func + 0.4 * torch.randn(X.size())\n```", "```py\n...\n# Plot and visualizing the data points in blue\nplt.plot(X.numpy(), Y.numpy(), 'b+', label='Y')\nplt.plot(X.numpy(), func.numpy(), 'r', label='func')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.grid('True', color='y')\nplt.show()\n```", "```py\n...\n# defining the function for forward pass for prediction\ndef forward(x):\n    return w * x + b\n\n# evaluating data points with Mean Square Error (MSE)\ndef criterion(y_pred, y):\n    return torch.mean((y_pred - y) ** 2)\n```", "```py\nw = torch.tensor(-10.0, requires_grad=True)\nb = torch.tensor(-20.0, requires_grad=True)\n\nstep_size = 0.1\nloss_BGD = []\nn_iter = 20\n```", "```py\nfor i in range (n_iter):\n    # making predictions with forward pass\n    Y_pred = forward(X)\n    # calculating the loss between original and predicted data points\n    loss = criterion(Y_pred, Y)\n    # storing the calculated loss in a list\n    loss_BGD.append(loss.item())\n    # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n    loss.backward()\n    # updateing the parameters after each iteration\n    w.data = w.data - step_size * w.grad.data\n    b.data = b.data - step_size * b.grad.data\n    # zeroing gradients after each iteration\n    w.grad.data.zero_()\n    b.grad.data.zero_()\n    # priting the values for understanding\n    print('{}, \\t{}, \\t{}, \\t{}'.format(i, loss.item(), w.item(), b.item()))\n```", "```py\n0, \t596.7191162109375, \t-1.8527469635009766, \t-16.062074661254883\n1, \t343.426513671875, \t-7.247585773468018, \t-12.83026123046875\n2, \t202.7098388671875, \t-3.616910219192505, \t-10.298759460449219\n3, \t122.16651153564453, \t-6.0132551193237305, \t-8.237251281738281\n4, \t74.85094451904297, \t-4.394278526306152, \t-6.6120076179504395\n5, \t46.450958251953125, \t-5.457883358001709, \t-5.295622825622559\n6, \t29.111614227294922, \t-4.735295295715332, \t-4.2531514167785645\n7, \t18.386211395263672, \t-5.206836700439453, \t-3.4119482040405273\n8, \t11.687058448791504, \t-4.883906364440918, \t-2.7437009811401367\n9, \t7.4728569984436035, \t-5.092618465423584, \t-2.205873966217041\n10, \t4.808231830596924, \t-4.948029518127441, \t-1.777699589729309\n11, \t3.1172332763671875, \t-5.040188312530518, \t-1.4337140321731567\n12, \t2.0413269996643066, \t-4.975278854370117, \t-1.159447193145752\n13, \t1.355530858039856, \t-5.0158305168151855, \t-0.9393846988677979\n14, \t0.9178376793861389, \t-4.986582279205322, \t-0.7637402415275574\n15, \t0.6382412910461426, \t-5.004333972930908, \t-0.6229321360588074\n16, \t0.45952412486076355, \t-4.991086006164551, \t-0.5104631781578064\n17, \t0.34523946046829224, \t-4.998797416687012, \t-0.42035552859306335\n18, \t0.27213525772094727, \t-4.992753028869629, \t-0.3483465909957886\n19, \t0.22536347806453705, \t-4.996064186096191, \t-0.2906789183616638\n```", "```py\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX = torch.arange(-5, 5, 0.1).view(-1, 1)\nfunc = -5 * X\nY = func + 0.4 * torch.randn(X.size())\n\n# defining the function for forward pass for prediction\ndef forward(x):\n    return w * x + b\n\n# evaluating data points with Mean Square Error (MSE)\ndef criterion(y_pred, y):\n    return torch.mean((y_pred - y) ** 2)\n\nw = torch.tensor(-10.0, requires_grad=True)\nb = torch.tensor(-20.0, requires_grad=True)\n\nstep_size = 0.1\nloss_BGD = []\nn_iter = 20\n\nfor i in range (n_iter):\n    # making predictions with forward pass\n    Y_pred = forward(X)\n    # calculating the loss between original and predicted data points\n    loss = criterion(Y_pred, Y)\n    # storing the calculated loss in a list\n    loss_BGD.append(loss.item())\n    # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n    loss.backward()\n    # updateing the parameters after each iteration\n    w.data = w.data - step_size * w.grad.data\n    b.data = b.data - step_size * b.grad.data\n    # zeroing gradients after each iteration\n    w.grad.data.zero_()\n    b.grad.data.zero_()\n    # priting the values for understanding\n    print('{}, \\t{}, \\t{}, \\t{}'.format(i, loss.item(), w.item(), b.item()))\n```", "```py\n0, \t596.7191162109375, \t-1.8527469635009766, \t-16.062074661254883\n1, \t343.426513671875, \t-7.247585773468018, \t-12.83026123046875\n2, \t202.7098388671875, \t-3.616910219192505, \t-10.298759460449219\n3, \t122.16651153564453, \t-6.0132551193237305, \t-8.237251281738281\n4, \t74.85094451904297, \t-4.394278526306152, \t-6.6120076179504395\n5, \t46.450958251953125, \t-5.457883358001709, \t-5.295622825622559\n6, \t29.111614227294922, \t-4.735295295715332, \t-4.2531514167785645\n7, \t18.386211395263672, \t-5.206836700439453, \t-3.4119482040405273\n8, \t11.687058448791504, \t-4.883906364440918, \t-2.7437009811401367\n9, \t7.4728569984436035, \t-5.092618465423584, \t-2.205873966217041\n10, \t4.808231830596924, \t-4.948029518127441, \t-1.777699589729309\n11, \t3.1172332763671875, \t-5.040188312530518, \t-1.4337140321731567\n12, \t2.0413269996643066, \t-4.975278854370117, \t-1.159447193145752\n13, \t1.355530858039856, \t-5.0158305168151855, \t-0.9393846988677979\n14, \t0.9178376793861389, \t-4.986582279205322, \t-0.7637402415275574\n15, \t0.6382412910461426, \t-5.004333972930908, \t-0.6229321360588074\n16, \t0.45952412486076355, \t-4.991086006164551, \t-0.5104631781578064\n17, \t0.34523946046829224, \t-4.998797416687012, \t-0.42035552859306335\n18, \t0.27213525772094727, \t-4.992753028869629, \t-0.3483465909957886\n19, \t0.22536347806453705, \t-4.996064186096191, \t-0.2906789183616638\n```", "```py\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX = torch.arange(-5, 5, 0.1).view(-1, 1)\nfunc = -5 * X\nY = func + 0.4 * torch.randn(X.size())\n\n# defining the function for forward pass for prediction\ndef forward(x):\n    return w * x + b\n\n# evaluating data points with Mean Square Error (MSE)\ndef criterion(y_pred, y):\n    return torch.mean((y_pred - y) ** 2)\n\nw = torch.tensor(-10.0, requires_grad=True)\nb = torch.tensor(-20.0, requires_grad=True)\n\nstep_size = 0.1\nloss_SGD = []\nn_iter = 20\n\nfor i in range (n_iter):    \n    # calculating true loss and storing it\n    Y_pred = forward(X)\n    # store the loss in the list\n    loss_SGD.append(criterion(Y_pred, Y).tolist())\n\n    for x, y in zip(X, Y):\n      # making a pridiction in forward pass\n      y_hat = forward(x)\n      # calculating the loss between original and predicted data points\n      loss = criterion(y_hat, y)\n      # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n      loss.backward()\n      # updateing the parameters after each iteration\n      w.data = w.data - step_size * w.grad.data\n      b.data = b.data - step_size * b.grad.data\n      # zeroing gradients after each iteration\n      w.grad.data.zero_()\n      b.grad.data.zero_()\n      # priting the values for understanding\n      print('{}, \\t{}, \\t{}, \\t{}'.format(i, loss.item(), w.item(), b.item()))\n```", "```py\n0, \t24.73763084411621, \t-5.02630615234375, \t-20.994739532470703\n0, \t455.0946960449219, \t-25.93259620666504, \t-16.7281494140625\n0, \t6968.82666015625, \t54.207733154296875, \t-33.424049377441406\n0, \t97112.9140625, \t-238.72393798828125, \t28.901844024658203\n....\n19, \t8858971136.0, \t-1976796.625, \t8770213.0\n19, \t271135948800.0, \t-1487331.875, \t8874354.0\n19, \t3010866446336.0, \t-3153109.5, \t8527317.0\n19, \t47926483091456.0, \t3631328.0, \t9911896.0\n```", "```py\n...\nplt.plot(loss_BGD, label=\"Batch Gradient Descent\")\nplt.xlabel('Epoch')\nplt.ylabel('Cost/Total loss')\nplt.legend()\nplt.show()\n```", "```py\nplt.plot(loss_SGD,label=\"Stochastic Gradient Descent\")\nplt.xlabel('Epoch')\nplt.ylabel('Cost/Total loss')\nplt.legend()\nplt.show()\n```", "```py\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Creating a function f(X) with a slope of -5\nX = torch.arange(-5, 5, 0.1).view(-1, 1)\nfunc = -5 * X\n\n# Adding Gaussian noise to the function f(X) and saving it in Y\nY = func + 0.4 * torch.randn(X.size())\n\n# Plot and visualizing the data points in blue\nplt.plot(X.numpy(), Y.numpy(), 'b+', label='Y')\nplt.plot(X.numpy(), func.numpy(), 'r', label='func')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.grid('True', color='y')\nplt.show()\n\n# defining the function for forward pass for prediction\ndef forward(x):\n    return w * x + b\n\n# evaluating data points with Mean Square Error (MSE)\ndef criterion(y_pred, y):\n    return torch.mean((y_pred - y) ** 2)\n\n# Batch gradient descent\nw = torch.tensor(-10.0, requires_grad=True)\nb = torch.tensor(-20.0, requires_grad=True)\nstep_size = 0.1\nloss_BGD = []\nn_iter = 20\n\nfor i in range (n_iter):\n    # making predictions with forward pass\n    Y_pred = forward(X)\n    # calculating the loss between original and predicted data points\n    loss = criterion(Y_pred, Y)\n    # storing the calculated loss in a list\n    loss_BGD.append(loss.item())\n    # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n    loss.backward()\n    # updateing the parameters after each iteration\n    w.data = w.data - step_size * w.grad.data\n    b.data = b.data - step_size * b.grad.data\n    # zeroing gradients after each iteration\n    w.grad.data.zero_()\n    b.grad.data.zero_()\n    # priting the values for understanding\n    print('{}, \\t{}, \\t{}, \\t{}'.format(i, loss.item(), w.item(), b.item()))\n\n# Stochastic gradient descent\nw = torch.tensor(-10.0, requires_grad=True)\nb = torch.tensor(-20.0, requires_grad=True)\nstep_size = 0.1\nloss_SGD = []\nn_iter = 20\n\nfor i in range(n_iter):  \n    # calculating true loss and storing it\n    Y_pred = forward(X)\n    # store the loss in the list\n    loss_SGD.append(criterion(Y_pred, Y).tolist())\n\n    for x, y in zip(X, Y):\n      # making a pridiction in forward pass\n      y_hat = forward(x)\n      # calculating the loss between original and predicted data points\n      loss = criterion(y_hat, y)\n      # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n      loss.backward()\n      # updateing the parameters after each iteration\n      w.data = w.data - step_size * w.grad.data\n      b.data = b.data - step_size * b.grad.data\n      # zeroing gradients after each iteration\n      w.grad.data.zero_()\n      b.grad.data.zero_()\n      # priting the values for understanding\n      print('{}, \\t{}, \\t{}, \\t{}'.format(i, loss.item(), w.item(), b.item()))\n\n# Plot graphs\nplt.plot(loss_BGD, label=\"Batch Gradient Descent\")\nplt.xlabel('Epoch')\nplt.ylabel('Cost/Total loss')\nplt.legend()\nplt.show()\n\nplt.plot(loss_SGD,label=\"Stochastic Gradient Descent\")\nplt.xlabel('Epoch')\nplt.ylabel('Cost/Total loss')\nplt.legend()\nplt.show()\n```"]