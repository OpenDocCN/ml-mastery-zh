["```py\npip install matplotlib seaborn bokeh\n```", "```py\npip install tensorflow pandas\n```", "```py\n# Importing from tensorflow and keras\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Reshape\nfrom tensorflow.keras import utils\nfrom tensorflow import dtypes, tensordot\nfrom tensorflow import convert_to_tensor, linalg, transpose\n# For math operations\nimport numpy as np\n# For plotting with matplotlib\nimport matplotlib.pyplot as plt\n# For plotting with seaborn\nimport seaborn as sns  \n# For plotting with bokeh\nfrom bokeh.plotting import figure, show\nfrom bokeh.models import Legend, LegendItem\n# For pandas dataframe\nimport pandas as pd\n```", "```py\n...\n# load dataset\n(x_train, train_labels), (_, _) = mnist.load_data()\n# Choose only the digits 0, 1, 2\ntotal_classes = 3\nind = np.where(train_labels < total_classes)\nx_train, train_labels = x_train[ind], train_labels[ind]\n# Shape of training data\ntotal_examples, img_length, img_width = x_train.shape\n# Print the statistics\nprint('Training data has ', total_examples, 'images')\nprint('Each image is of size ', img_length, 'x', img_width)\n```", "```py\nTraining data has  18623 images\nEach image is of size  28 x 28\n```", "```py\nimg_per_row = 8\nfig,ax = plt.subplots(nrows=2, ncols=img_per_row,\n                      figsize=(18,4),\n                      subplot_kw=dict(xticks=[], yticks=[]))\nfor row in [0, 1]:\n    for col in range(img_per_row):\n        ax[row, col].imshow(x_train[row*img_per_row + col].astype('int'))   \nplt.show()\n```", "```py\n...\n# Convert the dataset into a 2D array of shape 18623 x 784\nx = convert_to_tensor(np.reshape(x_train, (x_train.shape[0], -1)),\n                      dtype=dtypes.float32)\n# Eigen-decomposition from a 784 x 784 matrix\neigenvalues, eigenvectors = linalg.eigh(tensordot(transpose(x), x, axes=1))\n# Print the three largest eigenvalues\nprint('3 largest eigenvalues: ', eigenvalues[-3:])\n# Project the data to eigenvectors\nx_pca = tensordot(x, eigenvectors, axes=1)\n```", "```py\n3 largest eigenvalues:  tf.Tensor([5.1999642e+09 1.1419439e+10 4.8231231e+10], shape=(3,), dtype=float32)\n```", "```py\nfig, ax = plt.subplots(figsize=(12, 8))\nscatter = ax.scatter(x_pca[:, -1], x_pca[:, -2], c=train_labels, s=5)\nlegend_plt = ax.legend(*scatter.legend_elements(),\n                       loc=\"lower left\", title=\"Digits\")\nax.add_artist(legend_plt)\nplt.title('First Two Dimensions of Projected Data After Applying PCA')\nplt.show()\n```", "```py\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow import dtypes, tensordot\nfrom tensorflow import convert_to_tensor, linalg, transpose\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load dataset\n(x_train, train_labels), (_, _) = mnist.load_data()\n# Choose only the digits 0, 1, 2\ntotal_classes = 3\nind = np.where(train_labels < total_classes)\nx_train, train_labels = x_train[ind], train_labels[ind]\n# Verify the shape of training data\ntotal_examples, img_length, img_width = x_train.shape\nprint('Training data has ', total_examples, 'images')\nprint('Each image is of size ', img_length, 'x', img_width)\n\n# Convert the dataset into a 2D array of shape 18623 x 784\nx = convert_to_tensor(np.reshape(x_train, (x_train.shape[0], -1)),\n                      dtype=dtypes.float32)\n# Eigen-decomposition from a 784 x 784 matrix\neigenvalues, eigenvectors = linalg.eigh(tensordot(transpose(x), x, axes=1))\n# Print the three largest eigenvalues\nprint('3 largest eigenvalues: ', eigenvalues[-3:])\n# Project the data to eigenvectors\nx_pca = tensordot(x, eigenvectors, axes=1)\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(12, 8))\nscatter = ax.scatter(x_pca[:, -1], x_pca[:, -2], c=train_labels, s=5)\nlegend_plt = ax.legend(*scatter.legend_elements(),\n                       loc=\"lower left\", title=\"Digits\")\nax.add_artist(legend_plt)\nplt.title('First Two Dimensions of Projected Data After Applying PCA')\nplt.show()\n```", "```py\nfig = plt.figure(figsize=(12, 8))\nax = plt.axes(projection='3d')\nplt_3d = ax.scatter3D(x_pca[:, -1], x_pca[:, -2], x_pca[:, -3], c=train_labels, s=1)\nplt.colorbar(plt_3d)\nplt.show()\n```", "```py\nax.view_init(elev=30, azim=-60)\n```", "```py\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow import dtypes, tensordot\nfrom tensorflow import convert_to_tensor, linalg, transpose\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load dataset\n(x_train, train_labels), (_, _) = mnist.load_data()\n# Choose only the digits 0, 1, 2\ntotal_classes = 3\nind = np.where(train_labels < total_classes)\nx_train, train_labels = x_train[ind], train_labels[ind]\n# Verify the shape of training data\ntotal_examples, img_length, img_width = x_train.shape\nprint('Training data has ', total_examples, 'images')\nprint('Each image is of size ', img_length, 'x', img_width)\n\n# Convert the dataset into a 2D array of shape 18623 x 784\nx = convert_to_tensor(np.reshape(x_train, (x_train.shape[0], -1)),\n                      dtype=dtypes.float32)\n# Eigen-decomposition from a 784 x 784 matrix\neigenvalues, eigenvectors = linalg.eigh(tensordot(transpose(x), x, axes=1))\n# Print the three largest eigenvalues\nprint('3 largest eigenvalues: ', eigenvalues[-3:])\n# Project the data to eigenvectors\nx_pca = tensordot(x, eigenvectors, axes=1)\n\n# Create the plot\nfig = plt.figure(figsize=(12, 8))\nax = plt.axes(projection='3d')\nax.view_init(elev=30, azim=-60)\nplt_3d = ax.scatter3D(x_pca[:, -1], x_pca[:, -2], x_pca[:, -3], c=train_labels, s=1)\nplt.colorbar(plt_3d)\nplt.show()\n```", "```py\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.scatterplot(x_pca[:, -1], x_pca[:, -2],\n                style=train_labels, hue=train_labels,\n                palette=[\"red\", \"green\", \"blue\"])\nplt.title('First Two Dimensions of Projected Data After Applying PCA')\nplt.show()\n```", "```py\nsns.set(style = \"darkgrid\")\n```", "```py\ndf_mnist = pd.DataFrame(x_pca[:, -3:].numpy(), columns=[\"pca3\",\"pca2\",\"pca1\"])\ndf_mnist[\"label\"] = train_labels\nprint(df_mnist)\n```", "```py\n             pca3        pca2         pca1  label\n0     -537.730103  926.885254  1965.881592      0\n1      167.375885 -947.360107  1070.359375      1\n2      553.685425 -163.121826  1754.754272      2\n3     -642.905579 -767.283020  1053.937988      1\n4     -651.812988 -586.034424   662.468201      1\n...           ...         ...          ...    ...\n18618  415.358948 -645.245972   853.439209      1\n18619  754.555786    7.873116  1897.690552      2\n18620 -321.809357  665.038086  1840.480225      0\n18621  643.843628  -85.524895  1113.795166      2\n18622   94.964279 -549.570984   561.743042      1\n\n[18623 rows x 4 columns]\n```", "```py\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.scatterplot(data=df_mnist, x=\"pca1\", y=\"pca2\",\n                style=\"label\", hue=\"label\",\n                palette=[\"red\", \"green\", \"blue\"])\nplt.title('First Two Dimensions of Projected Data After Applying PCA')\nplt.show()\n```", "```py\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow import dtypes, tensordot\nfrom tensorflow import convert_to_tensor, linalg, transpose\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load dataset\n(x_train, train_labels), (_, _) = mnist.load_data()\n# Choose only the digits 0, 1, 2\ntotal_classes = 3\nind = np.where(train_labels < total_classes)\nx_train, train_labels = x_train[ind], train_labels[ind]\n# Verify the shape of training data\ntotal_examples, img_length, img_width = x_train.shape\nprint('Training data has ', total_examples, 'images')\nprint('Each image is of size ', img_length, 'x', img_width)\n\n# Convert the dataset into a 2D array of shape 18623 x 784\nx = convert_to_tensor(np.reshape(x_train, (x_train.shape[0], -1)),\n                      dtype=dtypes.float32)\n# Eigen-decomposition from a 784 x 784 matrix\neigenvalues, eigenvectors = linalg.eigh(tensordot(transpose(x), x, axes=1))\n# Print the three largest eigenvalues\nprint('3 largest eigenvalues: ', eigenvalues[-3:])\n# Project the data to eigenvectors\nx_pca = tensordot(x, eigenvectors, axes=1)\n\n# Making pandas DataFrame\ndf_mnist = pd.DataFrame(x_pca[:, -3:].numpy(), columns=[\"pca3\",\"pca2\",\"pca1\"])\ndf_mnist[\"label\"] = train_labels\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.scatterplot(data=df_mnist, x=\"pca1\", y=\"pca2\",\n                style=\"label\", hue=\"label\",\n                palette=[\"red\", \"green\", \"blue\"])\nplt.title('First Two Dimensions of Projected Data After Applying PCA')\nplt.show()\n```", "```py\ncolormap = {0: \"red\", 1:\"green\", 2:\"blue\"}\nmy_scatter = figure(title=\"First Two Dimensions of Projected Data After Applying PCA\", \n                    x_axis_label=\"Dimension 1\",\n                    y_axis_label=\"Dimension 2\")\nfor digit in [0, 1, 2]:\n    selection = x_pca[train_labels == digit]\n    my_scatter.scatter(selection[:,-1].numpy(), selection[:,-2].numpy(),\n                       color=colormap[digit], size=5,\n                       legend_label=\"Digit \"+str(digit))\nmy_scatter.legend.click_policy = \"hide\"\nshow(my_scatter)\n```", "```py\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow import dtypes, tensordot\nfrom tensorflow import convert_to_tensor, linalg, transpose\nimport numpy as np\nfrom bokeh.plotting import figure, show\n\n# Load dataset\n(x_train, train_labels), (_, _) = mnist.load_data()\n# Choose only the digits 0, 1, 2\ntotal_classes = 3\nind = np.where(train_labels < total_classes)\nx_train, train_labels = x_train[ind], train_labels[ind]\n# Verify the shape of training data\ntotal_examples, img_length, img_width = x_train.shape\nprint('Training data has ', total_examples, 'images')\nprint('Each image is of size ', img_length, 'x', img_width)\n\n# Convert the dataset into a 2D array of shape 18623 x 784\nx = convert_to_tensor(np.reshape(x_train, (x_train.shape[0], -1)),\n                      dtype=dtypes.float32)\n# Eigen-decomposition from a 784 x 784 matrix\neigenvalues, eigenvectors = linalg.eigh(tensordot(transpose(x), x, axes=1))\n# Print the three largest eigenvalues\nprint('3 largest eigenvalues: ', eigenvalues[-3:])\n# Project the data to eigenvectors\nx_pca = tensordot(x, eigenvectors, axes=1)\n\n# Create scatter plot in Bokeh\ncolormap = {0: \"red\", 1:\"green\", 2:\"blue\"}\nmy_scatter = figure(title=\"First Two Dimensions of Projected Data After Applying PCA\",\n                    x_axis_label=\"Dimension 1\",\n                    y_axis_label=\"Dimension 2\")\nfor digit in [0, 1, 2]:\n    selection = x_pca[train_labels == digit]\n    my_scatter.scatter(selection[:,-1].numpy(), selection[:,-2].numpy(),\n                       color=colormap[digit], size=5, alpha=0.5,\n                       legend_label=\"Digit \"+str(digit))\nmy_scatter.legend.click_policy = \"hide\"\nshow(my_scatter)\n```", "```py\nfrom bokeh.io import output_notebook\noutput_notebook()\n```", "```py\ncolormap = {0: \"red\", 1:\"green\", 2:\"blue\"}\ncolors = [colormap[i] for i in train_labels]\nmy_scatter = figure(title=\"First Two Dimensions of Projected Data After Applying PCA\", \n           x_axis_label=\"Dimension 1\", y_axis_label=\"Dimension 2\")\nscatter_obj = my_scatter.scatter(x_pca[:, -1].numpy(), x_pca[:, -2].numpy(), color=colors, size=5)\nlegend = Legend(items=[\n    LegendItem(label=\"Digit 0\", renderers=[scatter_obj], index=0),\n    LegendItem(label=\"Digit 1\", renderers=[scatter_obj], index=1),\n    LegendItem(label=\"Digit 2\", renderers=[scatter_obj], index=2),\n    ])\nmy_scatter.add_layout(legend)\nmy_scatter.legend.click_policy = \"hide\"\nshow(my_scatter)\n```", "```py\nepochs = 10\ny_train = utils.to_categorical(train_labels)\ninput_dim = img_length*img_width\n# Create a Sequential model\nmodel = Sequential()\n# First layer for reshaping input images from 2D to 1D\nmodel.add(Reshape((input_dim, ), input_shape=(img_length, img_width)))\n# Dense layer of 8 neurons\nmodel.add(Dense(8, activation='relu'))\n# Output layer\nmodel.add(Dense(total_classes, activation='softmax'))\n# Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(x_train, y_train, validation_split=0.33, epochs=epochs, batch_size=10, verbose=0)\nprint('Learning history: ', history.history)\n```", "```py\nLearning history:  {'loss': [0.5362154245376587, 0.08184114843606949, ...],\n'accuracy': [0.9426144361495972, 0.9763565063476562, ...],\n'val_loss': [0.09874073415994644, 0.07835448533296585, ...],\n'val_accuracy': [0.9716889262199402, 0.9788480401039124, ...]}\n```", "```py\nplt.plot(history.history['accuracy'], label=\"Training accuracy\")\nplt.plot(history.history['val_accuracy'], label=\"Validation accuracy\")\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n```", "```py\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Reshape\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load dataset\n(x_train, train_labels), (_, _) = mnist.load_data()\n# Choose only the digits 0, 1, 2\ntotal_classes = 3\nind = np.where(train_labels < total_classes)\nx_train, train_labels = x_train[ind], train_labels[ind]\n# Verify the shape of training data\ntotal_examples, img_length, img_width = x_train.shape\nprint('Training data has ', total_examples, 'images')\nprint('Each image is of size ', img_length, 'x', img_width)\n\n# Prepare for classifier network\nepochs = 10\ny_train = utils.to_categorical(train_labels)\ninput_dim = img_length*img_width\n# Create a Sequential model\nmodel = Sequential()\n# First layer for reshaping input images from 2D to 1D\nmodel.add(Reshape((input_dim, ), input_shape=(img_length, img_width)))\n# Dense layer of 8 neurons\nmodel.add(Dense(8, activation='relu'))\n# Output layer\nmodel.add(Dense(total_classes, activation='softmax'))\n# Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(x_train, y_train, validation_split=0.33, epochs=epochs, batch_size=10, verbose=0)\nprint('Learning history: ', history.history)\n\n# Plot accuracy in Matplotlib\nplt.plot(history.history['accuracy'], label=\"Training accuracy\")\nplt.plot(history.history['val_accuracy'], label=\"Validation accuracy\")\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n```", "```py\n# Create pandas DataFrame\ndf_history = pd.DataFrame(history.history)\nprint(df_history)\n\n# Plot using Seaborn\nmy_plot = sns.lineplot(data=df_history[[\"loss\",\"val_loss\"]])\nmy_plot.set_xlabel('Epochs')\nmy_plot.set_ylabel('Loss')\nplt.legend(labels=[\"Training\", \"Validation\"])\nplt.title('Training and Validation Loss')\nplt.show()\n```", "```py\n       loss  accuracy  val_loss  val_accuracy\n0  0.536215  0.942614  0.098741      0.971689\n1  0.081841  0.976357  0.078354      0.978848\n2  0.064002  0.978841  0.080637      0.972991\n3  0.055695  0.981726  0.064659      0.979987\n4  0.054693  0.984371  0.070817      0.983729\n5  0.053512  0.985173  0.069099      0.977709\n6  0.053916  0.983089  0.068139      0.979662\n7  0.048681  0.985093  0.064914      0.977709\n8  0.052084  0.982929  0.080508      0.971363\n9  0.040484  0.983890  0.111380      0.982590\n```", "```py\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Reshape\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load dataset\n(x_train, train_labels), (_, _) = mnist.load_data()\n# Choose only the digits 0, 1, 2\ntotal_classes = 3\nind = np.where(train_labels < total_classes)\nx_train, train_labels = x_train[ind], train_labels[ind]\n# Verify the shape of training data\ntotal_examples, img_length, img_width = x_train.shape\nprint('Training data has ', total_examples, 'images')\nprint('Each image is of size ', img_length, 'x', img_width)\n\n# Prepare for classifier network\nepochs = 10\ny_train = utils.to_categorical(train_labels)\ninput_dim = img_length*img_width\n# Create a Sequential model\nmodel = Sequential()\n# First layer for reshaping input images from 2D to 1D\nmodel.add(Reshape((input_dim, ), input_shape=(img_length, img_width)))\n# Dense layer of 8 neurons\nmodel.add(Dense(8, activation='relu'))\n# Output layer\nmodel.add(Dense(total_classes, activation='softmax'))\n# Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(x_train, y_train, validation_split=0.33, epochs=epochs, batch_size=10, verbose=0)\n\n# Prepare pandas DataFrame\ndf_history = pd.DataFrame(history.history)\nprint(df_history)\n\n# Plot loss in seaborn\nmy_plot = sns.lineplot(data=df_history[[\"loss\",\"val_loss\"]])\nmy_plot.set_xlabel('Epochs')\nmy_plot.set_ylabel('Loss')\nplt.legend(labels=[\"Training\", \"Validation\"])\nplt.title('Training and Validation Loss')\nplt.show()\n```", "```py\np = figure(title=\"Training and validation accuracy\",\n           x_axis_label=\"Epochs\", y_axis_label=\"Accuracy\")\nepochs_array = np.arange(epochs)\np.line(epochs_array, df_history['accuracy'], legend_label=\"Training\",\n       color=\"blue\", line_width=2)\np.line(epochs_array, df_history['val_accuracy'], legend_label=\"Validation\",\n       color=\"green\")\np.legend.click_policy = \"hide\"\np.legend.location = 'bottom_right'\nshow(p)\n```", "```py\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Reshape\nimport numpy as np\nimport pandas as pd\nfrom bokeh.plotting import figure, show\n\n# Load dataset\n(x_train, train_labels), (_, _) = mnist.load_data()\n# Choose only the digits 0, 1, 2\ntotal_classes = 3\nind = np.where(train_labels < total_classes)\nx_train, train_labels = x_train[ind], train_labels[ind]\n# Verify the shape of training data\ntotal_examples, img_length, img_width = x_train.shape\nprint('Training data has ', total_examples, 'images')\nprint('Each image is of size ', img_length, 'x', img_width)\n\n# Prepare for classifier network\nepochs = 10\ny_train = utils.to_categorical(train_labels)\ninput_dim = img_length*img_width\n# Create a Sequential model\nmodel = Sequential()\n# First layer for reshaping input images from 2D to 1D\nmodel.add(Reshape((input_dim, ), input_shape=(img_length, img_width)))\n# Dense layer of 8 neurons\nmodel.add(Dense(8, activation='relu'))\n# Output layer\nmodel.add(Dense(total_classes, activation='softmax'))\n# Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(x_train, y_train, validation_split=0.33, epochs=epochs, batch_size=10, verbose=0)\n\n# Prepare pandas DataFrame\ndf_history = pd.DataFrame(history.history)\nprint(df_history)\n\n# Plot accuracy in Bokeh\np = figure(title=\"Training and validation accuracy\",\n           x_axis_label=\"Epochs\", y_axis_label=\"Accuracy\")\nepochs_array = np.arange(epochs)\np.line(epochs_array, df_history['accuracy'], legend_label=\"Training\",\n       color=\"blue\", line_width=2)\np.line(epochs_array, df_history['val_accuracy'], legend_label=\"Validation\",\n       color=\"green\")\np.legend.click_policy = \"hide\"\np.legend.location = 'bottom_right'\nshow(p)\n```", "```py\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Reshape\nfrom tensorflow import dtypes, tensordot\nfrom tensorflow import convert_to_tensor, linalg, transpose\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load dataset\n(x_train, train_labels), (_, _) = mnist.load_data()\n# Choose only the digits 0, 1, 2\ntotal_classes = 3\nind = np.where(train_labels < total_classes)\nx_train, train_labels = x_train[ind], train_labels[ind]\n# Verify the shape of training data\ntotal_examples, img_length, img_width = x_train.shape\nprint('Training data has ', total_examples, 'images')\nprint('Each image is of size ', img_length, 'x', img_width)\n\n# Convert the dataset into a 2D array of shape 18623 x 784\nx = convert_to_tensor(np.reshape(x_train, (x_train.shape[0], -1)),\n                      dtype=dtypes.float32)\n# Eigen-decomposition from a 784 x 784 matrix\neigenvalues, eigenvectors = linalg.eigh(tensordot(transpose(x), x, axes=1))\n# Print the three largest eigenvalues\nprint('3 largest eigenvalues: ', eigenvalues[-3:])\n# Project the data to eigenvectors\nx_pca = tensordot(x, eigenvectors, axes=1)\n\n# Prepare for classifier network\nepochs = 10\ny_train = utils.to_categorical(train_labels)\ninput_dim = img_length*img_width\n# Create a Sequential model\nmodel = Sequential()\n# First layer for reshaping input images from 2D to 1D\nmodel.add(Reshape((input_dim, ), input_shape=(img_length, img_width)))\n# Dense layer of 8 neurons\nmodel.add(Dense(8, activation='relu'))\n# Output layer\nmodel.add(Dense(total_classes, activation='softmax'))\n# Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(x_train, y_train, validation_split=0.33, epochs=epochs, batch_size=10, verbose=0)\n\n# Prepare pandas DataFrame\ndf_history = pd.DataFrame(history.history)\nprint(df_history)\n\n# Plot side-by-side\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,6))\n# left plot\nscatter = ax[0].scatter(x_pca[:, -1], x_pca[:, -2], c=train_labels, s=5)\nlegend_plt = ax[0].legend(*scatter.legend_elements(),\n                         loc=\"lower left\", title=\"Digits\")\nax[0].add_artist(legend_plt)\nax[0].set_title('First Two Dimensions of Projected Data After Applying PCA')\n# right plot\nmy_plot = sns.lineplot(data=df_history[[\"loss\",\"val_loss\"]], ax=ax[1])\nmy_plot.set_xlabel('Epochs')\nmy_plot.set_ylabel('Loss')\nax[1].legend(labels=[\"Training\", \"Validation\"])\nax[1].set_title('Training and Validation Loss')\nplt.show()\n```", "```py\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Reshape\nfrom tensorflow import dtypes, tensordot\nfrom tensorflow import convert_to_tensor, linalg, transpose\nimport numpy as np\nimport pandas as pd\nfrom bokeh.plotting import figure, show\nfrom bokeh.layouts import row\n\n# Load dataset\n(x_train, train_labels), (_, _) = mnist.load_data()\n# Choose only the digits 0, 1, 2\ntotal_classes = 3\nind = np.where(train_labels < total_classes)\nx_train, train_labels = x_train[ind], train_labels[ind]\n# Verify the shape of training data\ntotal_examples, img_length, img_width = x_train.shape\nprint('Training data has ', total_examples, 'images')\nprint('Each image is of size ', img_length, 'x', img_width)\n\n# Convert the dataset into a 2D array of shape 18623 x 784\nx = convert_to_tensor(np.reshape(x_train, (x_train.shape[0], -1)),\n                      dtype=dtypes.float32)\n# Eigen-decomposition from a 784 x 784 matrix\neigenvalues, eigenvectors = linalg.eigh(tensordot(transpose(x), x, axes=1))\n# Print the three largest eigenvalues\nprint('3 largest eigenvalues: ', eigenvalues[-3:])\n# Project the data to eigenvectors\nx_pca = tensordot(x, eigenvectors, axes=1)\n\n# Prepare for classifier network\nepochs = 10\ny_train = utils.to_categorical(train_labels)\ninput_dim = img_length*img_width\n# Create a Sequential model\nmodel = Sequential()\n# First layer for reshaping input images from 2D to 1D\nmodel.add(Reshape((input_dim, ), input_shape=(img_length, img_width)))\n# Dense layer of 8 neurons\nmodel.add(Dense(8, activation='relu'))\n# Output layer\nmodel.add(Dense(total_classes, activation='softmax'))\n# Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(x_train, y_train, validation_split=0.33, epochs=epochs, batch_size=10, verbose=0)\n\n# Prepare pandas DataFrame\ndf_history = pd.DataFrame(history.history)\nprint(df_history)\n\n# Create scatter plot in Bokeh\ncolormap = {0: \"red\", 1:\"green\", 2:\"blue\"}\nmy_scatter = figure(title=\"First Two Dimensions of Projected Data After Applying PCA\",\n                    x_axis_label=\"Dimension 1\",\n                    y_axis_label=\"Dimension 2\",\n                    width=500, height=400)\nfor digit in [0, 1, 2]:\n    selection = x_pca[train_labels == digit]\n    my_scatter.scatter(selection[:,-1].numpy(), selection[:,-2].numpy(),\n                       color=colormap[digit], size=5, alpha=0.5,\n                       legend_label=\"Digit \"+str(digit))\nmy_scatter.legend.click_policy = \"hide\"\n\n# Plot accuracy in Bokeh\np = figure(title=\"Training and validation accuracy\",\n           x_axis_label=\"Epochs\", y_axis_label=\"Accuracy\",\n           width=500, height=400)\nepochs_array = np.arange(epochs)\np.line(epochs_array, df_history['accuracy'], legend_label=\"Training\",\n       color=\"blue\", line_width=2)\np.line(epochs_array, df_history['val_accuracy'], legend_label=\"Validation\",\n       color=\"green\")\np.legend.click_policy = \"hide\"\np.legend.location = 'bottom_right'\n\nshow(row(my_scatter, p))\n```"]