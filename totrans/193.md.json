["```py\nclean_dataset = load(open(filename, 'rb'))\n```", "```py\ndataset = clean_dataset[:self.n_sentences, :]\n```", "```py\nfor i in range(dataset[:, 0].size):\n\tdataset[i, 0] = \"<START> \" + dataset[i, 0] + \" <EOS>\"\n\tdataset[i, 1] = \"<START> \" + dataset[i, 1] + \" <EOS>\"\n```", "```py\nshuffle(dataset)\n```", "```py\ntrain = dataset[:int(self.n_sentences * self.train_split)]\n```", "```py\nenc_tokenizer = self.create_tokenizer(train[:, 0])\nenc_seq_length = self.find_seq_length(train[:, 0])\nenc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n```", "```py\ntrainX = enc_tokenizer.texts_to_sequences(train[:, 0])\ntrainX = pad_sequences(trainX, maxlen=enc_seq_length, padding='post')\ntrainX = convert_to_tensor(trainX, dtype=int64)\n```", "```py\ndec_tokenizer = self.create_tokenizer(train[:, 1])\ndec_seq_length = self.find_seq_length(train[:, 1])\ndec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n```", "```py\ntrainY = dec_tokenizer.texts_to_sequences(train[:, 1])\ntrainY = pad_sequences(trainY, maxlen=dec_seq_length, padding='post')\ntrainY = convert_to_tensor(trainY, dtype=int64)\n```", "```py\nfrom pickle import load\nfrom numpy.random import shuffle\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import convert_to_tensor, int64\n\nclass PrepareDataset:\n\tdef __init__(self, **kwargs):\n\t\tsuper(PrepareDataset, self).__init__(**kwargs)\n\t\tself.n_sentences = 10000  # Number of sentences to include in the dataset\n\t\tself.train_split = 0.9  # Ratio of the training data split\n\n\t# Fit a tokenizer\n\tdef create_tokenizer(self, dataset):\n\t\ttokenizer = Tokenizer()\n\t\ttokenizer.fit_on_texts(dataset)\n\n\t\treturn tokenizer\n\n\tdef find_seq_length(self, dataset):\n\t\treturn max(len(seq.split()) for seq in dataset)\n\n\tdef find_vocab_size(self, tokenizer, dataset):\n\t\ttokenizer.fit_on_texts(dataset)\n\n\t\treturn len(tokenizer.word_index) + 1\n\n\tdef __call__(self, filename, **kwargs):\n\t\t# Load a clean dataset\n\t\tclean_dataset = load(open(filename, 'rb'))\n\n\t\t# Reduce dataset size\n\t\tdataset = clean_dataset[:self.n_sentences, :]\n\n\t\t# Include start and end of string tokens\n\t\tfor i in range(dataset[:, 0].size):\n\t\t\tdataset[i, 0] = \"<START> \" + dataset[i, 0] + \" <EOS>\"\n\t\t\tdataset[i, 1] = \"<START> \" + dataset[i, 1] + \" <EOS>\"\n\n\t\t# Random shuffle the dataset\n\t\tshuffle(dataset)\n\n\t\t# Split the dataset\n\t\ttrain = dataset[:int(self.n_sentences * self.train_split)]\n\n\t\t# Prepare tokenizer for the encoder input\n\t\tenc_tokenizer = self.create_tokenizer(train[:, 0])\n\t\tenc_seq_length = self.find_seq_length(train[:, 0])\n\t\tenc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n\n\t\t# Encode and pad the input sequences\n\t\ttrainX = enc_tokenizer.texts_to_sequences(train[:, 0])\n\t\ttrainX = pad_sequences(trainX, maxlen=enc_seq_length, padding='post')\n\t\ttrainX = convert_to_tensor(trainX, dtype=int64)\n\n\t\t# Prepare tokenizer for the decoder input\n\t\tdec_tokenizer = self.create_tokenizer(train[:, 1])\n\t\tdec_seq_length = self.find_seq_length(train[:, 1])\n\t\tdec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n\n\t\t# Encode and pad the input sequences\n\t\ttrainY = dec_tokenizer.texts_to_sequences(train[:, 1])\n\t\ttrainY = pad_sequences(trainY, maxlen=dec_seq_length, padding='post')\n\t\ttrainY = convert_to_tensor(trainY, dtype=int64)\n\n\t\treturn trainX, trainY, train, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size\n```", "```py\n# Prepare the training data\ndataset = PrepareDataset()\ntrainX, trainY, train_orig, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')\n\nprint(train_orig[0, 0], '\\n', trainX[0, :])\n```", "```py\n<START> did tom tell you <EOS> \n tf.Tensor([ 1 25  4 97  5  2  0], shape=(7,), dtype=int64)\n```", "```py\nprint('Encoder sequence length:', enc_seq_length)\n```", "```py\nEncoder sequence length: 7\n```", "```py\nprint(train_orig[0, 1], '\\n', trainY[0, :])\n```", "```py\n<START> hat tom es dir gesagt <EOS> \n tf.Tensor([  1  14   5   7  42 162   2   0   0   0   0   0], shape=(12,), dtype=int64)\n```", "```py\nprint('Decoder sequence length:', dec_seq_length)\n```", "```py\nDecoder sequence length: 12\n```", "```py\ndef loss_fcn(target, prediction):\n    # Create mask so that the zero padding values are not included in the computation of loss\n    padding_mask = math.logical_not(equal(target, 0))\n    padding_mask = cast(padding_mask, float32)\n\n    # Compute a sparse categorical cross-entropy loss on the unmasked values\n    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * padding_mask\n\n    # Compute the mean loss over the unmasked values\n    return reduce_sum(loss) / reduce_sum(padding_mask)\n```", "```py\ndef accuracy_fcn(target, prediction):\n    # Create mask so that the zero padding values are not included in the computation of accuracy\n    padding_mask = math.logical_not(math.equal(target, 0))\n\n    # Find equal prediction and target values, and apply the padding mask\n    accuracy = equal(target, argmax(prediction, axis=2))\n    accuracy = math.logical_and(padding_mask, accuracy)\n\n    # Cast the True/False values to 32-bit-precision floating-point numbers\n    padding_mask = cast(padding_mask, float32)\n    accuracy = cast(accuracy, float32)\n\n    # Compute the mean accuracy over the unmasked values\n    return reduce_sum(accuracy) / reduce_sum(padding_mask)\n```", "```py\n# Define the model parameters\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_model = 512  # Dimensionality of model layers' outputs\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nn = 6  # Number of layers in the encoder stack\n\n# Define the training parameters\nepochs = 2\nbatch_size = 64\nbeta_1 = 0.9\nbeta_2 = 0.98\nepsilon = 1e-9\ndropout_rate = 0.1\n```", "```py\nclass LRScheduler(LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n        super(LRScheduler, self).__init__(**kwargs)\n\n        self.d_model = cast(d_model, float32)\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step_num):\n\n        # Linearly increasing the learning rate for the first warmup_steps, and decreasing it thereafter\n        arg1 = step_num ** -0.5\n        arg2 = step_num * (self.warmup_steps ** -1.5)\n\n        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)\n```", "```py\noptimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)\n```", "```py\ntrain_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\ntrain_dataset = train_dataset.batch(batch_size)\n```", "```py\ntraining_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n```", "```py\n@function\ndef train_step(encoder_input, decoder_input, decoder_output):\n    with GradientTape() as tape:\n\n        # Run the forward pass of the model to generate a prediction\n        prediction = training_model(encoder_input, decoder_input, training=True)\n\n        # Compute the training loss\n        loss = loss_fcn(decoder_output, prediction)\n\n        # Compute the training accuracy\n        accuracy = accuracy_fcn(decoder_output, prediction)\n\n    # Retrieve gradients of the trainable variables with respect to the training loss\n    gradients = tape.gradient(loss, training_model.trainable_weights)\n\n    # Update the values of the trainable variables by gradient descent\n    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n\n    train_loss(loss)\n    train_accuracy(accuracy)\n```", "```py\ntrain_loss = Mean(name='train_loss')\ntrain_accuracy = Mean(name='train_accuracy')\n\n# Create a checkpoint object and manager to manage multiple checkpoints\nckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\nckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=3)\n\nfor epoch in range(epochs):\n\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n\n    print(\"\\nStart of epoch %d\" % (epoch + 1))\n\n    # Iterate over the dataset batches\n    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n\n        # Define the encoder and decoder inputs, and the decoder output\n        encoder_input = train_batchX[:, 1:]\n        decoder_input = train_batchY[:, :-1]\n        decoder_output = train_batchY[:, 1:]\n\n        train_step(encoder_input, decoder_input, decoder_output)\n\n        if step % 50 == 0:\n            print(f'Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n\n    # Print epoch number and loss value at the end of every epoch\n    print(\"Epoch %d: Training Loss %.4f, Training Accuracy %.4f\" % (epoch + 1, train_loss.result(), train_accuracy.result()))\n\n    # Save a checkpoint after every five epochs\n    if (epoch + 1) % 5 == 0:\n        save_path = ckpt_manager.save()\n        print(\"Saved checkpoint at epoch %d\" % (epoch + 1))\n```", "```py\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import LearningRateSchedule\nfrom tensorflow.keras.metrics import Mean\nfrom tensorflow import data, train, math, reduce_sum, cast, equal, argmax, float32, GradientTape, TensorSpec, function, int64\nfrom keras.losses import sparse_categorical_crossentropy\nfrom model import TransformerModel\nfrom prepare_dataset import PrepareDataset\nfrom time import time\n\n# Define the model parameters\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_model = 512  # Dimensionality of model layers' outputs\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nn = 6  # Number of layers in the encoder stack\n\n# Define the training parameters\nepochs = 2\nbatch_size = 64\nbeta_1 = 0.9\nbeta_2 = 0.98\nepsilon = 1e-9\ndropout_rate = 0.1\n\n# Implementing a learning rate scheduler\nclass LRScheduler(LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n        super(LRScheduler, self).__init__(**kwargs)\n\n        self.d_model = cast(d_model, float32)\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step_num):\n\n        # Linearly increasing the learning rate for the first warmup_steps, and decreasing it thereafter\n        arg1 = step_num ** -0.5\n        arg2 = step_num * (self.warmup_steps ** -1.5)\n\n        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)\n\n# Instantiate an Adam optimizer\noptimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)\n\n# Prepare the training and test splits of the dataset\ndataset = PrepareDataset()\ntrainX, trainY, train_orig, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')\n\n# Prepare the dataset batches\ntrain_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\ntrain_dataset = train_dataset.batch(batch_size)\n\n# Create model\ntraining_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n\n# Defining the loss function\ndef loss_fcn(target, prediction):\n    # Create mask so that the zero padding values are not included in the computation of loss\n    padding_mask = math.logical_not(equal(target, 0))\n    padding_mask = cast(padding_mask, float32)\n\n    # Compute a sparse categorical cross-entropy loss on the unmasked values\n    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * padding_mask\n\n    # Compute the mean loss over the unmasked values\n    return reduce_sum(loss) / reduce_sum(padding_mask)\n\n# Defining the accuracy function\ndef accuracy_fcn(target, prediction):\n    # Create mask so that the zero padding values are not included in the computation of accuracy\n    padding_mask = math.logical_not(equal(target, 0))\n\n    # Find equal prediction and target values, and apply the padding mask\n    accuracy = equal(target, argmax(prediction, axis=2))\n    accuracy = math.logical_and(padding_mask, accuracy)\n\n    # Cast the True/False values to 32-bit-precision floating-point numbers\n    padding_mask = cast(padding_mask, float32)\n    accuracy = cast(accuracy, float32)\n\n    # Compute the mean accuracy over the unmasked values\n    return reduce_sum(accuracy) / reduce_sum(padding_mask)\n\n# Include metrics monitoring\ntrain_loss = Mean(name='train_loss')\ntrain_accuracy = Mean(name='train_accuracy')\n\n# Create a checkpoint object and manager to manage multiple checkpoints\nckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\nckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=3)\n\n# Speeding up the training process\n@function\ndef train_step(encoder_input, decoder_input, decoder_output):\n    with GradientTape() as tape:\n\n        # Run the forward pass of the model to generate a prediction\n        prediction = training_model(encoder_input, decoder_input, training=True)\n\n        # Compute the training loss\n        loss = loss_fcn(decoder_output, prediction)\n\n        # Compute the training accuracy\n        accuracy = accuracy_fcn(decoder_output, prediction)\n\n    # Retrieve gradients of the trainable variables with respect to the training loss\n    gradients = tape.gradient(loss, training_model.trainable_weights)\n\n    # Update the values of the trainable variables by gradient descent\n    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n\n    train_loss(loss)\n    train_accuracy(accuracy)\n\nfor epoch in range(epochs):\n\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n\n    print(\"\\nStart of epoch %d\" % (epoch + 1))\n\n    start_time = time()\n\n    # Iterate over the dataset batches\n    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n\n        # Define the encoder and decoder inputs, and the decoder output\n        encoder_input = train_batchX[:, 1:]\n        decoder_input = train_batchY[:, :-1]\n        decoder_output = train_batchY[:, 1:]\n\n        train_step(encoder_input, decoder_input, decoder_output)\n\n        if step % 50 == 0:\n            print(f'Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n            # print(\"Samples so far: %s\" % ((step + 1) * batch_size))\n\n    # Print epoch number and loss value at the end of every epoch\n    print(\"Epoch %d: Training Loss %.4f, Training Accuracy %.4f\" % (epoch + 1, train_loss.result(), train_accuracy.result()))\n\n    # Save a checkpoint after every five epochs\n    if (epoch + 1) % 5 == 0:\n        save_path = ckpt_manager.save()\n        print(\"Saved checkpoint at epoch %d\" % (epoch + 1))\n\nprint(\"Total time taken: %.2fs\" % (time() - start_time))\n```", "```py\nStart of epoch 1\nEpoch 1 Step 0 Loss 8.4525 Accuracy 0.0000\nEpoch 1 Step 50 Loss 7.6768 Accuracy 0.1234\nEpoch 1 Step 100 Loss 7.0360 Accuracy 0.1713\nEpoch 1: Training Loss 6.7109, Training Accuracy 0.1924\n\nStart of epoch 2\nEpoch 2 Step 0 Loss 5.7323 Accuracy 0.2628\nEpoch 2 Step 50 Loss 5.4360 Accuracy 0.2756\nEpoch 2 Step 100 Loss 5.2638 Accuracy 0.2839\nEpoch 2: Training Loss 5.1468, Training Accuracy 0.2908\nTotal time taken: 87.98s\n```"]