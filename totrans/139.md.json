["```py\nimport numpy as np\ndata = np.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n```", "```py\n# find the boundary at 66% of total samples\ncount = len(data)\nn_train = int(count * 0.66)\n# split the data at the boundary\ntrain_data = data[:n_train]\ntest_data = data[n_train:]\n```", "```py\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\ndata = np.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\ntrain_data, test_data = train_test_split(data, test_size=0.33)\n```", "```py\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\n\ndata = np.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\nX = data[:, 0:8]\ny = data[:, 8]\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport tqdm\n\n...\n\nmodel = nn.Sequential(\n    nn.Linear(8, 12),\n    nn.ReLU(),\n    nn.Linear(12, 8),\n    nn.ReLU(),\n    nn.Linear(8, 1),\n    nn.Sigmoid()\n)\n\n# loss function and optimizer\nloss_fn = nn.BCELoss()  # binary cross entropy\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\nn_epochs = 50    # number of epochs to run\nbatch_size = 10  # size of each batch\nbatches_per_epoch = len(Xtrain) // batch_size\n\nfor epoch in range(n_epochs):\n    with tqdm.trange(batches_per_epoch, unit=\"batch\", mininterval=0) as bar:\n        bar.set_description(f\"Epoch {epoch}\")\n        for i in bar:\n            # take a batch\n            start = i * batch_size\n            X_batch = X_train[start:start+batch_size]\n            y_batch = y_train[start:start+batch_size]\n            # forward pass\n            y_pred = model(X_batch)\n            loss = loss_fn(y_pred, y_batch)\n            # backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            # update weights\n            optimizer.step()\n            # print progress\n            bar.set_postfix(\n                loss=float(loss)\n            )\n```", "```py\nfor epoch in range(n_epochs):\n    with tqdm.trange(batches_per_epoch, unit=\"batch\", mininterval=0) as bar:\n        bar.set_description(f\"Epoch {epoch}\")\n        for i in bar:\n            # take a batch\n            start = i * batch_size\n            X_batch = X_train[start:start+batch_size]\n            y_batch = y_train[start:start+batch_size]\n            # forward pass\n            y_pred = model(X_batch)\n            loss = loss_fn(y_pred, y_batch)\n            # backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            # update weights\n            optimizer.step()\n            # print progress, with accuracy\n            acc = (y_pred.round() == y_batch).float().mean()\n            bar.set_postfix(\n                loss=float(loss)\n                acc=float(acc)\n            )\n```", "```py\nfor epoch in range(n_epochs):\n    with tqdm.trange(batches_per_epoch, unit=\"batch\", mininterval=0) as bar:\n        bar.set_description(f\"Epoch {epoch}\")\n        for i in bar:\n            # take a batch\n            start = i * batch_size\n            X_batch = X_train[start:start+batch_size]\n            y_batch = y_train[start:start+batch_size]\n            # forward pass\n            y_pred = model(X_batch)\n            loss = loss_fn(y_pred, y_batch)\n            # backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            # update weights\n            optimizer.step()\n            # print progress\n            acc = (y_pred.round() == y_batch).float().mean()\n            bar.set_postfix(\n                loss=float(loss),\n                acc=float(acc)\n            )\n    # evaluate model at end of epoch\n    y_pred = model(X_test)\n    acc = (y_pred.round() == y_test).float().mean()\n    acc = float(acc)\n    print(f\"End of {epoch}, accuracy {acc}\")\n```", "```py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport tqdm\nfrom sklearn.model_selection import train_test_split\n\ndata = np.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\nX = data[:, 0:8]\ny = data[:, 8]\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n\nmodel = nn.Sequential(\n    nn.Linear(8, 12),\n    nn.ReLU(),\n    nn.Linear(12, 8),\n    nn.ReLU(),\n    nn.Linear(8, 1),\n    nn.Sigmoid()\n)\n\n# loss function and optimizer\nloss_fn = nn.BCELoss()  # binary cross entropy\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\nn_epochs = 50    # number of epochs to run\nbatch_size = 10  # size of each batch\nbatches_per_epoch = len(X_train) // batch_size\n\nfor epoch in range(n_epochs):\n    with tqdm.trange(batches_per_epoch, unit=\"batch\", mininterval=0) as bar: #, disable=True) as bar:\n        bar.set_description(f\"Epoch {epoch}\")\n        for i in bar:\n            # take a batch\n            start = i * batch_size\n            X_batch = X_train[start:start+batch_size]\n            y_batch = y_train[start:start+batch_size]\n            # forward pass\n            y_pred = model(X_batch)\n            loss = loss_fn(y_pred, y_batch)\n            # backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            # update weights\n            optimizer.step()\n            # print progress\n            acc = (y_pred.round() == y_batch).float().mean()\n            bar.set_postfix(\n                loss=float(loss),\n                acc=float(acc)\n            )\n    # evaluate model at end of epoch\n    y_pred = model(X_test)\n    acc = (y_pred.round() == y_test).float().mean()\n    acc = float(acc)\n    print(f\"End of {epoch}, accuracy {acc}\")\n```", "```py\nEnd of 0, accuracy 0.5787401795387268\nEnd of 1, accuracy 0.6102362275123596\nEnd of 2, accuracy 0.6220472455024719\nEnd of 3, accuracy 0.6220472455024719\nEnd of 4, accuracy 0.6299212574958801\nEnd of 5, accuracy 0.6377952694892883\nEnd of 6, accuracy 0.6496062874794006\nEnd of 7, accuracy 0.6535432934761047\nEnd of 8, accuracy 0.665354311466217\nEnd of 9, accuracy 0.6614173054695129\nEnd of 10, accuracy 0.665354311466217\nEnd of 11, accuracy 0.665354311466217\nEnd of 12, accuracy 0.665354311466217\nEnd of 13, accuracy 0.665354311466217\nEnd of 14, accuracy 0.665354311466217\nEnd of 15, accuracy 0.6732283234596252\nEnd of 16, accuracy 0.6771653294563293\nEnd of 17, accuracy 0.6811023354530334\nEnd of 18, accuracy 0.6850393414497375\nEnd of 19, accuracy 0.6889764070510864\nEnd of 20, accuracy 0.6850393414497375\nEnd of 21, accuracy 0.6889764070510864\nEnd of 22, accuracy 0.6889764070510864\nEnd of 23, accuracy 0.6889764070510864\nEnd of 24, accuracy 0.6889764070510864\nEnd of 25, accuracy 0.6850393414497375\nEnd of 26, accuracy 0.6811023354530334\nEnd of 27, accuracy 0.6771653294563293\nEnd of 28, accuracy 0.6771653294563293\nEnd of 29, accuracy 0.6692913174629211\nEnd of 30, accuracy 0.6732283234596252\nEnd of 31, accuracy 0.6692913174629211\nEnd of 32, accuracy 0.6692913174629211\nEnd of 33, accuracy 0.6732283234596252\nEnd of 34, accuracy 0.6771653294563293\nEnd of 35, accuracy 0.6811023354530334\nEnd of 36, accuracy 0.6811023354530334\nEnd of 37, accuracy 0.6811023354530334\nEnd of 38, accuracy 0.6811023354530334\nEnd of 39, accuracy 0.6811023354530334\nEnd of 40, accuracy 0.6811023354530334\nEnd of 41, accuracy 0.6771653294563293\nEnd of 42, accuracy 0.6771653294563293\nEnd of 43, accuracy 0.6771653294563293\nEnd of 44, accuracy 0.6771653294563293\nEnd of 45, accuracy 0.6771653294563293\nEnd of 46, accuracy 0.6771653294563293\nEnd of 47, accuracy 0.6732283234596252\nEnd of 48, accuracy 0.6732283234596252\nEnd of 49, accuracy 0.6732283234596252\n```", "```py\ndef model_train(X_train, y_train, X_test, y_test):\n    # create new model\n    model = nn.Sequential(\n        nn.Linear(8, 12),\n        nn.ReLU(),\n        nn.Linear(12, 8),\n        nn.ReLU(),\n        nn.Linear(8, 1),\n        nn.Sigmoid()\n    )\n\n    # loss function and optimizer\n    loss_fn = nn.BCELoss()  # binary cross entropy\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n    n_epochs = 25    # number of epochs to run\n    batch_size = 10  # size of each batch\n    batches_per_epoch = len(X_train) // batch_size\n\n    for epoch in range(n_epochs):\n        with tqdm.trange(batches_per_epoch, unit=\"batch\", mininterval=0, disable=True) as bar:\n            bar.set_description(f\"Epoch {epoch}\")\n            for i in bar:\n                # take a batch\n                start = i * batch_size\n                X_batch = X_train[start:start+batch_size]\n                y_batch = y_train[start:start+batch_size]\n                # forward pass\n                y_pred = model(X_batch)\n                loss = loss_fn(y_pred, y_batch)\n                # backward pass\n                optimizer.zero_grad()\n                loss.backward()\n                # update weights\n                optimizer.step()\n                # print progress\n                acc = (y_pred.round() == y_batch).float().mean()\n                bar.set_postfix(\n                    loss=float(loss),\n                    acc=float(acc)\n                )\n    # evaluate accuracy at end of training\n    y_pred = model(X_test)\n    acc = (y_pred.round() == y_test).float().mean()\n    return float(acc)\n```", "```py\nfrom sklearn.model_selection import StratifiedKFold\n\n# define 5-fold cross validation test harness\nkfold = StratifiedKFold(n_splits=5, shuffle=True)\ncv_scores = []\nfor train, test in kfold.split(X, y):\n    # create model, train, and get accuracy\n    acc = model_train(X[train], y[train], X[test], y[test])\n    print(\"Accuracy: %.2f\" % acc)\n    cv_scores.append(acc)\n# evaluate the model\nprint(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores)*100, np.std(cv_scores)*100))\n```", "```py\nAccuracy: 0.64\nAccuracy: 0.67\nAccuracy: 0.68\nAccuracy: 0.63\nAccuracy: 0.59\n64.05% (+/- 3.30%)\n```", "```py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport tqdm\nfrom sklearn.model_selection import StratifiedKFold\n\ndata = np.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\nX = data[:, 0:8]\ny = data[:, 8]\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n\ndef model_train(X_train, y_train, X_test, y_test):\n    # create new model\n    model = nn.Sequential(\n        nn.Linear(8, 12),\n        nn.ReLU(),\n        nn.Linear(12, 8),\n        nn.ReLU(),\n        nn.Linear(8, 1),\n        nn.Sigmoid()\n    )\n\n    # loss function and optimizer\n    loss_fn = nn.BCELoss()  # binary cross entropy\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n    n_epochs = 25    # number of epochs to run\n    batch_size = 10  # size of each batch\n    batches_per_epoch = len(X_train) // batch_size\n\n    for epoch in range(n_epochs):\n        with tqdm.trange(batches_per_epoch, unit=\"batch\", mininterval=0, disable=True) as bar:\n            bar.set_description(f\"Epoch {epoch}\")\n            for i in bar:\n                # take a batch\n                start = i * batch_size\n                X_batch = X_train[start:start+batch_size]\n                y_batch = y_train[start:start+batch_size]\n                # forward pass\n                y_pred = model(X_batch)\n                loss = loss_fn(y_pred, y_batch)\n                # backward pass\n                optimizer.zero_grad()\n                loss.backward()\n                # update weights\n                optimizer.step()\n                # print progress\n                acc = (y_pred.round() == y_batch).float().mean()\n                bar.set_postfix(\n                    loss=float(loss),\n                    acc=float(acc)\n                )\n    # evaluate accuracy at end of training\n    y_pred = model(X_test)\n    acc = (y_pred.round() == y_test).float().mean()\n    return float(acc)\n\n# define 5-fold cross validation test harness\nkfold = StratifiedKFold(n_splits=5, shuffle=True)\ncv_scores = []\nfor train, test in kfold.split(X, y):\n    # create model, train, and get accuracy\n    acc = model_train(X[train], y[train], X[test], y[test])\n    print(\"Accuracy: %.2f\" % acc)\n    cv_scores.append(acc)\n# evaluate the model\nprint(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores)*100, np.std(cv_scores)*100))\n```"]