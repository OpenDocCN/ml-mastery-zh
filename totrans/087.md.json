["```py\n! wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/shampoo.csv\n```", "```py\nfile = open(\"sample_data/mnist_test.csv\")\n```", "```py\n!cat /proc/cpuinfo\n```", "```py\nprocessor\t: 0\nvendor_id\t: GenuineIntel\ncpu family\t: 6\nmodel\t\t: 63\nmodel name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\nstepping\t: 0\nmicrocode\t: 0x1\ncpu MHz\t: 2299.998\ncache size\t: 46080 KB\n…\n\nprocessor\t: 1\nvendor_id\t: GenuineIntel\ncpu family\t: 6\nmodel\t\t: 63\nmodel name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\nstepping\t: 0\nmicrocode\t: 0x1\ncpu MHz\t: 2299.998\ncache size\t: 46080 KB\n…\n```", "```py\n!nvidia-smi\n```", "```py\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   57C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+                                                                         \n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n```", "```py\nimport os\nfrom google.colab import drive\n\nMOUNTPOINT = \"/content/gdrive\"\nDATADIR = os.path.join(MOUNTPOINT, \"MyDrive\")\ndrive.mount(MOUNTPOINT)\n```", "```py\n...\n# writes directly to google drive\nwith open(f\"{DATADIR}/test.txt\", \"w\") as outfile:\n    outfile.write(\"Hello World!\")\n```", "```py\n...\nwith open(f\"{DATADIR}/test.txt\", \"r\") as infile:\n    file_data = infile.read()\n    print(file_data)\n```", "```py\nHello World!\n```", "```py\n%load_ext google.colab.data_table\n```", "```py\nfrom sklearn.datasets import fetch_openml\nX = fetch_openml(\"diabetes\", version=1, as_frame=True, return_X_y=False)[\"frame\"]\nX\n```", "```py\n%unload_ext google.colab.data_table\n```", "```py\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.layers import Input, Dense, Conv2D, Flatten, MaxPool2D\nfrom keras.models import Model\n\nclass LeNet5(tf.keras.Model):\n  def __init__(self):\n    super(LeNet5, self).__init__()\n    #creating layers in initializer\n    self.conv1 = Conv2D(filters=6, kernel_size=(5,5), padding=\"same\", activation=\"relu\")\n    self.max_pool2x2 = MaxPool2D(pool_size=(2,2))\n    self.conv2 = Conv2D(filters=16, kernel_size=(5,5), padding=\"same\", activation=\"relu\")\n    self.flatten = Flatten()\n    self.fc1 = Dense(units=120, activation=\"relu\")\n    self.fc2 = Dense(units=84, activation=\"relu\")\n    self.fc3=Dense(units=10, activation=\"softmax\")\n  def call(self, input_tensor):\n    conv1 = self.conv1(input_tensor)\n    maxpool1 = self.max_pool2x2(conv1)\n    conv2 = self.conv2(maxpool1)\n    maxpool2 = self.max_pool2x2(conv2)\n    flatten = self.flatten(maxpool2)\n    fc1 = self.fc1(flatten)\n    fc2 = self.fc2(fc1)\n    fc3 = self.fc3(fc2)\n    return fc3\n```", "```py\nimport os\nfrom google.colab import drive\nMOUNTPOINT = \"/content/gdrive\"\nDATADIR = os.path.join(MOUNTPOINT, \"MyDrive\")\ndrive.mount(MOUNTPOINT)\n```", "```py\nimport tensorflow as tf\n\ncheckpoint_path = DATADIR + \"/checkpoints/cp-epoch-{epoch}.ckpt\"\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)\n```", "```py\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.layers import Input, Dense, Conv2D, Flatten, MaxPool2D\nfrom keras.models import Model\n\nmnist_digits = keras.datasets.mnist\n(train_images, train_labels), (test_images, test_labels) = mnist_digits.load_data()\n\ninput_layer = Input(shape=(28,28,1))\nmodel = LeNet5()(input_layer)\nmodel = Model(inputs=input_layer, outputs=model)\nmodel.compile(optimizer=\"adam\", loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=\"acc\")\nmodel.fit(x=train_images, y=train_labels, batch_size=256, validation_data = [test_images, test_labels], epochs=5, callbacks=[cp_callback])\n```", "```py\nEpoch 1/5\n235/235 [==============================] - ETA: 0s - loss: 0.9580 - acc: 0.8367\nEpoch 1: saving model to /content/gdrive/MyDrive/checkpoints/cp-epoch-1.ckpt\n235/235 [==============================] - 11s 7ms/step - loss: 0.9580 - acc: 0.8367 - val_loss: 0.1672 - val_acc: 0.9492\nEpoch 2/5\n229/235 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9605\nEpoch 2: saving model to /content/gdrive/MyDrive/checkpoints/cp-epoch-2.ckpt\n235/235 [==============================] - 1s 5ms/step - loss: 0.1298 - acc: 0.9607 - val_loss: 0.0951 - val_acc: 0.9707\nEpoch 3/5\n234/235 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9746\nEpoch 3: saving model to /content/gdrive/MyDrive/checkpoints/cp-epoch-3.ckpt\n235/235 [==============================] - 1s 6ms/step - loss: 0.0811 - acc: 0.9746 - val_loss: 0.0800 - val_acc: 0.9749\nEpoch 4/5\n230/235 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9818\nEpoch 4: saving model to /content/gdrive/MyDrive/checkpoints/cp-epoch-4.ckpt\n235/235 [==============================] - 1s 6ms/step - loss: 0.0580 - acc: 0.9819 - val_loss: 0.0653 - val_acc: 0.9806\nEpoch 5/5\n222/235 [===========================>..] - ETA: 0s - loss: 0.0446 - acc: 0.9858\nEpoch 5: saving model to /content/gdrive/MyDrive/checkpoints/cp-epoch-5.ckpt\n235/235 [==============================] - 1s 6ms/step - loss: 0.0445 - acc: 0.9859 - val_loss: 0.0583 - val_acc: 0.9825\n```", "```py\nimport os\nfrom google.colab import drive\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.layers import Input, Dense, Conv2D, Flatten, MaxPool2D\nfrom keras.models import Model\n\nMOUNTPOINT = \"/content/gdrive\"\nDATADIR = os.path.join(MOUNTPOINT, \"MyDrive\")\ndrive.mount(MOUNTPOINT)\n\nclass LeNet5(tf.keras.Model):\n  def __init__(self):\n    super(LeNet5, self).__init__()\n    self.conv1 = Conv2D(filters=6, kernel_size=(5,5), padding=\"same\", activation=\"relu\")\n    self.max_pool2x2 = MaxPool2D(pool_size=(2,2))\n    self.conv2 = Conv2D(filters=16, kernel_size=(5,5), padding=\"same\", activation=\"relu\")\n    self.flatten = Flatten()\n    self.fc1 = Dense(units=120, activation=\"relu\")\n    self.fc2 = Dense(units=84, activation=\"relu\")\n    self.fc3=Dense(units=10, activation=\"softmax\")\n  def call(self, input_tensor):\n    conv1 = self.conv1(input_tensor)\n    maxpool1 = self.max_pool2x2(conv1)\n    conv2 = self.conv2(maxpool1)\n    maxpool2 = self.max_pool2x2(conv2)\n    flatten = self.flatten(maxpool2)\n    fc1 = self.fc1(flatten)\n    fc2 = self.fc2(fc1)\n    fc3 = self.fc3(fc2)\n    return fc3\n\nmnist_digits = keras.datasets.mnist\n(train_images, train_labels), (test_images, test_labels) = mnist_digits.load_data()\n\n# saving checkpoints\ncheckpoint_path = DATADIR + \"/checkpoints/cp-epoch-{epoch}.ckpt\"\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)\ninput_layer = Input(shape=(28,28,1))\nmodel = LeNet5()(input_layer)\nmodel = Model(inputs=input_layer, outputs=model)\nmodel.compile(optimizer=\"adam\", loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=\"acc\")\n\nmodel.fit(x=train_images, y=train_labels, batch_size=256, validation_data = [test_images, test_labels], \n          epochs=5, callbacks=[cp_callback])\n```", "```py\ncheckpoint_path = DATADIR + \"/checkpoints/cp-epoch-{epoch}.ckpt\"\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)\n\ninput_layer = Input(shape=(28,28,1))\nmodel = LeNet5()(input_layer)\nmodel = Model(inputs=input_layer, outputs=model)\nmodel.compile(optimizer=\"adam\", loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=\"acc\")\n\n# to resume from epoch 5 checkpoints\nmodel.load_weights(DATADIR + \"/checkpoints/cp-epoch-5.ckpt\")\n\n# continue training\nmodel.fit(x=train_images, y=train_labels, batch_size=256, validation_data = [test_images, test_labels], \n          epochs=5, callbacks=[cp_callback])\n```"]