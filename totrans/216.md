# 拉格朗日乘子法：支持向量机理论（第二部分：不可分离情况）

> [`machinelearningmastery.com/method-of-lagrange-multipliers-the-theory-behind-support-vector-machines-part-2-the-non-separable-case/`](https://machinelearningmastery.com/method-of-lagrange-multipliers-the-theory-behind-support-vector-machines-part-2-the-non-separable-case/)

本教程是[拉格朗日乘子法：支持向量机理论（第一部分：可分离情况）](https://machinelearningmastery.com/method-of-lagrange-multipliers-the-theory-behind-support-vector-machines-part-1-the-separable-case))的扩展，讲解了不可分离情况。在实际问题中，正负训练样本可能无法完全通过线性决策边界分开。本教程解释了如何构建容忍一定错误的软间隔。

在本教程中，我们将介绍线性 SVM 的基础知识。我们不会深入探讨使用核技巧推导出的非线性 SVM。内容足以理解 SVM 分类器背后的基本数学模型。

完成本教程后，你将知道：

+   软间隔的概念

+   如何在允许分类错误的情况下最大化间隔

+   如何制定优化问题并计算拉格朗日对偶

让我们开始吧。

![](https://machinelearningmastery.com/wp-content/uploads/2021/12/shakeel-ahmad-Z_MWEx6MgHI-unsplash-scaled.jpg)

拉格朗日乘子法：支持向量机理论（第二部分：不可分离情况）。

摄影师：Shakeel Ahmad，保留所有权利。

## 教程概述

本教程分为两部分；它们是：

1.  SVM 问题在正负样本不可线性分离的情况下的解决方案

    1.  分离超平面及其对应的放松约束

    1.  寻找软间隔的二次优化问题

1.  一个实例

## 先决条件

本教程假设你已经熟悉以下主题。你可以点击各个链接以获取更多信息。

+   [优化/数学编程简明介绍](https://machinelearningmastery.com/a-gentle-introduction-to-optimization-mathematical-programming/)

+   [拉格朗日乘子法简明介绍](https://machinelearningmastery.com/a-gentle-introduction-to-method-of-lagrange-multipliers/)

+   [拉格朗日乘子法与不等式约束](https://machinelearningmastery.com/lagrange-multiplier-approach-with-inequality-constraints/)

+   [拉格朗日乘子法：支持向量机理论（第一部分：可分离情况）](https://machinelearningmastery.com/method-of-lagrange-multipliers-the-theory-behind-support-vector-machines-part-1-the-separable-case))

## 本教程中使用的符号

这是第一部分的续集，因此将使用相同的符号。

+   $m$: 总训练点数

+   $x$: 数据点，是一个$n$维向量。每个维度由$j$索引。

+   $x^+$: 正例

+   $x^-$: 负例

+   $i$: 用于索引训练点的下标。$0 \leq i < m$

+   $j$: 用于索引数据点维度的下标。$1 \leq j \leq n$

+   $t$: 数据点的标签。它是$m$维向量

+   $T$: 转置操作符

+   $w$: 权重向量，表示超平面的系数。它是$n$维向量

+   $\alpha$: 拉格朗日乘子向量，是$m$维向量

+   $\mu$: 拉格朗日乘子向量，同样是$m$维向量

+   $\xi$: 分类中的错误。一个$m$维向量

### 想要开始学习机器学习中的微积分吗？

现在获取我的 7 天免费邮件速成课程（包含示例代码）。

点击注册，还可以获得课程的免费 PDF 电子书版本。

## 分隔超平面与放宽约束

让我们找出正例和负例之间的分隔超平面。回顾一下，分隔超平面由以下表达式给出，其中$w_j$为系数，$w_0$为任意常数，决定了超平面距离原点的距离：

$$

w^T x_i + w_0 = 0

$$

由于我们允许正例和负例位于超平面的错误侧面，我们有了一组放宽的约束。对于正例，我们定义$\xi_i \geq 0, \forall i$。

$$

w^T x_i^+ + w_0 \geq 1 – \xi_i

$$

对于负例，我们也要求：

$$

w^T x_i^- + w_0 \leq -1 + \xi_i

$$

通过使用类标签$t_i \in \{-1,+1\}$结合上述两个约束，我们得到所有点的以下约束：

$$

$t_i(w^T x_i + w_0) \geq 1 – \xi_i

$$

变量$\xi$为我们的模型提供了更多的灵活性。它有以下解释：

1.  $\xi_i =0$: 这意味着$x_i$被正确分类，这个数据点在超平面的正确侧面并远离边距。

1.  $0 < \xi_i < 1$: 当满足此条件时，$x_i$ 位于超平面的正确侧面但在边距内。

1.  $\xi_i > 0$: 满足此条件意味着$x_i$被错误分类。

因此，$\xi$量化了训练点分类中的错误。我们可以定义软错误为：

$$

E_{soft} = \sum_i \xi_i

$$

## 二次规划问题

我们现在可以制定目标函数以及其约束。我们仍然想要最大化边距，即我们希望最小化权重向量的范数。同时，我们还希望将软错误保持在尽可能小的范围内。因此，我们的新目标函数由以下表达式给出，其中$C$是用户定义的常数，代表惩罚因子或正则化常数。

$$

\frac{1}{2}||w||² + C \sum_i \xi_i

$$

因此，总的二次规划问题由以下表达式给出：

$$

\min_w \frac{1}{2}||w||² + C \sum_i \xi_i \;\text{ 使得 } t_i(w^Tx_i+w_0) \geq +1 – \xi_i, \forall i \; \text{ 以及 } \xi_i \geq 0, \forall i

$$

### C 的角色，正则化常数

要理解惩罚因子 $C$，考虑要最小化的乘积项 $C \sum_i \xi_i$。如果 $C$ 较大，那么软间隔 $\sum_i \xi_i$ 就会自动较小。如果 $C$ 接近零，则允许软间隔变大，从而使整体乘积变小。

简而言之，$C$ 值较大意味着我们对错误有很高的惩罚，因此我们的模型不能犯太多错误。$C$ 值较小则允许错误增加。

## 拉格朗日乘数法解法

让我们使用拉格朗日乘数法来解决我们之前制定的二次规划问题。拉格朗日函数如下所示：

$$

L(w, w_0, \alpha, \mu) = \frac{1}{2}||w||² + \sum_i \alpha_i\big(t_i(w^Tx_i+w_0) – 1 + \xi_i\big) – \sum_i \mu_i \xi_i

$$

要解决上述问题，我们设定以下内容：

\begin{equation}

\frac{\partial L}{ \partial w} = 0, \\

\frac{\partial L}{ \partial \alpha} = 0, \\

\frac{\partial L}{ \partial w_0} = 0, \\

\frac{\partial L}{ \partial \mu} = 0 \\

\end{equation}

解上述方程给出：

$$

w = \sum_i \alpha_i t_i x_i

$$

和

$$

0= C – \alpha_i – \mu_i

$$

将以上内容代入拉格朗日函数给出以下优化问题，也称为对偶问题：

$$

L_d = -\frac{1}{2} \sum_i \sum_k \alpha_i \alpha_k t_i t_k (x_i)^T (x_k) + \sum_i \alpha_i

$$

我们需要在以下约束条件下进行最大化：

\begin{equation}

\sum_i \alpha_i t_i = 0 \\ \text{ 并且 }

0 \leq \alpha_i \leq C, \forall i

\end{equation}

类似于[可分离情况](https://machinelearningmastery.com/method-of-lagrange-multipliers-the-theory-behind-support-vector-machines-part-1-the-separable-case)，我们可以用拉格朗日乘数表示 $w$。目标函数中没有 $w$ 项。每个数据点都有一个与之相关的拉格朗日乘数 $\alpha$ 和 $\mu$。

## 数学模型的解释和 $w_0$ 的计算

每个训练数据点 $x_i$ 都满足以下情况：

+   $\alpha_i = 0$: 第 i 个训练点位于超平面的正确一侧，远离边界。该点在测试点的分类中不起作用。

+   $0 < \alpha_i < C$: 第 i 个训练点是支持向量，位于边界上。对于此点 $\xi_i = 0$，且 $t_i(w^T x_i + w_0) = 1$，因此可以用于计算 $w_0$。实际上，$w_0$ 是从所有支持向量计算出来并取平均值。

+   $\alpha_i = C$: 第 i 个训练点要么位于超平面内边界的正确一侧，要么位于超平面错误的一侧。

下图将帮助您理解以上概念：

![](https://machinelearningmastery.com/wp-content/uploads/2021/12/soft1.png)

## 决定测试点的分类

任何测试点$x$的分类可以使用以下表达式来确定：

$$

y(x) = \sum_i \alpha_i t_i x^T x_i + w_0

$$

$y(x)$的正值表示$x\in+1$，负值表示$x\in-1$。因此，测试点的预测类别是$y(x)$的符号。

## Karush-Kuhn-Tucker 条件

上述受限优化问题满足 Karush-Kuhn-Tucker（KKT）条件，如下所示：

\begin{eqnarray}

\alpha_i &\geq& 0 \\

t_i y(x_i) -1 + \xi_i &\geq& 0 \\

\alpha_i(t_i y(x_i) -1 + \xi_i) &=& 0 \\

\mu_i \geq 0 \\

\xi_i \geq 0 \\

\mu_i\xi_i = 0

\end{eqnarray}

## 一个已解决的例子

![](https://machinelearningmastery.com/wp-content/uploads/2021/12/soft2.png)

上图展示了一个为二维训练点解决的例子，以阐明所有概念。需要注意该解决方案的一些事项如下：

+   训练数据点及其对应的标签作为输入。

+   用户定义的常量$C$被设置为 10。

+   该解决方案满足所有约束条件，然而，它不是**最优解决方案**。

+   我们必须确保所有的$\alpha$介于 0 和 C 之间。

+   所有负例的 alpha 之和应等于所有正例的 alpha 之和。

+   点（1,2）、（2,1）和（-2,-2）位于软间隔的超平面正确侧。它们的值被任意设置为 3、3 和 6，以平衡问题并满足约束条件。

+   $\alpha=C=10$的点位于间隔内或在超平面的错误侧。

## 进一步阅读

如果你希望深入了解此主题，本节提供了更多资源。

### 图书

+   [模式识别与机器学习](https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738) 由 Christopher M. Bishop 编写。

### 文章

+   [机器学习中的支持向量机](https://machinelearningmastery.com/support-vector-machines-for-machine-learning/)

+   [关于模式识别的支持向量机教程](https://www.di.ens.fr/~mallat/papiers/svmtutorial.pdf) 由 Christopher J.C. Burges 编写。

## 总结

在本教程中，你了解了使用拉格朗日乘子法找到 SVM 分类器中软间隔的方法。

具体来说，你学到了：

+   如何为不可分情况制定优化问题。

+   如何使用拉格朗日乘子法找到超平面和软间隔。

+   如何找到非常简单问题的分隔超平面方程。

对于这篇文章中讨论的 SVM，你有任何问题吗？请在下方评论中提出你的问题，我会尽力回答。
