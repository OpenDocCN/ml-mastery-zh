- en: Plotting the Training and Validation Loss Curves for the Transformer Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制 Transformer 模型的训练和验证损失曲线
- en: 原文：[https://machinelearningmastery.com/plotting-the-training-and-validation-loss-curves-for-the-transformer-model/](https://machinelearningmastery.com/plotting-the-training-and-validation-loss-curves-for-the-transformer-model/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/plotting-the-training-and-validation-loss-curves-for-the-transformer-model/](https://machinelearningmastery.com/plotting-the-training-and-validation-loss-curves-for-the-transformer-model/)
- en: We have previously seen how to train the Transformer model for neural machine
    translation. Before moving on to inferencing the trained model, let us first explore
    how to modify the training code slightly to be able to plot the training and validation
    loss curves that can be generated during the learning process.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经看到如何训练 Transformer 模型用于神经机器翻译。在进行训练模型的推断之前，让我们首先探索如何稍微修改训练代码，以便能够绘制在学习过程中生成的训练和验证损失曲线。
- en: The training and validation loss values provide important information because
    they give us a better insight into how the learning performance changes over the
    number of epochs and help us diagnose any problems with learning that can lead
    to an underfit or an overfit model. They will also inform us about the epoch with
    which to use the trained model weights at the inferencing stage.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和验证损失值提供了重要信息，因为它们让我们更好地了解学习性能如何随轮次的变化而变化，并帮助我们诊断任何可能导致模型欠拟合或过拟合的问题。它们还将告诉我们在推断阶段使用训练好的模型权重的轮次。
- en: In this tutorial, you will discover how to plot the training and validation
    loss curves for the Transformer model.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你将学习如何绘制 Transformer 模型的训练和验证损失曲线。
- en: 'After completing this tutorial, you will know:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本教程后，你将了解：
- en: How to modify the training code to include validation and test splits, in addition
    to a training split of the dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何修改训练代码以包括验证和测试划分，除了数据集的训练划分
- en: How to modify the training code to store the computed training and validation
    loss values, as well as the trained model weights
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何修改训练代码以存储计算出的训练和验证损失值，以及训练好的模型权重
- en: How to plot the saved training and validation loss curves
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何绘制保存的训练和验证损失曲线
- en: '**Kick-start your project** with my book [Building Transformer Models with
    Attention](https://machinelearningmastery.com/transformer-models-with-attention/).
    It provides **self-study tutorials** with **working code** to guide you into building
    a fully-working transformer model that can'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过我的书** [《构建具有注意力机制的 Transformer 模型》](https://machinelearningmastery.com/transformer-models-with-attention/)
    **启动你的项目**。它提供了 **自学教程** 和 **可用代码** 来指导你构建一个完全可用的 Transformer 模型。'
- en: '*translate sentences from one language to another*...'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*将句子从一种语言翻译成另一种语言*...'
- en: Let’s get started.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: '[![](../Images/8597482be644bf2982b6f4c90e8493e3.png)](https://machinelearningmastery.com/wp-content/uploads/2022/10/training_validation_loss_cover.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/8597482be644bf2982b6f4c90e8493e3.png)](https://machinelearningmastery.com/wp-content/uploads/2022/10/training_validation_loss_cover.jpg)'
- en: Plotting the training and validation loss curves for the Transformer model
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制 Transformer 模型的训练和验证损失曲线
- en: Photo by [Jack Anstey](https://unsplash.com/photos/zS4lUqLEiNA), some rights
    reserved.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Jack Anstey](https://unsplash.com/photos/zS4lUqLEiNA) 提供，部分权利保留。
- en: '**Tutorial Overview**'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**教程概述**'
- en: 'This tutorial is divided into four parts; they are:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程分为四部分，它们是：
- en: Recap of the Transformer Architecture
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer 架构回顾
- en: Preparing the Training, Validation, and Testing Splits of the Dataset
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据集的训练、验证和测试划分
- en: Training the Transformer Model
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练 Transformer 模型
- en: Plotting the Training and Validation Loss Curves
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制训练和验证损失曲线
- en: '**Prerequisites**'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**先决条件**'
- en: 'For this tutorial, we assume that you are already familiar with:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本教程，我们假设你已经熟悉：
- en: '[The theory behind the Transformer model](https://machinelearningmastery.com/the-transformer-model/)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Transformer 模型背后的理论](https://machinelearningmastery.com/the-transformer-model/)'
- en: '[An implementation of the Transformer model](https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Transformer 模型的实现](https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/)'
- en: '[Training the Transformer model](https://machinelearningmastery.com/training-the-transformer-model/)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[训练 Transformer 模型](https://machinelearningmastery.com/training-the-transformer-model/)'
- en: '**Recap of the Transformer Architecture**'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Transformer 架构回顾**'
- en: '[Recall](https://machinelearningmastery.com/the-transformer-model/) having
    seen that the Transformer architecture follows an encoder-decoder structure. The
    encoder, on the left-hand side, is tasked with mapping an input sequence to a
    sequence of continuous representations; the decoder, on the right-hand side, receives
    the output of the encoder together with the decoder output at the previous time
    step to generate an output sequence.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[回忆](https://machinelearningmastery.com/the-transformer-model/)你已经看到Transformer架构遵循编码器-解码器结构。左侧的编码器负责将输入序列映射到一系列连续表示；右侧的解码器接收编码器的输出以及前一个时间步的解码器输出，以生成输出序列。'
- en: '[![](../Images/5cc2fa00063bfd70298252dce57dbdcd.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/5cc2fa00063bfd70298252dce57dbdcd.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png)'
- en: The encoder-decoder structure of the Transformer architecture
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构的编码器-解码器结构
- en: Taken from “[Attention Is All You Need](https://arxiv.org/abs/1706.03762)“
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 取自“[Attention Is All You Need](https://arxiv.org/abs/1706.03762)“
- en: In generating an output sequence, the Transformer does not rely on recurrence
    and convolutions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成输出序列时，Transformer不依赖于递归和卷积。
- en: You have seen how to train the complete Transformer model, and you shall now
    see how to generate and plot the training and validation loss values that will
    help you diagnose the model’s learning performance.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到如何训练完整的Transformer模型，现在你将看到如何生成和绘制训练和验证损失值，这将帮助你诊断模型的学习性能。
- en: Want to Get Started With Building Transformer Models with Attention?
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 想要开始构建带有注意力机制的Transformer模型吗？
- en: Take my free 12-day email crash course now (with sample code).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在就参加我的免费12天电子邮件速成课程（包含示例代码）。
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 点击注册，还能获得课程的免费PDF电子书版本。
- en: '**Preparing the Training, Validation, and Testing Splits of the Dataset**'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**准备数据集的训练、验证和测试拆分**'
- en: 'In order to be able to include validation and test splits of the data, you
    will modify the code that [prepares the dataset](https://machinelearningmastery.com/?p=13585&preview=true)
    by introducing the following lines of code, which:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够包括数据的验证和测试拆分，你将通过引入以下代码行来修改[准备数据集的代码](https://machinelearningmastery.com/?p=13585&preview=true)，这些代码行：
- en: 'Specify the size of the validation data split. This, in turn, determines the
    size of the training and test splits of the data, which we will be dividing into
    a ratio of 80:10:10 for the training, validation, and test sets, respectively:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定验证数据拆分的大小。这反过来决定了训练数据和测试数据的大小，我们将把数据分成80:10:10的比例，分别用于训练集、验证集和测试集：
- en: Python
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Split the dataset into validation and test sets in addition to the training
    set:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了训练集外，将数据集拆分为验证集和测试集：
- en: Python
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Prepare the validation data by tokenizing, padding, and converting to a tensor.
    For this purpose, you will collect these operations into a function called `encode_pad`,
    as shown in the complete code listing below. This will avoid excessive repetition
    of code when performing these operations on the training data as well:'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过标记化、填充和转换为张量来准备验证数据。为此，你将把这些操作收集到一个名为`encode_pad`的函数中，如下面的完整代码列表所示。这将避免在对训练数据进行这些操作时代码的过度重复：
- en: Python
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Save the encoder and decoder tokenizers into pickle files and the test dataset
    into a text file to be used later during the inferencing stage:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将编码器和解码器的标记化器保存到pickle文件中，并将测试数据集保存到一个文本文件中，以便在推断阶段使用：
- en: Python
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The complete code listing is now updated as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码列表现已更新如下：
- en: Python
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Training the Transformer Model**'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**训练Transformer模型**'
- en: 'We shall introduce similar modifications to the code that [trains the Transformer
    model](https://machinelearningmastery.com/?p=13585&preview=true) to:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对[训练Transformer模型的代码](https://machinelearningmastery.com/?p=13585&preview=true)进行类似的修改，以：
- en: 'Prepare the validation dataset batches:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备验证数据集的批次：
- en: Python
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Monitor the validation loss metric:'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控验证损失指标：
- en: Python
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Initialize dictionaries to store the training and validation losses and eventually
    store the loss values in the respective dictionaries:'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化字典以存储训练和验证的损失，并最终将损失值存储在相应的字典中：
- en: Python
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Compute the validation loss:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算验证损失：
- en: Python
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Save the trained model weights at every epoch. You will use these at the inferencing
    stage to investigate the differences in results that the model produces at different
    epochs. In practice, it would be more efficient to include a callback method that
    halts the training process based on the metrics that are being monitored during
    training and only then save the model weights:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个周期保存训练的模型权重。你将在推理阶段使用这些权重来调查模型在不同周期产生的结果差异。在实践中，更高效的做法是包含一个回调方法，该方法根据训练过程中监控的指标停止训练过程，并在此时保存模型权重：
- en: Python
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, save the training and validation loss values into pickle files:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，将训练和验证损失值保存到pickle文件中：
- en: Python
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The modified code listing now becomes:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的代码列表现在变为：
- en: Python
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Plotting the Training and Validation Loss Curves**'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**绘制训练和验证损失曲线**'
- en: In order to be able to plot the training and validation loss curves, you will
    first load the pickle files containing the training and validation loss dictionaries
    that you saved when training the Transformer model earlier.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够绘制训练和验证损失曲线，你首先需要加载包含训练和验证损失字典的pickle文件，这些文件是你在早期训练Transformer模型时保存的。
- en: Then you will retrieve the training and validation loss values from the respective
    dictionaries and graph them on the same plot.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你将从各自的字典中检索训练和验证损失值，并在同一图上绘制它们。
- en: 'The code listing is as follows, which you should save into a separate Python
    script:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 代码列表如下，你应该将其保存到一个单独的Python脚本中：
- en: Python
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Running the code above generates a similar plot of the training and validation
    loss curves to the one below:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码会生成类似下面的训练和验证损失曲线图：
- en: '[![](../Images/857677d969f9ea1636d3455ad9447958.png)](https://machinelearningmastery.com/wp-content/uploads/2022/10/training_validation_loss_1.png)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/857677d969f9ea1636d3455ad9447958.png)](https://machinelearningmastery.com/wp-content/uploads/2022/10/training_validation_loss_1.png)'
- en: Line plots of the training and validation loss values over several training
    epochs
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和验证损失值在多个训练周期上的折线图
- en: Note that although you might see similar loss curves, they might not necessarily
    be identical to the ones above. This is because you are training the Transformer
    model from scratch, and the resulting training and validation loss values depend
    on the random initialization of the model weights.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管你可能会看到类似的损失曲线，但它们可能不一定与上面的一模一样。这是因为你从头开始训练Transformer模型，结果的训练和验证损失值取决于模型权重的随机初始化。
- en: Nonetheless, these loss curves give us a better insight into how the learning
    performance changes over the number of epochs and help us diagnose any problems
    with learning that can lead to an underfit or an overfit model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，这些损失曲线为我们提供了更好的洞察力，了解学习性能如何随训练周期数变化，并帮助我们诊断可能导致欠拟合或过拟合模型的学习问题。
- en: For more details on using the training and validation loss curves to diagnose
    the learning performance of a model, you can refer to [this tutorial](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/)
    by Jason Brownlee.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何使用训练和验证损失曲线来诊断模型的学习表现，您可以参考Jason Brownlee的[这篇教程](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/)。
- en: '**Further Reading**'
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: This section provides more resources on the topic if you are looking to go deeper.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了更多资源，如果你希望深入了解这个话题。
- en: '**Books**'
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**书籍**'
- en: '[Advanced Deep Learning with Python](https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X),
    2019'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python深度学习进阶](https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X)，2019年'
- en: '[Transformers for Natural Language Processing](https://www.amazon.com/Transformers-Natural-Language-Processing-architectures/dp/1800565798),
    2021'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于自然语言处理的Transformers](https://www.amazon.com/Transformers-Natural-Language-Processing-architectures/dp/1800565798)，2021年'
- en: '**Papers**'
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**文献**'
- en: '[Attention Is All You Need](https://arxiv.org/abs/1706.03762), 2017'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Attention Is All You Need](https://arxiv.org/abs/1706.03762)，2017年'
- en: '**Websites**'
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**网站**'
- en: How to use Learning Curves to Diagnose Machine Learning Model Performance, [https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/)
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用学习曲线诊断机器学习模型性能，[https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/)
- en: '**Summary**'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this tutorial, you discovered how to plot the training and validation loss
    curves for the Transformer model.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您学习了如何绘制Transformer模型的训练和验证损失曲线。
- en: 'Specifically, you learned:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，您学到了：
- en: How to modify the training code to include validation and test splits, in addition
    to a training split of the dataset
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何修改训练代码以包括验证集和测试集分割，除了数据集的训练分割。
- en: How to modify the training code to store the computed training and validation
    loss values, as well as the trained model weights
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何修改训练代码以存储计算的训练和验证损失值，以及训练好的模型权重。
- en: How to plot the saved training and validation loss curves
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何绘制保存的训练和验证损失曲线。
- en: Do you have any questions?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你有任何问题吗？
- en: Ask your questions in the comments below, and I will do my best to answer.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的评论中提出你的问题，我会尽力回答。
