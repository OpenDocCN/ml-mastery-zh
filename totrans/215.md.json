["```py\nimport numpy as np    \n# For optimization\nfrom scipy.optimize import Bounds, BFGS                     \nfrom scipy.optimize import LinearConstraint, minimize   \n# For plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# For generating dataset\nimport sklearn.datasets as dt\n```", "```py\nZERO = 1e-7\n```", "```py\ndat = np.array([[0, 3], [-1, 0], [1, 2], [2, 1], [3,3], [0, 0], [-1, -1], [-3, 1], [3, 1]])\nlabels = np.array([1, 1, 1, 1, 1, -1, -1, -1, -1])\n\ndef plot_x(x, t, alpha=[], C=0):\n    sns.scatterplot(dat[:,0], dat[:, 1], style=labels,\n    hue=labels, markers=['s', 'P'],\n    palette=['magenta', 'green'])\n    if len(alpha) > 0:\n        alpha_str = np.char.mod('%.1f', np.round(alpha, 1))\n        ind_sv = np.where(alpha > ZERO)[0]\n        for i in ind_sv:   \n            plt.gca().text(dat[i,0], dat[i, 1]-.25, alpha_str[i] )\n\nplot_x(dat, labels)\n```", "```py\n# Objective function\ndef lagrange_dual(alpha, x, t):\n    result = 0\n    ind_sv = np.where(alpha > ZERO)[0]\n    for i in ind_sv:\n        for k in ind_sv:\n            result = result + alpha[i]*alpha[k]*t[i]*t[k]*np.dot(x[i, :], x[k, :]) \n    result = 0.5*result - sum(alpha)     \n    return result\n```", "```py\nlinear_constraint = LinearConstraint(labels, [0], [0])\nprint(linear_constraint)\n```", "```py\n<scipy.optimize._constraints.LinearConstraint object at 0x12c87f5b0>\n```", "```py\nbounds_alpha = Bounds(np.zeros(dat.shape[0]), np.full(dat.shape[0], 10))\nprint(bounds_alpha)\n```", "```py\nBounds(array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([10, 10, 10, 10, 10, 10, 10, 10, 10]))\n```", "```py\ndef optimize_alpha(x, t, C):\n    m, n = x.shape\n    np.random.seed(1)\n    # Initialize alphas to random values\n    alpha_0 = np.random.rand(m)*C\n    # Define the constraint\n    linear_constraint = LinearConstraint(t, [0], [0])\n    # Define the bounds\n    bounds_alpha = Bounds(np.zeros(m), np.full(m, C))\n    # Find the optimal value of alpha\n    result = minimize(lagrange_dual, alpha_0, args = (x, t), method='trust-constr', \n                      hess=BFGS(), constraints=[linear_constraint],\n                      bounds=bounds_alpha)\n    # The optimized value of alpha lies in result.x\n    alpha = result.x\n    return alpha\n```", "```py\ndef get_w(alpha, t, x):\n    m = len(x)\n    # Get all support vectors\n    w = np.zeros(x.shape[1])\n    for i in range(m):\n        w = w + alpha[i]*t[i]*x[i, :]        \n    return w\n\ndef get_w0(alpha, t, x, w, C):\n    C_numeric = C-ZERO\n    # Indices of support vectors with alpha<C\n    ind_sv = np.where((alpha > ZERO)&(alpha < C_numeric))[0]\n    w0 = 0.0\n    for s in ind_sv:\n        w0 = w0 + t[s] - np.dot(x[s, :], w)\n    # Take the average    \n    w0 = w0 / len(ind_sv)\n    return w0\n```", "```py\ndef classify_points(x_test, w, w0):\n    # get y(x_test)\n    predicted_labels = np.sum(x_test*w, axis=1) + w0\n    predicted_labels = np.sign(predicted_labels)\n    # Assign a label arbitrarily a +1 if it is zero\n    predicted_labels[predicted_labels==0] = 1\n    return predicted_labels\n\ndef misclassification_rate(labels, predictions):\n    total = len(labels)\n    errors = sum(labels != predictions)\n    return errors/total*100\n```", "```py\ndef plot_hyperplane(w, w0):\n    x_coord = np.array(plt.gca().get_xlim())\n    y_coord = -w0/w[1] - w[0]/w[1] * x_coord\n    plt.plot(x_coord, y_coord, color='red')\n\ndef plot_margin(w, w0):\n    x_coord = np.array(plt.gca().get_xlim())\n    ypos_coord = 1/w[1] - w0/w[1] - w[0]/w[1] * x_coord\n    plt.plot(x_coord, ypos_coord, '--', color='green') \n    yneg_coord = -1/w[1] - w0/w[1] - w[0]/w[1] * x_coord\n    plt.plot(x_coord, yneg_coord, '--', color='magenta')\n```", "```py\ndef display_SVM_result(x, t, C):\n    # Get the alphas\n    alpha = optimize_alpha(x, t, C)   \n    # Get the weights\n    w = get_w(alpha, t, x)\n    w0 = get_w0(alpha, t, x, w, C)\n    plot_x(x, t, alpha, C)\n    xlim = plt.gca().get_xlim()\n    ylim = plt.gca().get_ylim()\n    plot_hyperplane(w, w0)\n    plot_margin(w, w0)\n    plt.xlim(xlim)\n    plt.ylim(ylim)\n    # Get the misclassification error and display it as title\n    predictions = classify_points(x, w, w0)\n    err = misclassification_rate(t, predictions)\n    title = 'C = ' + str(C) + ',  Errors: ' + '{:.1f}'.format(err) + '%'\n    title = title + ',  total SV = ' + str(len(alpha[alpha > ZERO]))\n    plt.title(title)\n\ndisplay_SVM_result(dat, labels, 100)    \nplt.show()\n```", "```py\ndat, labels = dt.make_blobs(n_samples=[20,20],\n                           cluster_std=1,\n                           random_state=0)\nlabels[labels==0] = -1\nplot_x(dat, labels)\n```", "```py\nfig = plt.figure(figsize=(8,25))\n\ni=0\nC_array = [1e-2, 100, 1e5]\n\nfor C in C_array:\n    fig.add_subplot(311+i)    \n    display_SVM_result(dat, labels, C)  \n    i = i + 1\n```", "```py\nimport numpy as np    \n# For optimization\nfrom scipy.optimize import Bounds, BFGS                     \nfrom scipy.optimize import LinearConstraint, minimize   \n# For plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# For generating dataset\nimport sklearn.datasets as dt\n\nZERO = 1e-7\n\ndef plot_x(x, t, alpha=[], C=0):\n    sns.scatterplot(dat[:,0], dat[:, 1], style=labels,\n    hue=labels, markers=['s', 'P'],\n    palette=['magenta', 'green'])\n    if len(alpha) > 0:\n        alpha_str = np.char.mod('%.1f', np.round(alpha, 1))\n        ind_sv = np.where(alpha > ZERO)[0]\n        for i in ind_sv:   \n            plt.gca().text(dat[i,0], dat[i, 1]-.25, alpha_str[i] )\n\n# Objective function\ndef lagrange_dual(alpha, x, t):\n    result = 0\n    ind_sv = np.where(alpha > ZERO)[0]\n    for i in ind_sv:\n        for k in ind_sv:\n            result = result + alpha[i]*alpha[k]*t[i]*t[k]*np.dot(x[i, :], x[k, :]) \n    result = 0.5*result - sum(alpha)     \n    return result \n\ndef optimize_alpha(x, t, C):\n    m, n = x.shape\n    np.random.seed(1)\n    # Initialize alphas to random values\n    alpha_0 = np.random.rand(m)*C\n    # Define the constraint\n    linear_constraint = LinearConstraint(t, [0], [0])\n    # Define the bounds\n    bounds_alpha = Bounds(np.zeros(m), np.full(m, C))\n    # Find the optimal value of alpha\n    result = minimize(lagrange_dual, alpha_0, args = (x, t), method='trust-constr', \n                      hess=BFGS(), constraints=[linear_constraint],\n                      bounds=bounds_alpha)\n    # The optimized value of alpha lies in result.x\n    alpha = result.x\n    return alpha\n\ndef get_w(alpha, t, x):\n    m = len(x)\n    # Get all support vectors\n    w = np.zeros(x.shape[1])\n    for i in range(m):\n        w = w + alpha[i]*t[i]*x[i, :]        \n    return w\n\ndef get_w0(alpha, t, x, w, C):\n    C_numeric = C-ZERO\n    # Indices of support vectors with alpha<C\n    ind_sv = np.where((alpha > ZERO)&(alpha < C_numeric))[0]\n    w0 = 0.0\n    for s in ind_sv:\n        w0 = w0 + t[s] - np.dot(x[s, :], w)\n    # Take the average    \n    w0 = w0 / len(ind_sv)\n    return w0\n\ndef classify_points(x_test, w, w0):\n    # get y(x_test)\n    predicted_labels = np.sum(x_test*w, axis=1) + w0\n    predicted_labels = np.sign(predicted_labels)\n    # Assign a label arbitrarily a +1 if it is zero\n    predicted_labels[predicted_labels==0] = 1\n    return predicted_labels\n\ndef misclassification_rate(labels, predictions):\n    total = len(labels)\n    errors = sum(labels != predictions)\n    return errors/total*100\n\ndef plot_hyperplane(w, w0):\n    x_coord = np.array(plt.gca().get_xlim())\n    y_coord = -w0/w[1] - w[0]/w[1] * x_coord\n    plt.plot(x_coord, y_coord, color='red')\n\ndef plot_margin(w, w0):\n    x_coord = np.array(plt.gca().get_xlim())\n    ypos_coord = 1/w[1] - w0/w[1] - w[0]/w[1] * x_coord\n    plt.plot(x_coord, ypos_coord, '--', color='green') \n    yneg_coord = -1/w[1] - w0/w[1] - w[0]/w[1] * x_coord\n    plt.plot(x_coord, yneg_coord, '--', color='magenta')  \n\ndef display_SVM_result(x, t, C):\n    # Get the alphas\n    alpha = optimize_alpha(x, t, C)   \n    # Get the weights\n    w = get_w(alpha, t, x)\n    w0 = get_w0(alpha, t, x, w, C)\n    plot_x(x, t, alpha, C)\n    xlim = plt.gca().get_xlim()\n    ylim = plt.gca().get_ylim()\n    plot_hyperplane(w, w0)\n    plot_margin(w, w0)\n    plt.xlim(xlim)\n    plt.ylim(ylim)\n    # Get the misclassification error and display it as title\n    predictions = classify_points(x, w, w0)\n    err = misclassification_rate(t, predictions)\n    title = 'C = ' + str(C) + ',  Errors: ' + '{:.1f}'.format(err) + '%'\n    title = title + ',  total SV = ' + str(len(alpha[alpha > ZERO]))\n    plt.title(title)\n\ndat = np.array([[0, 3], [-1, 0], [1, 2], [2, 1], [3,3], [0, 0], [-1, -1], [-3, 1], [3, 1]])\nlabels = np.array([1, 1, 1, 1, 1, -1, -1, -1, -1])                  \nplot_x(dat, labels)\nplt.show()\ndisplay_SVM_result(dat, labels, 100)    \nplt.show()\n\ndat, labels = dt.make_blobs(n_samples=[20,20],\n                           cluster_std=1,\n                           random_state=0)\nlabels[labels==0] = -1\nplot_x(dat, labels)\n\nfig = plt.figure(figsize=(8,25))\n\ni=0\nC_array = [1e-2, 100, 1e5]\n\nfor C in C_array:\n    fig.add_subplot(311+i)    \n    display_SVM_result(dat, labels, C)  \n    i = i + 1\n```"]