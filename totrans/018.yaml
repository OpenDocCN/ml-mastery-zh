- en: Get a Taste of LLMs from GPT4All
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://machinelearningmastery.com/get-a-taste-of-llms-from-gpt4all/](https://machinelearningmastery.com/get-a-taste-of-llms-from-gpt4all/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Large language models have become popular recently. ChatGPT is fashionable.
    Trying out ChatGPT to understand what LLMs are about is easy, but sometimes, you
    may want an offline alternative that can run on your computer. In this post, you
    will learn about GPT4All as an LLM that you can install on your computer. In particular,
    you will learn
  prefs: []
  type: TYPE_NORMAL
- en: What is GPT4All
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to install the desktop client for GPT4All
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to run GPT4All in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Get started and apply ChatGPT** with my book [Maximizing Productivity with
    ChatGPT](https://machinelearningmastery.com/productivity-with-chatgpt/). It provides
    **real-world use cases** and **prompt examples** designed to get you using ChatGPT
    quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.![](../Images/c623826cbb400d3a3cbc609df251a94c.png)
  prefs: []
  type: TYPE_NORMAL
- en: Get a Taste of LLMs from GPT4All
  prefs: []
  type: TYPE_NORMAL
- en: Picture generated by the author using Stable Diffusion. Some rights reserved.
  prefs: []
  type: TYPE_NORMAL
- en: '**Updates:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**2023-10-10**: Refreshed the Python code for gpt4all module version 1.0.12'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This post is divided into three parts; they are:'
  prefs: []
  type: TYPE_NORMAL
- en: What is GPT4All?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to get GPT4All
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use GPT4All in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is GPT4All?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The term “GPT” is derived from the title of a 2018 paper, “Improving Language
    Understanding by Generative Pre-Training” by Radford et al. This paper describes
    how transformer models are demonstrated to be able to understand human language.
  prefs: []
  type: TYPE_NORMAL
- en: Since then, many people attempted to develop language models using transformer
    architecture, and it has been found that a model large enough can give excellent
    results. However, many of the models developed are proprietary. There are either
    provided as a service with paid subscription or under a license with certain restrictive
    terms. Some are even impossible to run on commodity hardware due to is size.
  prefs: []
  type: TYPE_NORMAL
- en: GPT4All project tried to make the LLMs available to the public on common hardware.
    It allows you to train and deploy your model. Pretrained models are also available,
    with a small size that can reasonably run on a CPU.
  prefs: []
  type: TYPE_NORMAL
- en: How to get GPT4All
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s focus only on using the pre-trained models.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, GPT4All is available from [https://gpt4all.io/index.html](https://gpt4all.io/index.html),
    which you can run as a desktop application or using a Python library. You can
    download the installer for your OS to run a desktop client. The client is only
    a few hundred MB. You should see an installation screen as follows:![](../Images/4486a6688a08d8b8b6472c99ea7faaf4.png)
  prefs: []
  type: TYPE_NORMAL
- en: After you have the client installed, launching it the first time will prompt
    you to install a model, which can be as large as many GB. To start, you may pick
    “`gpt4all-j-v1.3-groovy`” (the GPT4All-J model). It is a relatively small but
    popular model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/381d212b3d682cc1fa359bb0873be2e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the client and model are ready, you can type your message in the input
    box. The model may expect a specific form of input, e.g., a particular language
    or style. This model expects a conversation style (like ChatGPT) and generally
    handles English well. For example, below is how it responds to the input “Give
    me a list of 10 colors and their RGB code”:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7aac7ad319c2da868217154cc3202ace.png)'
  prefs: []
  type: TYPE_IMG
- en: How to use GPT4All in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key component of GPT4All is the model. The desktop client is merely an interface
    to it. Besides the client, you can also invoke the model through a Python library.
  prefs: []
  type: TYPE_NORMAL
- en: 'The library is unsurprisingly named “`gpt4all`,” and you can install it with
    `pip` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Note: **This is a fast-moving library and the functions may change. The following
    code has been tested on version 1.0.12 but it may not work in future versions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Afterward, you can use it in Python in just a few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the above code will download the model file if you haven’t yet. Afterward,
    the model is loaded, input is provided, and the response is returned as a string.
    The output printed may be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The chat history of the session is stored in the model’s attribute `current_chat_session`
    as a Python list. An example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The history is a sequence of dialog in the format of Python dictionaries with
    keys `role` and `content`. The `role` can be `"system"`, `"assistant"`, or `"user"`,
    while `content` is a string of text. If you’re chatting with your model like the
    example, your role is `"user"` while the computer’s response is `"assistant"`.
    You can keep using the `generate()` call to continue your conversation. Below
    is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that you invoked the model multiple times in the for-loop. Each time it
    responded, the model took the output and appended it to the list of chat messages
    so you accumulated the context. Then you add a new dialog and invoke the model
    again. This is how the model remember the chat history. Below is an example of
    how the above code respond to your questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, the chat history accumulated by the end of the above code would
    be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You may get a better result from another model. You may also get a different
    result due to the randomness in the model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GPT4All is a nice tool you can play with on your computer. It allows you to
    explore the interaction with a large language model and help you better understand
    the capability and limitation of a model. In this post, you learned that:'
  prefs: []
  type: TYPE_NORMAL
- en: GPT4All has a desktop client that you can install on your computer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT4All has a Python interface that allows you to interact with a language model
    in code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are multiple language model available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
