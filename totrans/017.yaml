- en: What Are Zero-Shot Prompting and Few-Shot Prompting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是零-shot提示和少量示例提示
- en: 原文：[https://machinelearningmastery.com/what-are-zero-shot-prompting-and-few-shot-prompting/](https://machinelearningmastery.com/what-are-zero-shot-prompting-and-few-shot-prompting/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/what-are-zero-shot-prompting-and-few-shot-prompting/](https://machinelearningmastery.com/what-are-zero-shot-prompting-and-few-shot-prompting/)
- en: 'In the literature on language models, you will often encounter the terms “zero-shot
    prompting” and “few-shot prompting.” It is important to understand how a large
    language model generates an output. In this post, you will learn:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言模型的文献中，你会经常遇到“零-shot提示”和“少量示例提示”这两个术语。了解大型语言模型如何生成输出是很重要的。在本文中，你将学习：
- en: What is zero-shot and few-shot prompting?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是零-shot和少量示例提示？
- en: How to experiment with them in GPT4All
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在GPT4All中进行实验
- en: '![](../Images/2ddc864f7b525f85c2e41c227061d6f3.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ddc864f7b525f85c2e41c227061d6f3.png)'
- en: What Are Zero-Shot Prompting and Few-Shot Prompting
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是零-shot提示和少量示例提示
- en: Picture generated by the author using Stable Diffusion. Some rights reserved.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用稳定扩散生成的图片。版权所有。
- en: '**Get started and apply ChatGPT** with my book [Maximizing Productivity with
    ChatGPT](https://machinelearningmastery.com/productivity-with-chatgpt/). It provides
    **real-world use cases** and **prompt examples** designed to get you using ChatGPT
    quickly.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**开始使用并应用ChatGPT**，请参考我的书籍[《最大化ChatGPT生产力》](https://machinelearningmastery.com/productivity-with-chatgpt/)。它提供了**实际应用案例**和**提示示例**，旨在帮助你迅速使用ChatGPT。'
- en: Let’s get started.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Overview
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'This post is divided into three parts; they are:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文分为三部分；它们是：
- en: How Do Large Language Models Generate Output?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型如何生成输出？
- en: Zero-Shot Prompting
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零-shot提示
- en: Few-Shot Prompting
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 少量示例提示
- en: How Do Large Language Models Generate Output?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型如何生成输出？
- en: Large language models were trained with massive amounts of text data. They were
    trained to predict the next word from the input. It is found that, given the model
    is large enough, not only the grammar of human languages can be learned, but also
    the meaning of words, common knowledge, and primitive logic.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型经过大量文本数据的训练。它们被训练来预测输入的下一个单词。研究发现，只要模型足够大，不仅可以学习人类语言的语法，还可以学习单词的意义、常识和原始逻辑。
- en: Therefore, if you give the fragmented sentence “My neighbor’s dog is” to the
    model (as input, also known as **prompt**), it may predict with “smart” or “small”
    but not likely with “sequential,” although all these are adjectives. Similarly,
    if you provide a complete sentence to the model, you can expect a sentence that
    follows naturally from the model’s output. Repeatedly appending the model’s output
    to the original input and invoking the model again can make the model generate
    a lengthy response.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你将断裂的句子“My neighbor’s dog is”作为输入（也称为**提示**）提供给模型，它可能会预测“smart”或“small”，但不太可能是“sequential”，尽管这些都是形容词。同样，如果你向模型提供一个完整的句子，你可以期望模型的输出是自然跟随的句子。反复将模型的输出附加到原始输入上，并再次调用模型，可以让模型生成较长的回应。
- en: Zero-Shot Prompting
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零-shot提示
- en: In natural language processing models, zero-shot prompting means providing a
    prompt that is not part of the training data to the model, but the model can generate
    a result that you desire. This promising technique makes large language models
    useful for many tasks.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理模型中，零-shot提示意味着向模型提供一个不属于训练数据的提示，但模型可以生成你所期望的结果。这种有前景的技术使大型语言模型在许多任务中变得有用。
- en: 'To understand why this is useful, imagine the case of sentiment analysis: You
    can take paragraphs of different opinions and label them with a sentiment classification.
    Then you can train a machine learning model (e.g., RNN on text data) to take a
    paragraph as input and generate classification as output. But you would find that
    such a model is not adaptive. If you add a new class to the classification or
    ask not to classify the paragraph but summarize them, this model must be modified
    and retrained.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这为何有用，想象一下情感分析的情况：你可以将不同意见的段落进行标记，然后用情感分类对其进行标注。然后，你可以训练一个机器学习模型（例如，对文本数据进行RNN训练），将段落作为输入，生成分类作为输出。但你会发现这样的模型并不适应。如果你为分类添加了新的类别，或要求不是对段落进行分类而是总结，这个模型必须进行修改和重新训练。
- en: 'A large language model, however, needs not to be retrained. You can ask the
    model to classify a paragraph or summarize it if you know how to ask correctly.
    This means the model probably cannot classify a paragraph into categories A or
    B since the meaning of “A” and “B” are unclear. Still, it can classify into “positive
    sentiment” or “negative sentiment” since the model knows what should “positive”
    and “negative” be. This works because, during the training, the model learned
    the meaning of these words and acquired the ability to follow simple instructions.
    An example is the following, demonstrated using GPT4All with the model Vicuna-7B:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大型语言模型不需要重新训练。如果你知道如何正确提问，你可以让模型对一段文字进行分类或总结。这意味着模型可能无法将一段文字分类为A类或B类，因为“A”和“B”的含义不明确。然而，它可以将其分类为“积极情感”或“消极情感”，因为模型知道什么是“积极”和“消极”。之所以有效，是因为在训练过程中，模型学会了这些词的含义，并获得了遵循简单指令的能力。以下是一个例子，使用GPT4All的Vicuna-7B模型演示：
- en: '![](../Images/a5030f3e7b8e6ec590875ab58c4c497e.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5030f3e7b8e6ec590875ab58c4c497e.png)'
- en: 'The prompt provided was:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的提示是：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The response was a single word, “positive.” This is correct and concise. The
    model obviously can understand “awesome” is a positive sensation, but knowing
    to identify the sensation is because of the instruction at the beginning, “Classify
    the text into positive, neutral or negative.”
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 回应是一个单词，“积极”。这是正确且简洁的。模型显然可以理解“awesome”是一种积极的感觉，但能够识别这种感觉是因为最开始的指示：“将文本分类为积极、中性或消极。”
- en: In this example, you found that the model responded because it understood your
    instruction.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，你发现模型做出了回应，因为它理解了你的指示。
- en: Few-Shot Prompting
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 少量提示
- en: 'If you cannot describe what you want but still want a language model to give
    you answers, you can provide some examples. It is easier to demonstrate this with
    the following example:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不能描述你想要什么，但仍希望语言模型给出答案，你可以提供一些示例。以下例子更容易演示：
- en: '![](../Images/10af9f59dc79323a90a91f2c63808ff0.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10af9f59dc79323a90a91f2c63808ff0.png)'
- en: 'Still using the Vicuna-7B model in GPT4All, but this time, we are providing
    the prompt:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然使用GPT4All中的Vicuna-7B模型，但这次我们提供的提示是：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here you can see that no instruction on what to do is provided, but with some
    examples, the model can figure out how to respond. Also, note that the model responds
    with “Neg” rather than “Negative” since it is what is provided in the examples.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里你可以看到没有提供如何做的指示，但通过一些示例，模型可以搞清楚如何响应。还要注意，模型回应“Neg”而不是“Negative”，因为这是示例中提供的内容。
- en: '**Note**: Due to the model’s random nature, you may be unable to reproduce
    the exact result. You may also find a different output produced each time you
    run the model.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：由于模型的随机性，你可能无法重现完全相同的结果。你可能还会发现每次运行模型时产生不同的输出。'
- en: Guiding the model to respond with examples is called few-shot prompting.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 指导模型使用示例进行响应称为少量提示（few-shot prompting）。
- en: Summary
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this post, you learned some examples of prompting. Specifically, you learned:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，你学习了一些提示的示例。具体来说，你学到了：
- en: What are one-shot and few-shot prompting
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一次性提示（one-shot prompting）和少量提示（few-shot prompting）是什么？
- en: How a model works with one-shot and few-shot prompting
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型如何处理一次性和少量提示
- en: How to test out these prompting techniques with GPT4All
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用GPT4All测试这些提示技术
