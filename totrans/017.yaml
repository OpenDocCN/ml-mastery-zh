- en: What Are Zero-Shot Prompting and Few-Shot Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://machinelearningmastery.com/what-are-zero-shot-prompting-and-few-shot-prompting/](https://machinelearningmastery.com/what-are-zero-shot-prompting-and-few-shot-prompting/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In the literature on language models, you will often encounter the terms “zero-shot
    prompting” and “few-shot prompting.” It is important to understand how a large
    language model generates an output. In this post, you will learn:'
  prefs: []
  type: TYPE_NORMAL
- en: What is zero-shot and few-shot prompting?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to experiment with them in GPT4All
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/2ddc864f7b525f85c2e41c227061d6f3.png)'
  prefs: []
  type: TYPE_IMG
- en: What Are Zero-Shot Prompting and Few-Shot Prompting
  prefs: []
  type: TYPE_NORMAL
- en: Picture generated by the author using Stable Diffusion. Some rights reserved.
  prefs: []
  type: TYPE_NORMAL
- en: '**Get started and apply ChatGPT** with my book [Maximizing Productivity with
    ChatGPT](https://machinelearningmastery.com/productivity-with-chatgpt/). It provides
    **real-world use cases** and **prompt examples** designed to get you using ChatGPT
    quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This post is divided into three parts; they are:'
  prefs: []
  type: TYPE_NORMAL
- en: How Do Large Language Models Generate Output?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero-Shot Prompting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Few-Shot Prompting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How Do Large Language Models Generate Output?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models were trained with massive amounts of text data. They were
    trained to predict the next word from the input. It is found that, given the model
    is large enough, not only the grammar of human languages can be learned, but also
    the meaning of words, common knowledge, and primitive logic.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, if you give the fragmented sentence “My neighbor’s dog is” to the
    model (as input, also known as **prompt**), it may predict with “smart” or “small”
    but not likely with “sequential,” although all these are adjectives. Similarly,
    if you provide a complete sentence to the model, you can expect a sentence that
    follows naturally from the model’s output. Repeatedly appending the model’s output
    to the original input and invoking the model again can make the model generate
    a lengthy response.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In natural language processing models, zero-shot prompting means providing a
    prompt that is not part of the training data to the model, but the model can generate
    a result that you desire. This promising technique makes large language models
    useful for many tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand why this is useful, imagine the case of sentiment analysis: You
    can take paragraphs of different opinions and label them with a sentiment classification.
    Then you can train a machine learning model (e.g., RNN on text data) to take a
    paragraph as input and generate classification as output. But you would find that
    such a model is not adaptive. If you add a new class to the classification or
    ask not to classify the paragraph but summarize them, this model must be modified
    and retrained.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A large language model, however, needs not to be retrained. You can ask the
    model to classify a paragraph or summarize it if you know how to ask correctly.
    This means the model probably cannot classify a paragraph into categories A or
    B since the meaning of “A” and “B” are unclear. Still, it can classify into “positive
    sentiment” or “negative sentiment” since the model knows what should “positive”
    and “negative” be. This works because, during the training, the model learned
    the meaning of these words and acquired the ability to follow simple instructions.
    An example is the following, demonstrated using GPT4All with the model Vicuna-7B:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5030f3e7b8e6ec590875ab58c4c497e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The prompt provided was:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The response was a single word, “positive.” This is correct and concise. The
    model obviously can understand “awesome” is a positive sensation, but knowing
    to identify the sensation is because of the instruction at the beginning, “Classify
    the text into positive, neutral or negative.”
  prefs: []
  type: TYPE_NORMAL
- en: In this example, you found that the model responded because it understood your
    instruction.
  prefs: []
  type: TYPE_NORMAL
- en: Few-Shot Prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you cannot describe what you want but still want a language model to give
    you answers, you can provide some examples. It is easier to demonstrate this with
    the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10af9f59dc79323a90a91f2c63808ff0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Still using the Vicuna-7B model in GPT4All, but this time, we are providing
    the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here you can see that no instruction on what to do is provided, but with some
    examples, the model can figure out how to respond. Also, note that the model responds
    with “Neg” rather than “Negative” since it is what is provided in the examples.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**: Due to the model’s random nature, you may be unable to reproduce
    the exact result. You may also find a different output produced each time you
    run the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Guiding the model to respond with examples is called few-shot prompting.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this post, you learned some examples of prompting. Specifically, you learned:'
  prefs: []
  type: TYPE_NORMAL
- en: What are one-shot and few-shot prompting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How a model works with one-shot and few-shot prompting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to test out these prompting techniques with GPT4All
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
