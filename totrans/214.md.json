["```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define function f(x)\ndef f(x):\n    return x**2\n\n# compute f(x) = x^2 for x=-10 to x=10\nx = np.linspace(-10,10,500)\ny = f(x)\n# Plot f(x) on left half of the figure\nfig = plt.figure(figsize=(12,5))\nax = fig.add_subplot(121)\nax.plot(x, y)\nax.set_title(\"y=f(x)\")\n\n# f'(x) using the rate of change\ndelta_x = 0.0001\ny1 = (f(x+delta_x) - f(x))/delta_x\n# f'(x) using the rule\ny2 = 2 * x\n# Plot f'(x) on right half of the figure\nax = fig.add_subplot(122)\nax.plot(x, y1, c=\"r\", alpha=0.5, label=\"rate\")\nax.plot(x, y2, c=\"b\", alpha=0.5, label=\"rule\")\nax.set_title(\"y=f'(x)\")\nax.legend()\n\nplt.show()\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef f(x):\n    return 2*x\n\n# Set up x from -10 to 10 with small steps\ndelta_x = 0.1\nx = np.arange(-10, 10, delta_x)\n# Find f(x) * delta_x\nfx = f(x) * delta_x\n# Compute the running sum\ny = fx.cumsum()\n# Plot\nplt.plot(x, y)\nplt.show()\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the range for x and y\nx = np.linspace(-10,10,1000)\nxv, yv = np.meshgrid(x, x, indexing='ij')\n\n# Compute f(x,y) = x^2 + y^3\nzv = xv**2 + yv**3\n\n# Plot the surface\nfig = plt.figure(figsize=(6,6))\nax = fig.add_subplot(projection='3d')\nax.plot_surface(xv, yv, zv, cmap=\"viridis\")\nplt.show()\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the range for x and y\nx = np.linspace(-10,10,20)\nxv, yv = np.meshgrid(x, x, indexing='ij')\n\n# Compute the gradient of f(x,y)\nfx = 2*xv\nfy = 2*yv\n\n# Convert the vector (fx,fy) into size and direction\nsize = np.sqrt(fx**2 + fy**2)\ndir_x = fx/size\ndir_y = fy/size\n\n# Plot the surface\nplt.figure(figsize=(6,6))\nplt.quiver(xv, yv, dir_x, dir_y, size, cmap=\"viridis\")\nplt.show()\n```", "```py\nimport numpy as np\n\ndef f(x, y):\n    return x**2 + y**3\n\n# 0 to 360 degrees at 0.1-degree steps\nangles = np.arange(0, 360, 0.1)\n\n# coordinate to check\nx, y = 2, 3\n# step size for differentiation\nstep = 0.0001\n\n# To keep the size and direction of maximum rate of change\nmaxdf, maxangle = -np.inf, 0\nfor angle in angles:\n    # convert degree to radian\n    rad = angle * np.pi / 180\n    # delta x and delta y for a fixed step size\n    dx, dy = np.sin(rad)*step, np.cos(rad)*step\n    # rate of change at a small step\n    df = (f(x+dx, y+dy) - f(x,y))/step\n    # keep the maximum rate of change\n    if df > maxdf:\n        maxdf, maxangle = df, angle\n\n# Report the result\ndx, dy = np.sin(maxangle*np.pi/180), np.cos(maxangle*np.pi/180)\ngradx, grady = dx*maxdf, dy*maxdf\nprint(f\"Max rate of change at {maxangle} degrees\")\nprint(f\"Gradient vector at ({x},{y}) is ({dx*maxdf},{dy*maxdf})\")\n```", "```py\nMax rate of change at 8.4 degrees\nGradient vector at (2,3) is (3.987419245872443,27.002750276227097)\n```", "```py\nfrom sympy.abc import x, y\nfrom sympy import Matrix, pprint\n\nf = Matrix([2*x*y, x**2*y])\nvariables = Matrix([x,y])\npprint(f.jacobian(variables))\n```", "```py\n⎡ 2⋅y   2⋅x⎤\n⎢          ⎥\n⎢        2 ⎥\n⎣2⋅x⋅y  x  ⎦\n```", "```py\nfrom sklearn.datasets import make_circles\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nnp.random.seed(0)\n\n# Find a small float to avoid division by zero\nepsilon = np.finfo(float).eps\n\n# Sigmoid function and its differentiation\ndef sigmoid(z):\n    return 1/(1+np.exp(-z.clip(-500, 500)))\ndef dsigmoid(z):\n    s = sigmoid(z)\n    return 2 * s * (1-s)\n\n# ReLU function and its differentiation\ndef relu(z):\n    return np.maximum(0, z)\ndef drelu(z):\n    return (z > 0).astype(float)\n\n# Loss function L(y, yhat) and its differentiation\ndef cross_entropy(y, yhat):\n    \"\"\"Binary cross entropy function\n        L = - y log yhat - (1-y) log (1-yhat)\n\n    Args:\n        y, yhat (np.array): nx1 matrices which n are the number of data instances\n    Returns:\n        average cross entropy value of shape 1x1, averaging over the n instances\n    \"\"\"\n    return ( -(y.T @ np.log(yhat.clip(epsilon)) +\n               (1-y.T) @ np.log((1-yhat).clip(epsilon))\n              ) / y.shape[1] )\n\ndef d_cross_entropy(y, yhat):\n    \"\"\" dL/dyhat \"\"\"\n    return ( - np.divide(y, yhat.clip(epsilon))\n             + np.divide(1-y, (1-yhat).clip(epsilon)) )\n\nclass mlp:\n    '''Multilayer perceptron using numpy\n    '''\n    def __init__(self, layersizes, activations, derivatives, lossderiv):\n        \"\"\"remember config, then initialize array to hold NN parameters\n        without init\"\"\"\n        # hold NN config\n        self.layersizes = tuple(layersizes)\n        self.activations = tuple(activations)\n        self.derivatives = tuple(derivatives)\n        self.lossderiv = lossderiv\n        # parameters, each is a 2D numpy array\n        L = len(self.layersizes)\n        self.z = [None] * L\n        self.W = [None] * L\n        self.b = [None] * L\n        self.a = [None] * L\n        self.dz = [None] * L\n        self.dW = [None] * L\n        self.db = [None] * L\n        self.da = [None] * L\n\n    def initialize(self, seed=42):\n        \"\"\"initialize the value of weight matrices and bias vectors with small\n        random numbers.\"\"\"\n        np.random.seed(seed)\n        sigma = 0.1\n        for l, (n_in, n_out) in enumerate(zip(self.layersizes, self.layersizes[1:]), 1):\n            self.W[l] = np.random.randn(n_in, n_out) * sigma\n            self.b[l] = np.random.randn(1, n_out) * sigma\n\n    def forward(self, x):\n        \"\"\"Feed forward using existing `W` and `b`, and overwrite the result\n        variables `a` and `z`\n\n        Args:\n            x (numpy.ndarray): Input data to feed forward\n        \"\"\"\n        self.a[0] = x\n        for l, func in enumerate(self.activations, 1):\n            # z = W a + b, with `a` as output from previous layer\n            # `W` is of size rxs and `a` the size sxn with n the number of data\n            # instances, `z` the size rxn, `b` is rx1 and broadcast to each\n            # column of `z`\n            self.z[l] = (self.a[l-1] @ self.W[l]) + self.b[l]\n            # a = g(z), with `a` as output of this layer, of size rxn\n            self.a[l] = func(self.z[l])\n        return self.a[-1]\n\n    def backward(self, y, yhat):\n        \"\"\"back propagation using NN output yhat and the reference output y,\n        generates dW, dz, db, da\n        \"\"\"\n        # first `da`, at the output\n        self.da[-1] = self.lossderiv(y, yhat)\n        for l, func in reversed(list(enumerate(self.derivatives, 1))):\n            # compute the differentials at this layer\n            self.dz[l] = self.da[l] * func(self.z[l])\n            self.dW[l] = self.a[l-1].T @ self.dz[l]\n            self.db[l] = np.mean(self.dz[l], axis=0, keepdims=True)\n            self.da[l-1] = self.dz[l] @ self.W[l].T\n\n    def update(self, eta):\n        \"\"\"Updates W and b\n\n        Args:\n            eta (float): Learning rate\n        \"\"\"\n        for l in range(1, len(self.W)):\n            self.W[l] -= eta * self.dW[l]\n            self.b[l] -= eta * self.db[l]\n\n# Make data: Two circles on x-y plane as a classification problem\nX, y = make_circles(n_samples=1000, factor=0.5, noise=0.1)\ny = y.reshape(-1,1) # our model expects a 2D array of (n_sample, n_dim)\n\n# Build a model\nmodel = mlp(layersizes=[2, 4, 3, 1],\n            activations=[relu, relu, sigmoid],\n            derivatives=[drelu, drelu, dsigmoid],\n            lossderiv=d_cross_entropy)\nmodel.initialize()\nyhat = model.forward(X)\nloss = cross_entropy(y, yhat)\nscore = accuracy_score(y, (yhat > 0.5))\nprint(f\"Before training - loss value {loss} accuracy {score}\")\n\n# train for each epoch\nn_epochs = 150\nlearning_rate = 0.005\nfor n in range(n_epochs):\n    model.forward(X)\n    yhat = model.a[-1]\n    model.backward(y, yhat)\n    model.update(learning_rate)\n    loss = cross_entropy(y, yhat)\n    score = accuracy_score(y, (yhat > 0.5))\n    print(f\"Iteration {n} - loss value {loss} accuracy {score}\")\n```", "```py\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef objective(w):\n    return w[0]**2 + w[1]**2\n\ndef constraint1(w):\n    \"Inequality for point (0,0)\"\n    return -1*w[2] - 1\n\ndef constraint2(w):\n    \"Inequality for point (1,2)\"\n    return w[0] + 2*w[1] + w[2] - 1\n\ndef constraint3(w):\n    \"Inequality for point (2,1)\"\n    return 2*w[0] + w[1] + w[2] - 1\n\n# initial guess\nw0 = np.array([1, 1, 1])\n\n# optimize\nbounds = ((-10,10), (-10,10), (-10,10))\nconstraints = [\n    {\"type\":\"ineq\", \"fun\":constraint1},\n    {\"type\":\"ineq\", \"fun\":constraint2},\n    {\"type\":\"ineq\", \"fun\":constraint3},\n]\nsolution = minimize(objective, w0, method=\"SLSQP\", bounds=bounds, constraints=constraints)\nw = solution.x\nprint(\"Objective:\", objective(w))\nprint(\"Solution:\", w)\n```", "```py\nObjective: 0.8888888888888942\nSolution: [ 0.66666667  0.66666667 -1\\.        ]\n```"]