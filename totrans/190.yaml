- en: A Brief Introduction to BERT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT 简介
- en: 原文：[https://machinelearningmastery.com/a-brief-introduction-to-bert/](https://machinelearningmastery.com/a-brief-introduction-to-bert/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/a-brief-introduction-to-bert/](https://machinelearningmastery.com/a-brief-introduction-to-bert/)
- en: As we learned [what a Transformer is](https://machinelearningmastery.com/the-transformer-model/)
    and how we might [train the Transformer model](https://machinelearningmastery.com/training-the-transformer-model/),
    we notice that it is a great tool to make a computer understand human language.
    However, the Transformer was originally designed as a model to translate one language
    to another. If we repurpose it for a different task, we would likely need to retrain
    the whole model from scratch. Given the time it takes to train a Transformer model
    is enormous, we would like to have a solution that enables us to readily reuse
    the trained Transformer for many different tasks. BERT is such a model. It is
    an extension of the encoder part of a Transformer.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们了解了 [变换器是什么](https://machinelearningmastery.com/the-transformer-model/)
    和我们可能如何 [训练变换器模型](https://machinelearningmastery.com/training-the-transformer-model/)，我们注意到它是让计算机理解人类语言的一个很好的工具。然而，变换器最初设计为一个将一种语言翻译成另一种语言的模型。如果我们将其重新用于其他任务，我们可能需要从头开始重新训练整个模型。考虑到训练变换器模型所需的时间非常长，我们希望有一个解决方案，可以使我们能够方便地重用训练好的变换器模型进行多种不同的任务。BERT
    就是这样一个模型。它是变换器编码器部分的扩展。
- en: In this tutorial, you will learn what BERT is and discover what it can do.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你将了解什么是 BERT 并发现它能做什么。
- en: 'After completing this tutorial, you will know:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本教程后，你将了解：
- en: What is a Bidirectional Encoder Representations from Transformer (BERT)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是来自变换器的双向编码表示（BERT）
- en: How a BERT model can be reused for different purposes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 模型如何被重新用于不同的目的
- en: How you can use a pre-trained BERT model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用预训练的 BERT 模型
- en: '**Kick-start your project** with my book [Building Transformer Models with
    Attention](https://machinelearningmastery.com/transformer-models-with-attention/).
    It provides **self-study tutorials** with **working code** to guide you into building
    a fully-working transformer model that can'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过我的书** [《构建具有注意力的变换器模型》](https://machinelearningmastery.com/transformer-models-with-attention/)
    **启动你的项目**。它提供了**自学教程**和**工作代码**，指导你构建一个完全可用的变换器模型。'
- en: '*translate sentences from one language to another*...'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*将句子从一种语言翻译成另一种语言*...'
- en: Let’s get started.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: '![](../Images/b5fb41fb006b15fe8b995c010e2212e8.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5fb41fb006b15fe8b995c010e2212e8.png)'
- en: A brief introduction to BERT
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 简介
- en: Photo by [Samet Erköseoğlu](https://unsplash.com/photos/B0nUaoWnr0M), some rights
    reserved.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Samet Erköseoğlu](https://unsplash.com/photos/B0nUaoWnr0M)，保留部分权利。
- en: '**Tutorial Overview**'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**教程概述**'
- en: 'This tutorial is divided into four parts; they are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程分为四个部分；它们是：
- en: From Transformer Model to BERT
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从变换器模型到 BERT
- en: What Can BERT Do?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 能做什么？
- en: Using Pre-Trained BERT Model for Summarization
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练的 BERT 模型进行摘要
- en: Using Pre-Trained BERT Model for Question-Answering
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练的 BERT 模型进行问答
- en: '**Prerequisites**'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**先决条件**'
- en: 'For this tutorial, we assume that you are already familiar with:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们假设你已经熟悉：
- en: '[The theory behind the Transformer model](https://machinelearningmastery.com/the-transformer-model/)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[变换器模型背后的理论](https://machinelearningmastery.com/the-transformer-model/)'
- en: '[An implementation of the Transformer model](https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[变换器模型的实现](https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/)'
- en: '**From Transformer Model to BERT**'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**从变换器模型到 BERT**'
- en: 'In the transformer model, the encoder and decoder are connected to make a seq2seq
    model in order for you to perform a translation, such as from English to German,
    as you saw before. Recall that the attention equation says:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在变换器模型中，编码器和解码器连接在一起形成一个 seq2seq 模型，以便你可以执行翻译任务，例如从英语到德语，正如你之前所见。回想一下注意力方程式说：
- en: $$\text{attention}(Q,K,V) = \text{softmax}\Big(\frac{QK^\top}{\sqrt{d_k}}\Big)V$$
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: $$\text{attention}(Q,K,V) = \text{softmax}\Big(\frac{QK^\top}{\sqrt{d_k}}\Big)V$$
- en: But each of the $Q$, $K$, and $V$ above is an embedding vector transformed by
    a weight matrix in the transformer model. Training a transformer model means finding
    these weight matrices. Once the weight matrices are learned, the transformer becomes
    a **language model,** which means it represents a way to understand the language
    that you used to train it.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 但是上述的 $Q$、$K$ 和 $V$ 都是通过变换器模型中的权重矩阵转换得到的嵌入向量。训练一个变换器模型意味着找到这些权重矩阵。一旦权重矩阵被学习到，变换器就成为一个**语言模型**，这意味着它代表了一种理解你用来训练它的语言的方式。
- en: '[![](../Images/5cc2fa00063bfd70298252dce57dbdcd.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/5cc2fa00063bfd70298252dce57dbdcd.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png)'
- en: The encoder-decoder structure of the Transformer architecture
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构的编码器-解码器结构
- en: Taken from “[Attention Is All You Need](https://arxiv.org/abs/1706.03762)“
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 取自“[Attention Is All You Need](https://arxiv.org/abs/1706.03762)”
- en: A transformer has encoder and decoder parts. As the name implies, the encoder
    transforms sentences and paragraphs into an internal format (a numerical matrix)
    that understands the context, whereas the decoder does the reverse. Combining
    the encoder and decoder allows a transformer to perform seq2seq tasks, such as
    translation. If you take out the encoder part of the transformer, it can tell
    you something about the context, which can do something interesting.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器具有编码器和解码器部分。顾名思义，编码器将句子和段落转换为理解上下文的内部格式（一个数值矩阵），而解码器则执行相反的操作。结合编码器和解码器使得转换器可以执行序列到序列的任务，例如翻译。如果你去掉转换器的编码器部分，它可以告诉你一些关于上下文的信息，这可能会带来一些有趣的东西。
- en: The Bidirectional Encoder Representation from Transformer (BERT) leverages the
    attention model to get a deeper understanding of the language context. BERT is
    a stack of many encoder blocks. The input text is separated into tokens as in
    the transformer model, and each token will be transformed into a vector at the
    output of BERT.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 双向编码器表示的转换器（BERT）利用注意力模型来更深入地理解语言上下文。BERT是由多个编码器块堆叠而成。输入文本像在转换器模型中一样被分隔成标记，每个标记在BERT输出时会被转换成一个向量。
- en: '**What Can BERT Do?**'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**BERT能做什么？**'
- en: A BERT model is trained using the **masked language model** (MLM) and **next
    sentence prediction** (NSP) simultaneously.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: BERT模型同时使用**掩码语言模型**（MLM）和**下一个句子预测**（NSP）进行训练。
- en: '![](../Images/703fca0a92cbf0ed7bbb94abed7c69dc.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/703fca0a92cbf0ed7bbb94abed7c69dc.png)'
- en: BERT model
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: BERT模型
- en: Each training sample for BERT is a pair of sentences from a document. The two
    sentences can be consecutive in the document or not. There will be a `[CLS]` token
    prepended to the first sentence (to represent the **class**) and a `[SEP]` token
    appended to each sentence (as a **separator**). Then, the two sentences will be
    concatenated as a sequence of tokens to become a training sample. A small percentage
    of the tokens in the training sample is *masked* with a special token `[MASK]` or
    replaced with a random token.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 每个BERT训练样本是一对来自文档的句子。这两个句子可以是文档中的连续句子，也可以不是。第一个句子前会加上一个`[CLS]`标记（表示**类别**），每个句子后会加上一个`[SEP]`标记（作为**分隔符**）。然后，将两个句子拼接成一个标记序列，作为一个训练样本。训练样本中的一小部分标记会用特殊标记`[MASK]`掩码或替换为随机标记。
- en: Before it is fed into the BERT model, the tokens in the training sample will
    be transformed into embedding vectors, with the positional encodings added, and
    particular to BERT, with **segment embeddings** added as well to mark whether
    the token is from the first or the second sentence.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入到BERT模型之前，训练样本中的标记将被转换成嵌入向量，并添加位置编码，特别是BERT还会添加**段落嵌入**以标记标记是来自第一句还是第二句。
- en: 'Each input token to the BERT model will produce one output vector. In a well-trained
    BERT model, we expect:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: BERT模型的每个输入词将产生一个输出向量。在训练良好的BERT模型中，我们期望：
- en: output corresponding to the masked token can reveal what the original token
    was
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被掩码的词，输出结果可以揭示原始词是什么。
- en: output corresponding to the `[CLS]` token at the beginning can reveal whether
    the two sentences are consecutive in the document
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对应于`[CLS]`标记的输出可以揭示两个句子在文档中是否是连续的。
- en: Then, the weights trained in the BERT model can understand the language context
    well.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，BERT模型中训练得到的权重可以很好地理解语言上下文。
- en: Once you have such a BERT model, you can use it for many **downstream tasks**.
    For example, by adding an appropriate classification layer on top of an encoder
    and feeding in only one sentence to the model instead of a pair, you can take
    the class token `[CLS]` as input for sentiment classification. It works because
    the output of the class token is trained to aggregate the attention for the entire
    input.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你拥有这样的BERT模型，你可以将其用于许多**下游任务**。例如，通过在编码器上添加一个适当的分类层，并仅将一句话输入模型而不是一对句子，你可以将类别标记`[CLS]`作为情感分类的输入。这是因为类别标记的输出经过训练，可以聚合整个输入的注意力。
- en: Another example is to take a question as the first sentence and the text (e.g.,
    a paragraph) as the second sentence, then the output token from the second sentence
    can mark the position where the answer to the question rested. It works because
    the output of each token reveals some information about that token in the context
    of the entire input.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是将一个问题作为第一句话，将文本（例如，一个段落）作为第二句话，然后第二句话中的输出标记可以标记出问题答案所在的位置。它有效的原因是每个标记的输出在整个输入的上下文中揭示了有关该标记的一些信息。
- en: Using Pre-Trained BERT Model for Summarization
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预训练的 BERT 模型进行摘要
- en: A transformer model takes a long time to train from scratch. The BERT model
    would take even longer. But the purpose of BERT is to create one model that can
    be reused for many different tasks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始训练一个 Transformer 模型需要很长时间。BERT 模型则需要更长的时间。但 BERT 的目的是创建一个可以用于多种不同任务的模型。
- en: 'There are pre-trained BERT models that you can use readily. In the following,
    you will see a few use cases. The text used in the following example is from:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些预训练的 BERT 模型可以直接使用。接下来，你将看到一些使用案例。以下示例使用的文本来自：
- en: '[https://www.project-syndicate.org/commentary/bank-of-england-gilt-purchases-necessary-but-mistakes-made-by-willem-h-buiter-and-anne-c-sibert-2022-10](https://www.project-syndicate.org/commentary/bank-of-england-gilt-purchases-necessary-but-mistakes-made-by-willem-h-buiter-and-anne-c-sibert-2022-10)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.project-syndicate.org/commentary/bank-of-england-gilt-purchases-necessary-but-mistakes-made-by-willem-h-buiter-and-anne-c-sibert-2022-10](https://www.project-syndicate.org/commentary/bank-of-england-gilt-purchases-necessary-but-mistakes-made-by-willem-h-buiter-and-anne-c-sibert-2022-10)'
- en: Theoretically, a BERT model is an encoder that maps each input token to an output
    vector, which can be extended to an infinite length sequence of tokens. In practice,
    there are limitations imposed in the implementation of other components that limit
    the input size. Mostly, a few hundred tokens should work, as not every implementation
    can take thousands of tokens in one shot. You can save the entire article in `article.txt`
    (a copy is available [here](https://machinelearningmastery.com/wp-content/uploads/2022/10/article.txt)).
    In case your model needs a smaller text, you can use only a few paragraphs from
    it.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，BERT 模型是一个编码器，将每个输入标记映射到一个输出向量，这可以扩展到无限长度的标记序列。在实践中，其他组件的实现会施加限制，限制输入大小。通常，几百个标记应该是可以的，因为并非所有实现都能一次处理数千个标记。你可以将整篇文章保存为
    `article.txt`（一个副本可以在[这里](https://machinelearningmastery.com/wp-content/uploads/2022/10/article.txt)获取）。如果你的模型需要更小的文本，你可以只使用其中的几个段落。
- en: First, let’s explore the task for summarization. Using BERT, the idea is to
    *extract* a few sentences from the original text that represent the entire text.
    You can see this task is similar to next sentence prediction, in which if given
    a sentence and the text, you want to classify if they are related.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们探讨摘要任务。使用 BERT 的想法是从原始文本中 *提取* 几句话，这些句子代表整个文本。你可以看到这个任务类似于下一句预测，其中如果给定一句话和文本，你希望分类它们是否相关。
- en: To do that, you need to use the Python module `bert-extractive-summarizer`
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，你需要使用 Python 模块 `bert-extractive-summarizer`
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It is a wrapper to some Hugging Face models to provide the summarization task
    pipeline. Hugging Face is a platform that allows you to publish machine learning
    models, mainly on NLP tasks.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一些 Hugging Face 模型的包装器，用于提供摘要任务流水线。Hugging Face 是一个允许你发布机器学习模型的平台，主要用于 NLP
    任务。
- en: 'Once you have installed `bert-extractive-summarizer`, producing a summary is
    just a few lines of code:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你安装了 `bert-extractive-summarizer`，生成摘要只需要几行代码：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This gives the output:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: That’s the complete code! Behind the scene, spaCy was used on some preprocessing,
    and Hugging Face was used to launch the model. The model used was named `distilbert-base-uncased`.
    DistilBERT is a simplified BERT model that can run faster and use less memory.
    The model is an “uncased” one, which means the uppercase or lowercase in the input
    text is considered the same once it is transformed into embedding vectors.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是完整的代码！在幕后，spaCy 被用于一些预处理，而 Hugging Face 被用于启动模型。使用的模型名为 `distilbert-base-uncased`。DistilBERT
    是一个简化版的 BERT 模型，可以更快运行并使用更少的内存。该模型是一个“uncased”模型，这意味着输入文本中的大写或小写在转换为嵌入向量后被视为相同。
- en: The output from the summarizer model is a string. As you specified `num_sentences=3`
    in invoking the model, the summary is three selected sentences from the text.
    This approach is called the **extractive summary**. The alternative is an **abstractive
    summary**, in which the summary is generated rather than extracted from the text.
    This would need a different model than BERT.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要模型的输出是一个字符串。由于你在调用模型时指定了`num_sentences=3`，因此摘要是从文本中选择的三句话。这种方法称为**提取式摘要**。另一种方法是**抽象式摘要**，其中摘要是生成的，而不是从文本中提取的。这需要不同于BERT的模型。
- en: Want to Get Started With Building Transformer Models with Attention?
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 想要开始构建带有注意力机制的变换器模型吗？
- en: Take my free 12-day email crash course now (with sample code).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 立即参加我的免费12天电子邮件速成课程（附样本代码）。
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 点击注册并获得课程的免费PDF电子书版本。
- en: Using Pre-Trained BERT Model for Question-Answering
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预训练BERT模型进行问答
- en: The other example of using BERT is to match questions to answers. You will give
    both the question and the text to the model and look for the output of the beginning
    *and* the end of the answer from the text.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用BERT的另一个示例是将问题与答案匹配。你将问题和文本都提供给模型，并从文本中寻找答案的开始 *和* 结束位置的输出。
- en: 'A quick example would be just a few lines of code as follows, reusing the same
    example text as in the previous example:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一个快速的示例就是如下几行代码，重用前面示例中的相同文本：
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, Hugging Face is used directly. If you have installed the module used
    in the previous example, the Hugging Face Python module is a dependence that you
    already installed. Otherwise, you may need to install it with `pip`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，直接使用了Hugging Face。如果你已经安装了前面示例中使用的模块，那么Hugging Face Python模块是你已经安装的依赖项。否则，你可能需要用`pip`进行安装：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And to actually use a Hugging Face model, you should have **both** PyTorch
    and TensorFlow installed as well:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 而且为了实际使用Hugging Face模型，你还应该安装**both** PyTorch和TensorFlow：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output of the code above is a Python dictionary, as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出是一个Python字典，如下所示：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This is where you can find the answer (which is a sentence from the input text),
    as well as the begin and end position in the token order where this answer was
    from. The score can be regarded as the confidence score from the model that the
    answer could fit the question.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以找到答案（即输入文本中的一句话），以及这个答案在标记顺序中的起始和结束位置。这个分数可以被视为模型对答案适合问题的置信度分数。
- en: Behind the scenes, what the model did was generate a probability score for the
    best beginning in the text that answers the question, as well as the text for
    the best ending. Then the answer is extracted by finding the location of the highest
    probabilities.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在后台，模型所做的是生成一个概率分数，用于确定文本中回答问题的最佳起始位置，以及最佳结束位置。然后通过查找最高概率的位置来提取答案。
- en: '**Further Reading**'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: This section provides more resources on the topic if you are looking to go deeper.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了更多关于该主题的资源，如果你想深入了解。
- en: '**Papers**'
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**论文**'
- en: '[Attention Is All You Need](https://arxiv.org/abs/1706.03762), 2017'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Attention Is All You Need](https://arxiv.org/abs/1706.03762)，2017'
- en: '[BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805),
    2019'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BERT：深度双向变换器语言理解的预训练](https://arxiv.org/abs/1810.04805)，2019'
- en: '[DistilBERT, a distilled version of BERT: smaller, faster, cheaper, and lighter](https://arxiv.org/abs/1910.01108),
    2019'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DistilBERT，BERT的精简版本：更小、更快、更便宜、更轻](https://arxiv.org/abs/1910.01108)，2019'
- en: '**Summary**'
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this tutorial, you discovered what BERT is and how to use a pre-trained BERT
    model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你发现了BERT是什么以及如何使用预训练的BERT模型。
- en: 'Specifically, you learned:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，你学到了：
- en: How is BERT created as an extension to Transformer models
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT如何作为对变换器模型的扩展创建
- en: How to use pre-trained BERT models for extractive summarization and question
    answering
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用预训练BERT模型进行提取式摘要和问答
