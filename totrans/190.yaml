- en: A Brief Introduction to BERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://machinelearningmastery.com/a-brief-introduction-to-bert/](https://machinelearningmastery.com/a-brief-introduction-to-bert/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As we learned [what a Transformer is](https://machinelearningmastery.com/the-transformer-model/)
    and how we might [train the Transformer model](https://machinelearningmastery.com/training-the-transformer-model/),
    we notice that it is a great tool to make a computer understand human language.
    However, the Transformer was originally designed as a model to translate one language
    to another. If we repurpose it for a different task, we would likely need to retrain
    the whole model from scratch. Given the time it takes to train a Transformer model
    is enormous, we would like to have a solution that enables us to readily reuse
    the trained Transformer for many different tasks. BERT is such a model. It is
    an extension of the encoder part of a Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, you will learn what BERT is and discover what it can do.
  prefs: []
  type: TYPE_NORMAL
- en: 'After completing this tutorial, you will know:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a Bidirectional Encoder Representations from Transformer (BERT)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How a BERT model can be reused for different purposes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How you can use a pre-trained BERT model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kick-start your project** with my book [Building Transformer Models with
    Attention](https://machinelearningmastery.com/transformer-models-with-attention/).
    It provides **self-study tutorials** with **working code** to guide you into building
    a fully-working transformer model that can'
  prefs: []
  type: TYPE_NORMAL
- en: '*translate sentences from one language to another*...'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5fb41fb006b15fe8b995c010e2212e8.png)'
  prefs: []
  type: TYPE_IMG
- en: A brief introduction to BERT
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Samet Erköseoğlu](https://unsplash.com/photos/B0nUaoWnr0M), some rights
    reserved.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tutorial Overview**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This tutorial is divided into four parts; they are:'
  prefs: []
  type: TYPE_NORMAL
- en: From Transformer Model to BERT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What Can BERT Do?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Pre-Trained BERT Model for Summarization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Pre-Trained BERT Model for Question-Answering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prerequisites**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this tutorial, we assume that you are already familiar with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[The theory behind the Transformer model](https://machinelearningmastery.com/the-transformer-model/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An implementation of the Transformer model](https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**From Transformer Model to BERT**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the transformer model, the encoder and decoder are connected to make a seq2seq
    model in order for you to perform a translation, such as from English to German,
    as you saw before. Recall that the attention equation says:'
  prefs: []
  type: TYPE_NORMAL
- en: $$\text{attention}(Q,K,V) = \text{softmax}\Big(\frac{QK^\top}{\sqrt{d_k}}\Big)V$$
  prefs: []
  type: TYPE_NORMAL
- en: But each of the $Q$, $K$, and $V$ above is an embedding vector transformed by
    a weight matrix in the transformer model. Training a transformer model means finding
    these weight matrices. Once the weight matrices are learned, the transformer becomes
    a **language model,** which means it represents a way to understand the language
    that you used to train it.
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/5cc2fa00063bfd70298252dce57dbdcd.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The encoder-decoder structure of the Transformer architecture
  prefs: []
  type: TYPE_NORMAL
- en: Taken from “[Attention Is All You Need](https://arxiv.org/abs/1706.03762)“
  prefs: []
  type: TYPE_NORMAL
- en: A transformer has encoder and decoder parts. As the name implies, the encoder
    transforms sentences and paragraphs into an internal format (a numerical matrix)
    that understands the context, whereas the decoder does the reverse. Combining
    the encoder and decoder allows a transformer to perform seq2seq tasks, such as
    translation. If you take out the encoder part of the transformer, it can tell
    you something about the context, which can do something interesting.
  prefs: []
  type: TYPE_NORMAL
- en: The Bidirectional Encoder Representation from Transformer (BERT) leverages the
    attention model to get a deeper understanding of the language context. BERT is
    a stack of many encoder blocks. The input text is separated into tokens as in
    the transformer model, and each token will be transformed into a vector at the
    output of BERT.
  prefs: []
  type: TYPE_NORMAL
- en: '**What Can BERT Do?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A BERT model is trained using the **masked language model** (MLM) and **next
    sentence prediction** (NSP) simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/703fca0a92cbf0ed7bbb94abed7c69dc.png)'
  prefs: []
  type: TYPE_IMG
- en: BERT model
  prefs: []
  type: TYPE_NORMAL
- en: Each training sample for BERT is a pair of sentences from a document. The two
    sentences can be consecutive in the document or not. There will be a `[CLS]` token
    prepended to the first sentence (to represent the **class**) and a `[SEP]` token
    appended to each sentence (as a **separator**). Then, the two sentences will be
    concatenated as a sequence of tokens to become a training sample. A small percentage
    of the tokens in the training sample is *masked* with a special token `[MASK]` or
    replaced with a random token.
  prefs: []
  type: TYPE_NORMAL
- en: Before it is fed into the BERT model, the tokens in the training sample will
    be transformed into embedding vectors, with the positional encodings added, and
    particular to BERT, with **segment embeddings** added as well to mark whether
    the token is from the first or the second sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each input token to the BERT model will produce one output vector. In a well-trained
    BERT model, we expect:'
  prefs: []
  type: TYPE_NORMAL
- en: output corresponding to the masked token can reveal what the original token
    was
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: output corresponding to the `[CLS]` token at the beginning can reveal whether
    the two sentences are consecutive in the document
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, the weights trained in the BERT model can understand the language context
    well.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have such a BERT model, you can use it for many **downstream tasks**.
    For example, by adding an appropriate classification layer on top of an encoder
    and feeding in only one sentence to the model instead of a pair, you can take
    the class token `[CLS]` as input for sentiment classification. It works because
    the output of the class token is trained to aggregate the attention for the entire
    input.
  prefs: []
  type: TYPE_NORMAL
- en: Another example is to take a question as the first sentence and the text (e.g.,
    a paragraph) as the second sentence, then the output token from the second sentence
    can mark the position where the answer to the question rested. It works because
    the output of each token reveals some information about that token in the context
    of the entire input.
  prefs: []
  type: TYPE_NORMAL
- en: Using Pre-Trained BERT Model for Summarization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A transformer model takes a long time to train from scratch. The BERT model
    would take even longer. But the purpose of BERT is to create one model that can
    be reused for many different tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are pre-trained BERT models that you can use readily. In the following,
    you will see a few use cases. The text used in the following example is from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.project-syndicate.org/commentary/bank-of-england-gilt-purchases-necessary-but-mistakes-made-by-willem-h-buiter-and-anne-c-sibert-2022-10](https://www.project-syndicate.org/commentary/bank-of-england-gilt-purchases-necessary-but-mistakes-made-by-willem-h-buiter-and-anne-c-sibert-2022-10)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theoretically, a BERT model is an encoder that maps each input token to an output
    vector, which can be extended to an infinite length sequence of tokens. In practice,
    there are limitations imposed in the implementation of other components that limit
    the input size. Mostly, a few hundred tokens should work, as not every implementation
    can take thousands of tokens in one shot. You can save the entire article in `article.txt`
    (a copy is available [here](https://machinelearningmastery.com/wp-content/uploads/2022/10/article.txt)).
    In case your model needs a smaller text, you can use only a few paragraphs from
    it.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s explore the task for summarization. Using BERT, the idea is to
    *extract* a few sentences from the original text that represent the entire text.
    You can see this task is similar to next sentence prediction, in which if given
    a sentence and the text, you want to classify if they are related.
  prefs: []
  type: TYPE_NORMAL
- en: To do that, you need to use the Python module `bert-extractive-summarizer`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It is a wrapper to some Hugging Face models to provide the summarization task
    pipeline. Hugging Face is a platform that allows you to publish machine learning
    models, mainly on NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have installed `bert-extractive-summarizer`, producing a summary is
    just a few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: That’s the complete code! Behind the scene, spaCy was used on some preprocessing,
    and Hugging Face was used to launch the model. The model used was named `distilbert-base-uncased`.
    DistilBERT is a simplified BERT model that can run faster and use less memory.
    The model is an “uncased” one, which means the uppercase or lowercase in the input
    text is considered the same once it is transformed into embedding vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The output from the summarizer model is a string. As you specified `num_sentences=3`
    in invoking the model, the summary is three selected sentences from the text.
    This approach is called the **extractive summary**. The alternative is an **abstractive
    summary**, in which the summary is generated rather than extracted from the text.
    This would need a different model than BERT.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Get Started With Building Transformer Models with Attention?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Take my free 12-day email crash course now (with sample code).
  prefs: []
  type: TYPE_NORMAL
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  prefs: []
  type: TYPE_NORMAL
- en: Using Pre-Trained BERT Model for Question-Answering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The other example of using BERT is to match questions to answers. You will give
    both the question and the text to the model and look for the output of the beginning
    *and* the end of the answer from the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick example would be just a few lines of code as follows, reusing the same
    example text as in the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, Hugging Face is used directly. If you have installed the module used
    in the previous example, the Hugging Face Python module is a dependence that you
    already installed. Otherwise, you may need to install it with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'And to actually use a Hugging Face model, you should have **both** PyTorch
    and TensorFlow installed as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the code above is a Python dictionary, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This is where you can find the answer (which is a sentence from the input text),
    as well as the begin and end position in the token order where this answer was
    from. The score can be regarded as the confidence score from the model that the
    answer could fit the question.
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes, what the model did was generate a probability score for the
    best beginning in the text that answers the question, as well as the text for
    the best ending. Then the answer is extracted by finding the location of the highest
    probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Reading**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section provides more resources on the topic if you are looking to go deeper.
  prefs: []
  type: TYPE_NORMAL
- en: '**Papers**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Attention Is All You Need](https://arxiv.org/abs/1706.03762), 2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805),
    2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[DistilBERT, a distilled version of BERT: smaller, faster, cheaper, and lighter](https://arxiv.org/abs/1910.01108),
    2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this tutorial, you discovered what BERT is and how to use a pre-trained BERT
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, you learned:'
  prefs: []
  type: TYPE_NORMAL
- en: How is BERT created as an extension to Transformer models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use pre-trained BERT models for extractive summarization and question
    answering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
