["```py\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\n\n# loading training data\ntrain_dataset = datasets.MNIST(root='./data', \n                               train=True, \n                               transform=transforms.ToTensor(),\n                               download=True)\n#loading test data\ntest_dataset = datasets.MNIST(root='./data', \n                              train=False, \n                              transform=transforms.ToTensor())\n```", "```py\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n  0%|          | 0/9912422 [00:00<?, ?it/s]\nExtracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n  0%|          | 0/28881 [00:00<?, ?it/s]\nExtracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n  0%|          | 0/1648877 [00:00<?, ?it/s]\nExtracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n  0%|          | 0/4542 [00:00<?, ?it/s]\nExtracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n```", "```py\nprint(\"number of training samples: \" + str(len(train_dataset)) + \"\\n\" +\n      \"number of testing samples: \" + str(len(test_dataset)))\n```", "```py\nnumber of training samples: 60000\nnumber of testing samples: 10000\n```", "```py\nprint(\"datatype of the 1st training sample: \", train_dataset[0][0].type())\nprint(\"size of the 1st training sample: \", train_dataset[0][0].size())\n```", "```py\ndatatype of the 1st training sample:  torch.FloatTensor\nsize of the 1st training sample:  torch.Size([1, 28, 28])\n```", "```py\n# check the label of first two training sample\nprint(\"label of the first taining sample: \", train_dataset[0][1])\nprint(\"label of the second taining sample: \", train_dataset[1][1])\n```", "```py\nlabel of the first taining sample:  5\nlabel of the second taining sample:  0\n```", "```py\nimg_5 = train_dataset[0][0].numpy().reshape(28, 28)\nplt.imshow(img_5, cmap='gray')\nplt.show()\nimg_0 = train_dataset[1][0].numpy().reshape(28, 28)\nplt.imshow(img_0, cmap='gray')\nplt.show()\n```", "```py\n...\nfrom torch.utils.data import DataLoader\n\n# load train and test data samples into dataloader\nbatach_size = 32\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batach_size, shuffle=True) \ntest_loader = DataLoader(dataset=test_dataset, batch_size=batach_size, shuffle=False)\n```", "```py\n# build custom module for logistic regression\nclass LogisticRegression(torch.nn.Module):    \n    # build the constructor\n    def __init__(self, n_inputs, n_outputs):\n        super(LogisticRegression, self).__init__()\n        self.linear = torch.nn.Linear(n_inputs, n_outputs)\n    # make predictions\n    def forward(self, x):\n        y_pred = torch.sigmoid(self.linear(x))\n        return y_pred\n```", "```py\n# instantiate the model\nn_inputs = 28*28 # makes a 1D vector of 784\nn_outputs = 10\nlog_regr = LogisticRegression(n_inputs, n_outputs)\n```", "```py\n...\n\n# defining the optimizer\noptimizer = torch.optim.SGD(log_regr.parameters(), lr=0.001)\n# defining Cross-Entropy loss\ncriterion = torch.nn.CrossEntropyLoss()\n\nepochs = 50\nLoss = []\nacc = []\nfor epoch in range(epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n        outputs = log_regr(images.view(-1, 28*28))\n        loss = criterion(outputs, labels)\n        # Loss.append(loss.item())\n        loss.backward()\n        optimizer.step()\n    Loss.append(loss.item())\n    correct = 0\n    for images, labels in test_loader:\n        outputs = log_regr(images.view(-1, 28*28))\n        _, predicted = torch.max(outputs.data, 1)\n        correct += (predicted == labels).sum()\n    accuracy = 100 * (correct.item()) / len(test_dataset)\n    acc.append(accuracy)\n    print('Epoch: {}. Loss: {}. Accuracy: {}'.format(epoch, loss.item(), accuracy))\n```", "```py\nEpoch: 0\\. Loss: 2.211054563522339\\. Accuracy: 61.63\nEpoch: 1\\. Loss: 2.1178536415100098\\. Accuracy: 74.81\nEpoch: 2\\. Loss: 2.0735440254211426\\. Accuracy: 78.47\nEpoch: 3\\. Loss: 2.040225028991699\\. Accuracy: 80.17\nEpoch: 4\\. Loss: 1.9637292623519897\\. Accuracy: 81.05\nEpoch: 5\\. Loss: 2.000900983810425\\. Accuracy: 81.44\n...\nEpoch: 45\\. Loss: 1.6549798250198364\\. Accuracy: 86.3\nEpoch: 46\\. Loss: 1.7053509950637817\\. Accuracy: 86.31\nEpoch: 47\\. Loss: 1.7396119832992554\\. Accuracy: 86.36\nEpoch: 48\\. Loss: 1.6963073015213013\\. Accuracy: 86.37\nEpoch: 49\\. Loss: 1.6838685274124146\\. Accuracy: 86.46\n```", "```py\nplt.plot(Loss)\nplt.xlabel(\"no. of epochs\")\nplt.ylabel(\"total loss\")\nplt.title(\"Loss\")\nplt.show()\n```", "```py\nplt.plot(acc)\nplt.xlabel(\"no. of epochs\")\nplt.ylabel(\"total accuracy\")\nplt.title(\"Accuracy\")\nplt.show()\n```", "```py\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n\n# loading training data\ntrain_dataset = datasets.MNIST(root='./data', \n                               train=True, \n                               transform=transforms.ToTensor(),\n                               download=True)\n# loading test data\ntest_dataset = datasets.MNIST(root='./data', \n                              train=False, \n                              transform=transforms.ToTensor())\n\nprint(\"number of training samples: \" + str(len(train_dataset)) + \"\\n\" +\n      \"number of testing samples: \" + str(len(test_dataset)))\nprint(\"datatype of the 1st training sample: \", train_dataset[0][0].type())\nprint(\"size of the 1st training sample: \", train_dataset[0][0].size())\n\n# check the label of first two training sample\nprint(\"label of the first taining sample: \", train_dataset[0][1])\nprint(\"label of the second taining sample: \", train_dataset[1][1])\n\nimg_5 = train_dataset[0][0].numpy().reshape(28, 28)\nplt.imshow(img_5, cmap='gray')\nplt.show()\nimg_0 = train_dataset[1][0].numpy().reshape(28, 28)\nplt.imshow(img_0, cmap='gray')\nplt.show()\n\n# load train and test data samples into dataloader\nbatach_size = 32\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batach_size, shuffle=True) \ntest_loader = DataLoader(dataset=test_dataset, batch_size=batach_size, shuffle=False)\n\n# build custom module for logistic regression\nclass LogisticRegression(torch.nn.Module):    \n    # build the constructor\n    def __init__(self, n_inputs, n_outputs):\n        super().__init__()\n        self.linear = torch.nn.Linear(n_inputs, n_outputs)\n    # make predictions\n    def forward(self, x):\n        y_pred = torch.sigmoid(self.linear(x))\n        return y_pred\n\n# instantiate the model\nn_inputs = 28*28 # makes a 1D vector of 784\nn_outputs = 10\nlog_regr = LogisticRegression(n_inputs, n_outputs)\n\n# defining the optimizer\noptimizer = torch.optim.SGD(log_regr.parameters(), lr=0.001)\n# defining Cross-Entropy loss\ncriterion = torch.nn.CrossEntropyLoss()\n\nepochs = 50\nLoss = []\nacc = []\nfor epoch in range(epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n        outputs = log_regr(images.view(-1, 28*28))\n        loss = criterion(outputs, labels)\n        # Loss.append(loss.item())\n        loss.backward()\n        optimizer.step()\n    Loss.append(loss.item())\n    correct = 0\n    for images, labels in test_loader:\n        outputs = log_regr(images.view(-1, 28*28))\n        _, predicted = torch.max(outputs.data, 1)\n        correct += (predicted == labels).sum()\n    accuracy = 100 * (correct.item()) / len(test_dataset)\n    acc.append(accuracy)\n    print('Epoch: {}. Loss: {}. Accuracy: {}'.format(epoch, loss.item(), accuracy))\n\nplt.plot(Loss)\nplt.xlabel(\"no. of epochs\")\nplt.ylabel(\"total loss\")\nplt.title(\"Loss\")\nplt.show()\n\nplt.plot(acc)\nplt.xlabel(\"no. of epochs\")\nplt.ylabel(\"total accuracy\")\nplt.title(\"Accuracy\")\nplt.show()\n```"]