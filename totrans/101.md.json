["```py\nimport pickle\n```", "```py\ntest_dict = {\"Hello\": \"World!\"}\nwith open(\"test.pickle\", \"wb\") as outfile:\n \t# \"wb\" argument opens the file in binary mode\n\tpickle.dump(test_dict, outfile)\n```", "```py\nwith open(\"test.pickle\", \"rb\") as infile:\n \ttest_dict_reconstructed = pickle.load(infile)\n```", "```py\nimport pickle\n\n# A test object\ntest_dict = {\"Hello\": \"World!\"}\n\n# Serialization\nwith open(\"test.pickle\", \"wb\") as outfile:\n    pickle.dump(test_dict, outfile)\nprint(\"Written object\", test_dict)\n\n# Deserialization\nwith open(\"test.pickle\", \"rb\") as infile:\n    test_dict_reconstructed = pickle.load(infile)\nprint(\"Reconstructed object\", test_dict_reconstructed)\n\nif test_dict == test_dict_reconstructed:\n    print(\"Reconstruction success\")\n```", "```py\ntest_dict_ba = pickle.dumps(test_dict)      # b'\\x80\\x04\\x95\\x15…\n```", "```py\ntest_dict_reconstructed_ba = pickle.loads(test_dict_ba)\n```", "```py\nimport pickle\n\nclass NewClass:\n    def __init__(self, data):\n        print(data)\n        self.data = data\n\n# Create an object of NewClass\nnew_class = NewClass(1)\n\n# Serialize and deserialize\npickled_data = pickle.dumps(new_class)\nreconstructed = pickle.loads(pickled_data)\n\n# Verify\nprint(\"Data from reconstructed object:\", reconstructed.data)\n```", "```py\n1\nData from reconstructed object: 1\n```", "```py\nimport pickle\n\ndef test():\n    return \"Hello world!\"\n\n# Serialize and deserialize\npickled_function = pickle.dumps(test)\nreconstructed_function = pickle.loads(pickled_function)\n\n# Verify\nprint (reconstructed_function()) #prints “Hello, world!”\n```", "```py\nimport pickle\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dense, AveragePooling2D, Dropout, Flatten\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Load MNIST digits\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n# Reshape data to (n_samples, height, wiedth, n_channel)\nX_train = np.expand_dims(X_train, axis=3).astype(\"float32\")\nX_test = np.expand_dims(X_test, axis=3).astype(\"float32\")\n\n# One-hot encode the output\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\n# LeNet5 model\nmodel = Sequential([\n    Conv2D(6, (5,5), input_shape=(28,28,1), padding=\"same\", activation=\"tanh\"),\n    AveragePooling2D((2,2), strides=2),\n    Conv2D(16, (5,5), activation=\"tanh\"),\n    AveragePooling2D((2,2), strides=2),\n    Conv2D(120, (5,5), activation=\"tanh\"),\n    Flatten(),\n    Dense(84, activation=\"tanh\"),\n    Dense(10, activation=\"softmax\")\n])\n\n# Train the model\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nearlystopping = EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[earlystopping])\n\n# Evaluate the model\nprint(model.evaluate(X_test, y_test, verbose=0))\n\n# Pickle to serialize and deserialize\npickled_model = pickle.dumps(model)\nreconstructed = pickle.loads(pickled_model)\n\n# Evaluate again\nprint(reconstructed.evaluate(X_test, y_test, verbose=0))\n```", "```py\nEpoch 1/100\n1875/1875 [==============================] - 15s 7ms/step - loss: 0.1517 - accuracy: 0.9541 - val_loss: 0.0958 - val_accuracy: 0.9661\nEpoch 2/100\n1875/1875 [==============================] - 15s 8ms/step - loss: 0.0616 - accuracy: 0.9814 - val_loss: 0.0597 - val_accuracy: 0.9822\nEpoch 3/100\n1875/1875 [==============================] - 16s 8ms/step - loss: 0.0493 - accuracy: 0.9846 - val_loss: 0.0449 - val_accuracy: 0.9853\nEpoch 4/100\n1875/1875 [==============================] - 17s 9ms/step - loss: 0.0394 - accuracy: 0.9876 - val_loss: 0.0496 - val_accuracy: 0.9838\nEpoch 5/100\n1875/1875 [==============================] - 17s 9ms/step - loss: 0.0320 - accuracy: 0.9898 - val_loss: 0.0394 - val_accuracy: 0.9870\nEpoch 6/100\n1875/1875 [==============================] - 16s 9ms/step - loss: 0.0294 - accuracy: 0.9908 - val_loss: 0.0373 - val_accuracy: 0.9872\nEpoch 7/100\n1875/1875 [==============================] - 21s 11ms/step - loss: 0.0252 - accuracy: 0.9921 - val_loss: 0.0370 - val_accuracy: 0.9879\nEpoch 8/100\n1875/1875 [==============================] - 18s 10ms/step - loss: 0.0223 - accuracy: 0.9931 - val_loss: 0.0386 - val_accuracy: 0.9880\nEpoch 9/100\n1875/1875 [==============================] - 15s 8ms/step - loss: 0.0219 - accuracy: 0.9930 - val_loss: 0.0418 - val_accuracy: 0.9871\nEpoch 10/100\n1875/1875 [==============================] - 15s 8ms/step - loss: 0.0162 - accuracy: 0.9950 - val_loss: 0.0531 - val_accuracy: 0.9853\nEpoch 11/100\n1875/1875 [==============================] - 15s 8ms/step - loss: 0.0169 - accuracy: 0.9941 - val_loss: 0.0340 - val_accuracy: 0.9895\nEpoch 12/100\n1875/1875 [==============================] - 15s 8ms/step - loss: 0.0165 - accuracy: 0.9944 - val_loss: 0.0457 - val_accuracy: 0.9874\nEpoch 13/100\n1875/1875 [==============================] - 15s 8ms/step - loss: 0.0137 - accuracy: 0.9955 - val_loss: 0.0407 - val_accuracy: 0.9879\nEpoch 14/100\n1875/1875 [==============================] - 16s 8ms/step - loss: 0.0159 - accuracy: 0.9945 - val_loss: 0.0442 - val_accuracy: 0.9871\nEpoch 15/100\n1875/1875 [==============================] - 16s 8ms/step - loss: 0.0125 - accuracy: 0.9956 - val_loss: 0.0434 - val_accuracy: 0.9882\n[0.0340442918241024, 0.9894999861717224]\n[0.0340442918241024, 0.9894999861717224]\n```", "```py\npip install h5py\n```", "```py\nconda install h5py\n```", "```py\nimport h5py\n\nwith h5py.File(\"test.hdf5\", \"w\") as file:\n    dataset = file.create_dataset(\"test_dataset\", (100,), type=\"i4\")\n```", "```py\ndataset[0]  #retrieves element at index 0 of dataset\n```", "```py\ndataset[:10]\n```", "```py\nwith h5py.File(\"test.hdf5\", \"r\") as file:\n    print (file.keys()) #gets names of datasets that are in the file\n    dataset = file[\"test_dataset\"]\n```", "```py\nwith h5py.File(\"test.hdf5\", \"w\") as file:\n    # creates new group_1 in file\n    file.create_group(\"group_1\")\n    group1 = file[\"group_1\"]\n    # creates dataset inside group1\n    group1.create_dataset(\"dataset1\", shape=(10,))\n    # to access the dataset\n    dataset = file[\"group_1\"][\"dataset1\"]\n```", "```py\nwith h5py.File(\"test.hdf5\", \"w\") as file:\n    # creates dataset inside group1\n    file.create_dataset(\"group1/dataset1\", shape=(10,))\n```", "```py\nfrom tensorflow import keras\n\n# Create model\nmodel = keras.models.Sequential([\n \tkeras.layers.Input(shape=(10,)),\n \tkeras.layers.Dense(1)\n])\n\nmodel.compile(optimizer=\"adam\", loss=\"mse\")\n\n# using the .h5 extension in the file name specifies that the model\n# should be saved in HDF5 format\nmodel.save(\"my_model.h5\")\n```", "```py\n...\nmodel = keras.models.load_model(\"my_model.h5\")\n\n# to check that the model has been successfully reconstructed\nprint(model.summary)\n```", "```py\n/\n/model_weights\n/model_weights/dense\n/model_weights/dense/dense\n/model_weights/dense/dense/bias:0\n/model_weights/dense/dense/kernel:0\n/model_weights/top_level_model_weights\n```", "```py\nimport h5py\n\nwith h5py.File(\"my_model.h5\", \"r\") as infile:\n    print(infile[\"/model_weights/dense/dense/kernel:0\"][:])\n```", "```py\n[[ 0.6872471 ]\n [-0.51016176]\n [-0.5604881 ]\n [ 0.3387223 ]\n [ 0.52146655]\n [-0.6960067 ]\n [ 0.38258582]\n [-0.05564564]\n [ 0.1450575 ]\n [-0.3391946 ]]\n```", "```py\nimport json\nimport h5py\n\nwith h5py.File(\"my_model.h5\", \"r\") as infile:\n    for key in infile.attrs.keys():\n        formatted = infile.attrs[key]\n        if key.endswith(\"_config\"):\n            formatted = json.dumps(json.loads(formatted), indent=4)\n        print(f\"{key}: {formatted}\")\n```", "```py\nbackend: tensorflow\nkeras_version: 2.7.0\nmodel_config: {\n    \"class_name\": \"Sequential\",\n    \"config\": {\n        \"name\": \"sequential\",\n        \"layers\": [\n            {\n                \"class_name\": \"InputLayer\",\n                \"config\": {\n                    \"batch_input_shape\": [\n                        null,\n                        10\n                    ],\n                    \"dtype\": \"float32\",\n                    \"sparse\": false,\n                    \"ragged\": false,\n                    \"name\": \"input_1\"\n                }\n            },\n            {\n                \"class_name\": \"Dense\",\n                \"config\": {\n                    \"name\": \"dense\",\n                    \"trainable\": true,\n                    \"dtype\": \"float32\",\n                    \"units\": 1,\n                    \"activation\": \"linear\",\n                    \"use_bias\": true,\n                    \"kernel_initializer\": {\n                        \"class_name\": \"GlorotUniform\",\n                        \"config\": {\n                            \"seed\": null\n                        }\n                    },\n                    \"bias_initializer\": {\n                        \"class_name\": \"Zeros\",\n                        \"config\": {}\n                    },\n                    \"kernel_regularizer\": null,\n                    \"bias_regularizer\": null,\n                    \"activity_regularizer\": null,\n                    \"kernel_constraint\": null,\n                    \"bias_constraint\": null\n                }\n            }\n        ]\n    }\n}\ntraining_config: {\n    \"loss\": \"mse\",\n    \"metrics\": null,\n    \"weighted_metrics\": null,\n    \"loss_weights\": null,\n    \"optimizer_config\": {\n        \"class_name\": \"Adam\",\n        \"config\": {\n            \"name\": \"Adam\",\n            \"learning_rate\": 0.001,\n            \"decay\": 0.0,\n            \"beta_1\": 0.9,\n            \"beta_2\": 0.999,\n            \"epsilon\": 1e-07,\n            \"amsgrad\": false\n        }\n    }\n}\n```"]