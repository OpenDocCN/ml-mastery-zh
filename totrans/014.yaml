- en: A Gentle Introduction to Hallucinations in Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://machinelearningmastery.com/a-gentle-introduction-to-hallucinations-in-large-language-models/](https://machinelearningmastery.com/a-gentle-introduction-to-hallucinations-in-large-language-models/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Large Language Models (LLMs) are known to have “hallucinations.” This is a
    behavior in that the model speaks false knowledge as if it is accurate. In this
    post, you will learn why hallucinations are a nature of an LLM. Specifically,
    you will learn:'
  prefs: []
  type: TYPE_NORMAL
- en: Why LLMs hallucinate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to make hallucinations work for you
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to mitigate hallucinations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Get started and apply ChatGPT** with my book [Maximizing Productivity with
    ChatGPT](https://machinelearningmastery.com/productivity-with-chatgpt/). It provides
    **real-world use cases** and **prompt examples** designed to get you using ChatGPT
    quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.![](../Images/b45994193a49a6c8b003fe66305f1902.png)
  prefs: []
  type: TYPE_NORMAL
- en: A Gentle Introduction to Hallucinations in Large Language Models
  prefs: []
  type: TYPE_NORMAL
- en: Picture generated by the author using Stable Diffusion. Some rights reserved.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This post is divided into three parts; they are
  prefs: []
  type: TYPE_NORMAL
- en: What are Hallucinations in Large Language Models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Hallucinations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mitigating Hallucinations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are Hallucinations in Large Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A large language model is a trained machine learning model that generates text
    based on the prompt you provided. The model’s training equipped it with some knowledge
    derived from the training data we provided. It is difficult to tell what knowledge
    a model remembers or what it does not. In fact, when a model generates text, it
    can’t tell if the generation is accurate.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of LLMs, “hallucination” refers to a phenomenon where the model
    generates text that is incorrect, nonsensical, or not real. Since LLMs are not
    databases or search engines, they would not cite where their response is based
    on. These models generate text as an extrapolation from the prompt you provided.
    The result of extrapolation is not necessarily supported by any training data,
    but is the most correlated from the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand hallucination, you can build a two-letter bigrams Markov model
    from some text: Extract a long piece of text, build a table of every pair of neighboring
    letters and tally the count. For example, “hallucinations in large language models”
    would produce “HA”, “AL”, “LL”, “LU”, etc. and there is one count of “LU” and
    two counts of “LA.” Now if you started with a prompt of “L”, you are twice as
    likely to produce “LA” than “LL” or “LS”. Then with a prompt of “LA”, you have
    an equal probability of producing “AL”, “AT”, “AR”, or “AN”. Then you may try
    with a prompt of “LAT” and continue this process. Eventually, this model invented
    a new word that didn’t exist. This is a result of the statistical patterns. You
    may say your Markov model hallucinated a spelling.'
  prefs: []
  type: TYPE_NORMAL
- en: Hallucination in LLMs is not much more complex than this, even if the model
    is much more sophisticated. From a high level, hallucination is caused by limited
    contextual understanding since the model is obligated to transform the prompt
    and the training data into an abstraction, in which some information may be lost.
    Moreover, noise in the training data may also provide a skewed statistical pattern
    that leads the model to respond in a way you do not expect.
  prefs: []
  type: TYPE_NORMAL
- en: Using Hallucinations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may consider hallucinations a feature in large language models. You want
    to see the models hallucinate if you want them to be creative. For example, if
    you ask ChatGPT or other Large Language Models to give you a plot of a fantasy
    story, you want it not to copy from any existing one but to generate a new character,
    scene, and storyline. This is possible only if the models are not looking up data
    that they were trained on.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason you may want hallucinations is when looking for diversity, for
    example, asking for ideas. It is like asking the models to brainstorm for you.
    You want to have derivations from the existing ideas that you may find in the
    training data, but not exactly the same. Hallucinations can help you explore different
    possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: Many language models have a “temperature” parameter. You can control the temperature
    in ChatGPT using the API instead of the web interface. This is a parameter of
    randomness. The higher temperature can introduce more hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating Hallucinations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Language models are not search engines or databases. Hallucinations are unavoidable.
    What is annoying is that the models generate text with mistakes that is hard to
    spot.
  prefs: []
  type: TYPE_NORMAL
- en: If the contaminated training data caused the hallucination, you can clean up
    the data and retrain the model. However, most models are too large to train on
    your own devices. Even fine-tuning an existing model may be impossible on commodity
    hardware. The best mitigation may be human intervention in the result, and asking
    the model to regenerate if it went gravely wrong.
  prefs: []
  type: TYPE_NORMAL
- en: The other solution to avoid hallucinations is controlled generation. It means
    providing enough details and constraints in the prompt to the model. Hence the
    model has limited freedom to hallucinate. The reason for prompt engineering is
    to specify the role and scenario to the model to guide the generation, so that
    it does not hallucinate unbounded.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this post, you learned how an LLM hallucinates. In particular,
  prefs: []
  type: TYPE_NORMAL
- en: Why hallucination would be useful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to limit the hallucination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s worth noting that while hallucination can be mitigated, but probably not
    completely eliminated. There is a trade-off between creativity and accuracy.
  prefs: []
  type: TYPE_NORMAL
