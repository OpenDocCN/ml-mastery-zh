- en: A Gentle Introduction to Hallucinations in Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对大语言模型中幻觉的温和介绍
- en: 原文：[https://machinelearningmastery.com/a-gentle-introduction-to-hallucinations-in-large-language-models/](https://machinelearningmastery.com/a-gentle-introduction-to-hallucinations-in-large-language-models/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/a-gentle-introduction-to-hallucinations-in-large-language-models/](https://machinelearningmastery.com/a-gentle-introduction-to-hallucinations-in-large-language-models/)
- en: 'Large Language Models (LLMs) are known to have “hallucinations.” This is a
    behavior in that the model speaks false knowledge as if it is accurate. In this
    post, you will learn why hallucinations are a nature of an LLM. Specifically,
    you will learn:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs）以“幻觉”而闻名。这是一种行为，即模型以准确的方式说出虚假的知识。在这篇文章中，你将了解为什么幻觉是 LLM 的一种特性。具体来说，你将了解：
- en: Why LLMs hallucinate
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么大语言模型会出现幻觉
- en: How to make hallucinations work for you
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何让幻觉为你服务
- en: How to mitigate hallucinations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何减少幻觉
- en: '**Get started and apply ChatGPT** with my book [Maximizing Productivity with
    ChatGPT](https://machinelearningmastery.com/productivity-with-chatgpt/). It provides
    **real-world use cases** and **prompt examples** designed to get you using ChatGPT
    quickly.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**开始使用并应用 ChatGPT**，可以参考我的书籍 [《用 ChatGPT 最大化生产力》](https://machinelearningmastery.com/productivity-with-chatgpt/)。这本书提供了**实际案例**和**提示示例**，旨在让你快速上手使用
    ChatGPT。'
- en: Let’s get started.![](../Images/b45994193a49a6c8b003fe66305f1902.png)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！[](../Images/b45994193a49a6c8b003fe66305f1902.png)
- en: A Gentle Introduction to Hallucinations in Large Language Models
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对大语言模型中幻觉的温和介绍
- en: Picture generated by the author using Stable Diffusion. Some rights reserved.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用 Stable Diffusion 生成。保留所有权利。
- en: Overview
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: This post is divided into three parts; they are
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文分为三个部分，它们是
- en: What are Hallucinations in Large Language Models
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大语言模型中的幻觉是什么
- en: Using Hallucinations
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用幻觉
- en: Mitigating Hallucinations
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少幻觉的影响
- en: What are Hallucinations in Large Language Models
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大语言模型中的幻觉是什么
- en: A large language model is a trained machine learning model that generates text
    based on the prompt you provided. The model’s training equipped it with some knowledge
    derived from the training data we provided. It is difficult to tell what knowledge
    a model remembers or what it does not. In fact, when a model generates text, it
    can’t tell if the generation is accurate.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型是一个经过训练的机器学习模型，它根据你提供的提示生成文本。模型的训练使其具备了从我们提供的训练数据中获得的一些知识。很难判断模型记住了什么知识或者忘记了什么。实际上，当模型生成文本时，它无法判断生成的内容是否准确。
- en: In the context of LLMs, “hallucination” refers to a phenomenon where the model
    generates text that is incorrect, nonsensical, or not real. Since LLMs are not
    databases or search engines, they would not cite where their response is based
    on. These models generate text as an extrapolation from the prompt you provided.
    The result of extrapolation is not necessarily supported by any training data,
    but is the most correlated from the prompt.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LLM 的上下文中，“幻觉”指的是模型生成的不正确、不合理或不真实的文本现象。由于 LLM 不是数据库或搜索引擎，它们不会引用其响应所基于的来源。这些模型生成的文本是基于你提供的提示进行外推的。外推的结果不一定得到任何训练数据的支持，但却是与提示最相关的内容。
- en: 'To understand hallucination, you can build a two-letter bigrams Markov model
    from some text: Extract a long piece of text, build a table of every pair of neighboring
    letters and tally the count. For example, “hallucinations in large language models”
    would produce “HA”, “AL”, “LL”, “LU”, etc. and there is one count of “LU” and
    two counts of “LA.” Now if you started with a prompt of “L”, you are twice as
    likely to produce “LA” than “LL” or “LS”. Then with a prompt of “LA”, you have
    an equal probability of producing “AL”, “AT”, “AR”, or “AN”. Then you may try
    with a prompt of “LAT” and continue this process. Eventually, this model invented
    a new word that didn’t exist. This is a result of the statistical patterns. You
    may say your Markov model hallucinated a spelling.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解幻觉，你可以从一些文本中构建一个二字母的马尔科夫模型：提取一段长文本，建立每对相邻字母的表格，并统计次数。例如，“大语言模型中的幻觉”会生成“HA”，“AL”，“LL”，“LU”等，其中“LU”出现了一次，“LA”出现了两次。现在，如果你从提示“L”开始，你产生“LA”的可能性是产生“LL”或“LS”的两倍。然后以提示“LA”开始，你有相等的概率产生“AL”，“AT”，“AR”或“AN”。然后你可以尝试以提示“LAT”继续这个过程。最终，这个模型发明了一个不存在的新词。这是统计模式的结果。你可以说你的马尔科夫模型出现了拼写幻觉。
- en: Hallucination in LLMs is not much more complex than this, even if the model
    is much more sophisticated. From a high level, hallucination is caused by limited
    contextual understanding since the model is obligated to transform the prompt
    and the training data into an abstraction, in which some information may be lost.
    Moreover, noise in the training data may also provide a skewed statistical pattern
    that leads the model to respond in a way you do not expect.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）中的幻觉现象并不比这更复杂，即使模型更为复杂。从高层次看，幻觉是由于有限的上下文理解造成的，因为模型必须将提示和训练数据转化为一种抽象，其中一些信息可能会丢失。此外，训练数据中的噪音也可能提供一种偏差的统计模式，导致模型做出你不期望的回应。
- en: Using Hallucinations
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用幻觉
- en: You may consider hallucinations a feature in large language models. You want
    to see the models hallucinate if you want them to be creative. For example, if
    you ask ChatGPT or other Large Language Models to give you a plot of a fantasy
    story, you want it not to copy from any existing one but to generate a new character,
    scene, and storyline. This is possible only if the models are not looking up data
    that they were trained on.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将幻觉视为大型语言模型中的一种特性。如果你希望模型具有创造力，你可能希望看到它们产生幻觉。例如，如果你要求 ChatGPT 或其他大型语言模型为你提供一个奇幻故事的情节，你希望它生成一个全新的角色、场景和情节，而不是从现有的故事中复制。这只有在模型没有查阅它们训练过的数据时才有可能。
- en: Another reason you may want hallucinations is when looking for diversity, for
    example, asking for ideas. It is like asking the models to brainstorm for you.
    You want to have derivations from the existing ideas that you may find in the
    training data, but not exactly the same. Hallucinations can help you explore different
    possibilities.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会在寻找多样性时希望出现幻觉，例如，要求提供创意。这就像是让模型为你进行头脑风暴。你希望从现有的训练数据中得到一些派生的想法，而不是完全相同的东西。幻觉可以帮助你探索不同的可能性。
- en: Many language models have a “temperature” parameter. You can control the temperature
    in ChatGPT using the API instead of the web interface. This is a parameter of
    randomness. The higher temperature can introduce more hallucinations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 许多语言模型都有一个“温度”参数。你可以通过 API 控制 ChatGPT 的温度，而不是通过网页界面。这是一个随机性的参数。更高的温度可以引入更多的幻觉。
- en: Mitigating Hallucinations
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 减少幻觉
- en: Language models are not search engines or databases. Hallucinations are unavoidable.
    What is annoying is that the models generate text with mistakes that is hard to
    spot.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型不是搜索引擎或数据库。幻觉是不可避免的。令人恼火的是，模型生成的文本中可能包含难以发现的错误。
- en: If the contaminated training data caused the hallucination, you can clean up
    the data and retrain the model. However, most models are too large to train on
    your own devices. Even fine-tuning an existing model may be impossible on commodity
    hardware. The best mitigation may be human intervention in the result, and asking
    the model to regenerate if it went gravely wrong.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果污染的训练数据导致了幻觉，你可以清理数据并重新训练模型。然而，大多数模型过于庞大，无法在自己的设备上进行训练。即使是微调现有模型也可能在普通硬件上不可行。最佳的解决方法可能是对结果进行人工干预，并在模型出现严重错误时要求其重新生成。
- en: The other solution to avoid hallucinations is controlled generation. It means
    providing enough details and constraints in the prompt to the model. Hence the
    model has limited freedom to hallucinate. The reason for prompt engineering is
    to specify the role and scenario to the model to guide the generation, so that
    it does not hallucinate unbounded.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 避免幻觉的另一种解决方案是受控生成。这意味着在提示中提供足够的细节和约束，以限制模型的自由度，从而减少幻觉的产生。提示工程的目的是为模型指定角色和场景，以指导生成过程，从而避免无边界的幻觉。
- en: Summary
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this post, you learned how an LLM hallucinates. In particular,
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，你了解了大型语言模型如何产生幻觉。特别是，
- en: Why hallucination would be useful
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么幻觉会有用
- en: How to limit the hallucination
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何限制幻觉
- en: It’s worth noting that while hallucination can be mitigated, but probably not
    completely eliminated. There is a trade-off between creativity and accuracy.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，虽然幻觉可以得到缓解，但可能无法完全消除。在创造力和准确性之间存在权衡。
