- en: Understanding LangChain LLM Output Parser
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://machinelearningmastery.com/understanding-langchain-llm-output-parser/](https://machinelearningmastery.com/understanding-langchain-llm-output-parser/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Understanding LangChain LLM Output Parser](../Images/da4313cfc413dea84a5ed4b24931a0ff.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by nikitabuida on [Freepik](https://www.freepik.com/free-photo/computer-program-code_1154343.htm#fromView=search&page=2&position=42&uuid=15ea92ef-e413-4e1e-8489-219875d1a2af)
  prefs: []
  type: TYPE_NORMAL
- en: The large Language Model, or LLM, has revolutionized how people work. By helping
    users generate the answer from a text prompt, LLM can do many things, such as
    answering questions, summarizing, planning events, and more.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are times when the output from LLM is not up to our standard.
    For example, the text generated could be thoroughly wrong and need further direction.
    This is where the LLM Output Parser could help.
  prefs: []
  type: TYPE_NORMAL
- en: By standardizing the output result with LangChain Output Parser, we can have
    some control over the output. So, how does it work? Let’s get into it.
  prefs: []
  type: TYPE_NORMAL
- en: Preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, we would rely on the LangChain packages, so we need to install
    them in the environment. To do that, you can use the following code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Also, we would use the OpenAI GPT model for text generation, so ensure that
    you have API access to them. You can get the API key from the OpenAI platform.
  prefs: []
  type: TYPE_NORMAL
- en: I would work in the Visual Studio Code IDE, but you could work in any preferred
    IDE. Create a file called `.env` within your project folder and put the OpenAI
    API key inside. It should look like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once everything is ready, we will move on to the central part of the article.
  prefs: []
  type: TYPE_NORMAL
- en: Output Parser
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use many types of output parsers from LangChain to standardize our LLM
    output. We would try several of them to understand the output parser better.
  prefs: []
  type: TYPE_NORMAL
- en: First, we would try Pydantic Parser. It’s an output parser that we could use
    to control and validate the output from the generated text. Let’s use them better
    with an example. Create a Python script in your IDE and then copy the code below
    to your script.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We initially imported the packages in the code above and loaded the OpenAI key
    with the `load_dotenv`. After that, we create a class called `MovieReview` which
    contains all the information output we want. The output would deliver output from
    the title, year, genre, rating, summary, and review. In each output, we define
    the description of the output we want.
  prefs: []
  type: TYPE_NORMAL
- en: From the output, we create a validator for the year and rating to ensure the
    result is not what we wanted. You can also add more validation mechanisms if required.
  prefs: []
  type: TYPE_NORMAL
- en: Then we create the prompt template that would accept our query input and the
    format it should be.
  prefs: []
  type: TYPE_NORMAL
- en: The last thing we do is create the model chain and pass the query to get our
    result. For note, the `chain` variable above accepts structure using “|” which
    is a unique method in the LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the result is similar to below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see the output follows the format we want and the result passes our
    validation method.
  prefs: []
  type: TYPE_NORMAL
- en: Pedantic Parser is the standard Output Parser we can use. We can use the other
    Output Parser if we already have a specific format in mind. For example, we can
    use the CSV Parser if we want the result only in the comma-separated items.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The result is a list with the values separated by the comma. You can expand
    the template in any way you like if the result is comma-separated.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also possible to get the output in datetime format. By changing the code
    and prompt, we can expect the result we want.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the result is in the datetime format.
  prefs: []
  type: TYPE_NORMAL
- en: That’s all about the LangChain LLM Output Parsers. You can visit their documentation
    to find the Output Parsers you require or use the Pydantic to structure it yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, we have learned about the LangChain Output Parser, which standardizes
    the generated text from LLM. We can use the Pydantic Parser to structure the LLM
    output and provide the result you want. There are many other Output Parsers from
    LangChain that could be suitable for your situation, such as the CSV parser and
    the Datetime parser.
  prefs: []
  type: TYPE_NORMAL
