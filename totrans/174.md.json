["```py\n!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n%pip install -q -U --pre triton\n%pip install -q accelerate transformers ftfy bitsandbytes natsort safetensors xformers\n```", "```py\nimport json\nimport os\n\nfrom google.colab import files\nimport shutil\n\nfrom natsort import natsorted\nfrom glob import glob\n\nimport torch\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline, DDIMScheduler\nfrom IPython.display import display\n```", "```py\n!mkdir -p ~/.huggingface\nHUGGINGFACE_TOKEN = \"put your token here\"\n!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token\n```", "```py\nMODEL_NAME = \"runwayml/stable-diffusion-v1-5\"\nOUTPUT_DIR = \"/content/stable_diffusion_weights/zwx\"\n\n# Create output directory\n!mkdir -p $OUTPUT_DIR\n```", "```py\n# The concepts_list is a list of concepts/subject, each represented as a dictionary\nconcepts_list = [\n    {\n        \"instance_prompt\":   \"photo of zwx dog\",\n        \"class_prompt\":      \"photo of a dog\",\n        \"instance_data_dir\": \"/content/data/zwx\",\n        \"class_data_dir\":    \"/content/data/dog\"\n    },\n]\n\n# Create a directory for each concept according to its instance_data_dir\nfor c in concepts_list:\n    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n\n#Dump the concepts_list to a JSON file\nwith open(\"concepts_list.json\", \"w\") as f:\n    json.dump(concepts_list, f, indent=4)\n```", "```py\n!python3 train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n  --output_dir=$OUTPUT_DIR \\\n  --revision=\"fp16\" \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --seed=1337 \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --train_text_encoder \\\n  --mixed_precision=\"fp16\" \\\n  --use_8bit_adam \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=1e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --num_class_images=50 \\\n  --sample_batch_size=4 \\\n  --max_train_steps=800 \\\n  --save_interval=10000 \\\n  --save_sample_prompt=\"photo of zwx dog\" \\\n  --concepts_list=\"concepts_list.json\"\n```", "```py\nWEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\nckpt_path = WEIGHTS_DIR + \"/model.ckpt\"\n\nhalf_arg = \"\"\nfp16 = True\nif fp16:\n    half_arg = \"--half\"\n!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\nprint(f\"[*] Converted ckpt saved at {ckpt_path}\")\n```", "```py\nmodel_path = WEIGHTS_DIR\n\npipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None,\n                                               torch_dtype=torch.float16\n                                              ).to(\"cuda\")\npipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\npipe.enable_xformers_memory_efficient_attention()\n\ng_cuda = torch.Generator(device='cuda')\nseed = 52362\ng_cuda.manual_seed(seed)\n```", "```py\nprompt = \"photo of zwx dog in a bucket wearing 3d glasses\"\nnegative_prompt = \"\"\nnum_samples = 4\nguidance_scale = 7.5\nnum_inference_steps = 24\nheight = 512\nwidth = 512\n\nwith autocast(\"cuda\"), torch.inference_mode():\n    images = pipe(\n        prompt,\n        height=height,\n        width=width,\n        negative_prompt=negative_prompt,\n        num_images_per_prompt=num_samples,\n        num_inference_steps=num_inference_steps,\n        guidance_scale=guidance_scale,\n        generator=g_cuda\n    ).images\n\nfor img in images:\n    display(img)\n\n# Free runtime memory\nexit()\n```"]