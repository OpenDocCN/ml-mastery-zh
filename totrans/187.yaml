- en: Brief Introduction to Diffusion Models for Image Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像生成扩散模型简介
- en: 原文：[https://machinelearningmastery.com/brief-introduction-to-diffusion-models-for-image-generation/](https://machinelearningmastery.com/brief-introduction-to-diffusion-models-for-image-generation/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/brief-introduction-to-diffusion-models-for-image-generation/](https://machinelearningmastery.com/brief-introduction-to-diffusion-models-for-image-generation/)
- en: The advance of generative machine learning models makes computers capable of
    creative work. In the scope of drawing pictures, there are a few notable models
    that allow you to convert a textual description into an array of pixels. The most
    powerful models today are part of the family of diffusion models. In this post,
    you will learn how this kind of model works and how you can control its output.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式机器学习模型的进步使得计算机能够进行创造性工作。在绘画范围内，有几个显著的模型允许您将文本描述转换为像素数组。如今最强大的模型之一属于扩散模型家族。在本文中，您将了解这种模型的工作原理以及如何控制其输出。
- en: '**Kick-start your project** with my book [Mastering Digital Art with Stable
    Diffusion](https://machinelearningmastery.com/mastering-digital-art-with-stable-diffusion/).
    It provides **self-study tutorials** with **working code**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我的书[《稳定扩散的数字艺术》](https://machinelearningmastery.com/mastering-digital-art-with-stable-diffusion/)，**启动您的项目**。它提供了带有**工作代码**的**自学教程**。
- en: Let’s get started.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: '![](../Images/0f3ef41aa2051eaf506a110b6bc983fe.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f3ef41aa2051eaf506a110b6bc983fe.png)'
- en: Brief Introduction to Diffusion Models for Image Generation
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图像生成扩散模型简介
- en: Photo by [Dhruvin Pandya](https://unsplash.com/photos/a-mountain-covered-in-snow-under-a-cloudy-sky-IXSnNQZ8ufY).
    Some rights reserved.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Dhruvin Pandya](https://unsplash.com/photos/a-mountain-covered-in-snow-under-a-cloudy-sky-IXSnNQZ8ufY)提供。部分权利保留。
- en: Overview
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'This post is in three parts; they are:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文分为三部分，它们是：
- en: Workflow of Diffusion Models
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩散模型工作流程
- en: Variation in Output
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出的变化
- en: How It was Trained
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是如何训练的
- en: Workflow of Diffusion Models
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩散模型工作流程
- en: Considering the goal of converting a description of a picture in text into a
    picture in an array of pixels, the machine learning model should have its output
    as an array of RGB values. But how should you provide text input to a model, and
    how is the conversion performed?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑将图片描述文本转换为像素数组的目标，机器学习模型的输出应该是一组RGB值。但是您应该如何向模型提供文本输入，以及如何进行转换呢？
- en: Since the text input describes the output, the model needs to **understand**
    what the text means. The better such a description is understood, the more accurate
    your model can generate the output. Hence, the trivial solution of treating the
    text as a character string does not work well. You need a module that can understand
    natural language, and the state-of-the-art solution would be to represent the
    input as embedding vectors.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文本输入描述了输出，模型需要**理解**文本的含义。对这种描述理解得越好，您的模型生成输出就越精确。因此，将文本视为字符串的琐碎解决方案效果不佳。您需要一个能理解自然语言的模块，而当今的最先进解决方案是将输入表示为嵌入向量。
- en: Embedding representation of input text not only allows you to distill the **meaning**
    of the text but also provides a uniform shape of the input since the text of various
    lengths can be transformed into a standard size of tensors.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入表示的输入文本不仅允许您提取文本的**含义**，还提供了输入的统一形状，因为可以将各种长度的文本转换为张量的标准大小。
- en: There are multiple ways to convert a tensor of the embedding representation
    into pixels. Recall how the generative adversarial network (GAN) works; you should
    notice this is in a similar structure, namely, the input (text) is converted into
    a latent structure (embedding), and then converted into the output (pixels).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以将嵌入表示的张量转换为像素。回想一下生成对抗网络（GAN）的工作原理；您应该注意到这是类似的结构，即输入（文本）被转换为潜在结构（嵌入），然后转换为输出（像素）。
- en: 'Diffusion models are a family of neural network models that consider embedding
    to be a **hint** to restore a picture from random pixels. Below is a figure from
    the paper by Rombach et al. to illustrate this workflow:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型是一类神经网络模型，考虑到嵌入是从随机像素恢复图片的**提示**。下面是来自Rombach等人论文的图示，以说明这一工作流程：
- en: '![](../Images/4b5e0eec24a9e8aca0e6b3943b472e8a.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b5e0eec24a9e8aca0e6b3943b472e8a.png)'
- en: Stable Diffusion architecture. Figure from Rombach et al (2021)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散架构。Rombach等人（2021）的图。
- en: In this figure, the workflow is from right to left. The output at the left is
    to convert a tensor into pixel space, using a **decoder** network denoted with
    $\mathcal{D}$. The input at the right is converted into an embedding $\tau_\theta$
    which is used as the **conditioning tensor**. The key structure is in the latent
    space in the middle. The generation part is at the lower half of the green box,
    which converts $z_T$ into $z_{T-1}$ using a **denoising network** $\epsilon_\theta$.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，工作流程是从右到左。左侧的输出是将张量转换为像素空间，使用标记为 $\mathcal{D}$ 的 **解码器** 网络。右侧的输入转换为嵌入
    $\tau_\theta$，用于作为 **条件张量**。关键结构位于中间的潜在空间。生成部分位于绿色框的下半部分，它使用 **去噪网络** $\epsilon_\theta$
    将 $z_T$ 转换为 $z_{T-1}$。
- en: The denoising network takes an input tensor $z_T$ and the embedding $\tau_\theta$,
    outputs a tensor $z_{T-1}$. The output is a tensor that is “better” than the input
    in the sense that it matches better to the embedding. In its simplest form, the
    decoder $\mathcal{D}$ does nothing but copy over the output from the latent space.
    The input and output tensors $z_T$ and $z_{T-1}$ of the denoising network are
    arrays of RGB pixels, which the network make it **less noisy**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪网络接受输入张量 $z_T$ 和嵌入 $\tau_\theta$，输出张量 $z_{T-1}$。输出的张量在某种意义上“比”输入更好，因为它与嵌入的匹配度更高。在最简单的形式下，解码器
    $\mathcal{D}$ 只是简单地从潜在空间复制输出。去噪网络的输入和输出张量 $z_T$ 和 $z_{T-1}$ 是 RGB 像素的数组，网络使其 **减少噪声**。
- en: 'It is called the denoising network because it assumes the embedding can describe
    the output perfectly, but the input and the output differ because some pixels
    are replaced by random values. The network model aimed at removing such random
    values and restoring the original pixel. It is a difficult task, but the model
    assumes the noise pixels are added uniformly, and the noise follows a Gaussian
    model. Hence, the model can be reused many times, each producing an improvement
    in the input. Below is an illustration from the paper by Ho et al. for such a
    concept:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 它被称为去噪网络，因为它假设嵌入可以完美描述输出，但输入和输出不同，因为一些像素被随机值替代。网络模型旨在去除这些随机值并恢复原始像素。这是一个困难的任务，但模型假设噪声像素是均匀添加的，噪声遵循高斯模型。因此，该模型可以多次重复使用，每次都能改善输入。以下是
    Ho 等人论文中的一个概念插图：
- en: '![](../Images/22e279186296f5fb5e2121014a67a000.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22e279186296f5fb5e2121014a67a000.png)'
- en: Denoising an image. Figure from Ho et al. (2020)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪图像。图来自 Ho 等人（2020）
- en: 'Because of this structure, the denoising network assumes the input $z_T$ and
    output $z_{T-1}$ are in the same shape, such that the network can be repeated
    until the final output $z$ is produced. The denoising U-net block in the previous
    figure is to keep the input and output of the same shape. The denoising block
    conceptually is to perform the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这种结构，去噪网络假设输入 $z_T$ 和输出 $z_{T-1}$ 具有相同的形状，使得网络可以重复使用，直到生成最终输出 $z$。前面的图中的去噪
    U-net 块保持输入和输出的形状相同。去噪块在概念上是执行以下操作：
- en: $$
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: \begin{aligned}
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: \begin{aligned}
- en: w_t &= \textrm{NoisePredictor}(z_t, \tau_\theta, t) \\
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: w_t &= \textrm{NoisePredictor}(z_t, \tau_\theta, t) \\
- en: z_{t-1} &= z_t – w_t
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: z_{t-1} &= z_t – w_t
- en: \end{aligned}
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: \end{aligned}
- en: $$
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: that is, the noise component $w_t$ is predicted from the noisy image $z_t$,
    the conditioning tensor $\tau_\theta$, and the step count $t$. The noise predictor
    then based on $t$ to estimate the level of noise in $z_t$ conditioned on the what
    the final image $z=z_0$ should be as described by the tensor $\tau_theta$. The
    value of $t$ is helpful to the predictor because the larger the value, the more
    noise it is in $z_t$.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 即，噪声成分 $w_t$ 从噪声图像 $z_t$、条件张量 $\tau_\theta$ 和步骤计数 $t$ 中预测。噪声预测器根据 $t$ 来估计 $z_t$
    中的噪声水平，这一水平以最终图像 $z=z_0$ 应该是什么由张量 $\tau_\theta$ 描述。$t$ 的值对预测器有帮助，因为值越大，$z_t$ 中的噪声越多。
- en: Subtracting the noise from $z_t$ will be the denoised image $z_{t-1}$, which
    can be feed into the denoising network again until $z=z_0$ is produced. The number
    of times $T$ this network processed the tensor is a design parameter to the entire
    diffusion model. Because in this model, the noise is formulated as Gaussian, a
    part of the decoder $\mathcal{D}$ is to convert the latent space tensor $z$ into
    a three-channel tensor and quantize floating point values into RGB.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从 $z_t$ 中减去噪声将得到去噪图像 $z_{t-1}$，可以再次输入去噪网络，直到生成 $z=z_0$。网络处理张量的次数 $T$ 是整个扩散模型的设计参数。因为在这个模型中，噪声被建模为高斯噪声，解码器
    $\mathcal{D}$ 的一部分是将潜在空间张量 $z$ 转换为三通道张量，并将浮点值量化为 RGB。
- en: Variation in Output
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出的变化
- en: Once the neural network is trained, the weights in each layer are fixed, and
    the output is deterministic as long as the input is deterministic. However, in
    this diffusion model workflow, the input is the text that will be converted into
    embedding vectors. The denoising model takes an additional input, the initial
    $z_T$ tensor in the latent space. This is usually generated randomly, such as
    by sampling a Gaussian distribution and filling in the tensor of the shape that
    the denoising network expects. With a different starting tensor, you get a different
    output. This is how you can generate different output by the same input text.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦神经网络训练完成，每一层的权重将固定，只要输入是确定的，输出也是确定的。然而，在这个扩散模型工作流程中，输入是将被转换为嵌入向量的文本。去噪模型需要额外的输入，即潜在空间中的初始$z_T$张量。这通常是随机生成的，例如通过采样高斯分布并填充去噪网络预期形状的张量。不同的起始张量会产生不同的输出。这就是如何通过相同的输入文本生成不同的输出。
- en: Indeed, the reality is much more complicated than this. Remember that the denoising
    network is run in multiple steps, each step aimed at improving the output a bit
    until the pristine final output is produced. The network can take an additional
    hint of which step it is in (e.g., step 5 of a total of 10 steps scheduled). Gaussian
    noise is parameterized by its mean and variation, which you can provide a function
    to calculate. The better you can model the noise expected at each step, the better
    the denoising network can remove the noise. In the **Stable Diffusion** model,
    the denoising network needs a sample of random noise reflecting the noise intensity
    as in that step to predict the noise component from an noisy image. Algorithm
    2 in the figure below shows this, which such randomness is introduced as $\sigma_t\mathbf{z}$.
    You can select the **sampler** for such a purpose. Some samplers converge faster
    than others (i.e., you use fewer steps). You can also consider the latent space
    model as a **variation autoencoder**, in which the variations introduced affect
    the output as well.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，现实要复杂得多。请记住，去噪网络在多个步骤中运行，每一步旨在改善输出，直到产生完美的最终输出。网络可以获得额外的提示，指示它处于哪个步骤（例如，总共10步中的第5步）。高斯噪声由其均值和方差参数化，你可以提供一个函数来计算。你能够更好地建模每一步预期的噪声，去噪网络就能更好地去除噪声。在**稳定扩散**模型中，去噪网络需要一个反映该步骤噪声强度的随机噪声样本，以预测来自加噪图像的噪声部分。下图中的算法2显示了这一点，这种随机性被引入为$\sigma_t\mathbf{z}$。你可以选择**采样器**来实现这一目的。一些采样器比其他的收敛更快（即，你使用的步骤更少）。你还可以将潜在空间模型视为**变分自编码器**，其中引入的变异也会影响输出。
- en: How It Was Trained
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练过程
- en: Taking the Stable Diffusion model as an example, you can see that the most important
    component in the workflow is the denosing model in the latent space. Indeed, the
    input model is not trained but adopts an existing text embedding model, such as
    BERT or T5\. The output model can also be an off-the-shelf model, such as a super-resolution
    model that converts a 256×256 pixel image into a 512×512 pixel image.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以稳定扩散模型为例，你可以看到工作流程中最重要的组件是潜在空间中的去噪模型。确实，输入模型并未经过训练，而是采用现有的文本嵌入模型，例如BERT或T5。输出模型也可以是现成的模型，例如将256×256像素图像转换为512×512像素图像的超分辨率模型。
- en: 'Training the denoising network model conceptually is as follows: You pick an
    image and add some noise to it. Then you created a tuple of three components:
    The image, the noise, and the noisy image. The network is then trained to estimate
    the noise part of the noisy image. The noise part can vary by different weight
    of adding noise to the pixels, as well as the Gaussian parameters to generate
    the noise. The training algorithm is depicted in Algorithm 1 as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对去噪网络模型的概念性训练过程如下：你选择一张图像并添加一些噪声。然后，你创建一个由三部分组成的元组：图像、噪声和加噪图像。网络随后被训练以估计加噪图像中的噪声部分。噪声部分可以通过不同的像素噪声权重以及生成噪声的高斯参数来变化。训练算法如算法1所示：
- en: '![](../Images/45877fe1f3934923b63f80b48e3b0d51.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45877fe1f3934923b63f80b48e3b0d51.png)'
- en: Training and sampling algorithms. Figure from Ho et al (2020)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和采样算法。图自Ho等人（2020）
- en: Since the denoising network assumes the noise is **additive**, the noise predicted
    can be subtracted from the input to produce the output. As described above, the
    denoising network takes not only the image as the input but also the embedding
    that reflects the text input. The embedding plays a role in that the noise to
    detect is conditioned to the embedding, which means the output should related
    to the embedding, and the noise to detect should fit a conditional probability
    distribution. Technically, the image and the embedding meet each other using a
    cross-attention mechanism in the latent model, which is not shown in the skeleton
    of algorithms above.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于去噪网络假设噪声是**附加的**，可以从输入中减去预测的噪声以产生输出。如上所述，去噪网络不仅以图像作为输入，还以反映文本输入的嵌入作为输入。嵌入在于，用于检测的噪声被条件化于嵌入中，这意味着输出应该与嵌入相关，而检测的噪声应适合条件概率分布。技术上讲，图像和嵌入在潜在模型中通过交叉注意力机制相互遇到，这在上述算法的骨架中未显示。
- en: There is a lot of vocabulary to describe a picture, and you can imagine it is
    not easy to make the network model learn how to correlate a word to a picture.
    It is reported that Stable Diffusion model, for example, was trained with 2.3
    billion images and consumed 150 thousand GPU hours, using the [LAION-5B dataset](https://laion.ai/blog/laion-5b/)
    (which has 5.85 billion images with text descriptions). However, once the model
    is trained, you can use it on a commodity computer such as your laptop.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 描述图片的词汇很多，想象一下让网络模型学习如何将一个词与图片相关联并不容易。例如，报道称，稳定扩散模型是通过23亿张图片进行训练的，并消耗了15万个GPU小时，使用了[LAION-5B数据集](https://laion.ai/blog/laion-5b/)（该数据集拥有58.5亿张带文本描述的图片）。然而，一旦模型训练完成，您可以在像您的笔记本电脑这样的商品计算机上使用它。
- en: Further Readings
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Below are several papers that created the diffusion models for image generation
    as we know it today:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是几篇创建了今天我们所知的扩散模型图像生成的论文：
- en: “High-Resolution Image Synthesis with Latent Diffusion Models” by Rombach, Blattmann,
    Lorenz, Esser, and Ommer (2021)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “高分辨率图像合成与潜在扩散模型” by Rombach, Blattmann, Lorenz, Esser, and Ommer (2021)
- en: '[arXiv 2112.10752](https://arxiv.org/abs/2112.10752)'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[arXiv 2112.10752](https://arxiv.org/abs/2112.10752)'
- en: “Denoising Diffusion Probabilistic Models” by Ho, Jain, and Abbeel (2020)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “去噪扩散概率模型” by Ho, Jain, and Abbeel (2020)
- en: '[arXiv 2006.11239](https://arxiv.org/abs/2006.11239)'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[arXiv 2006.11239](https://arxiv.org/abs/2006.11239)'
- en: “Diffusion Models Beat GANs on Image Synthesis” by Dhariwal and Nichol (2021)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “扩散模型在图像合成上击败了GANs” by Dhariwal and Nichol (2021)
- en: '[arXiv 2105.05233](https://arxiv.org/abs/2105.05233)'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[arXiv 2105.05233](https://arxiv.org/abs/2105.05233)'
- en: “Improved Denoising Diffusion Probabilistic Models” by Nichol and Dhariwal (2021)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “改进的去噪扩散概率模型” by Nichol and Dhariwal (2021)
- en: '[arXiv 2102.09672](https://arxiv.org/abs/2102.09672)'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[arXiv 2102.09672](https://arxiv.org/abs/2102.09672)'
- en: Summary
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this post, you saw an overview of how a diffusion model works. In particular,
    you learned
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，您看到了扩散模型如何工作的概述。特别是，您学到了
- en: The image generation workflow has multiple steps, the diffusion model works
    at the latent space as a denoising neural network.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像生成工作流程具有多个步骤，扩散模型在潜在空间中作为去噪神经网络工作。
- en: Image generation is achieved by starting from a noisy image, which is an array
    of randomly generated pixels.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像生成是通过从噪声图像开始实现的，这是一个由随机生成的像素数组组成的图像。
- en: In each step in the latent space, the denoising network removes some noise,
    conditioned to the input text description of the final image in the form of embedding
    vectors.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在潜在空间的每个步骤中，去噪网络消除一些噪声，条件是最终图像的输入文本描述的嵌入向量形式。
- en: The output image is obtained by decoding the output from the latent space.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出图像通过解码从潜在空间输出来获取。
