["```py\npip install skorch\n```", "```py\nclass MyClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        ...\n\n    def forward(self, x):\n        ...\n        return x\n\n# create the skorch wrapper\nmodel = NeuralNetClassifier(\n    module=MyClassifier\n)\n```", "```py\nmodel = NeuralNetClassifier(\n    module=MyClassifier,\n    max_epochs=150,\n    batch_size=10\n)\n```", "```py\nimport torch.nn as nn\nfrom skorch import NeuralNetClassifier\n\nclass SonarClassifier(nn.Module):\n    def __init__(self, n_layers=3):\n        super().__init__()\n        self.layers = []\n        self.acts = []\n        for i in range(n_layers):\n            self.layers.append(nn.Linear(60, 60))\n            self.acts.append(nn.ReLU())\n            self.add_module(f\"layer{i}\", self.layers[-1])\n            self.add_module(f\"act{i}\", self.acts[-1])\n        self.output = nn.Linear(60, 1)\n\n    def forward(self, x):\n        for layer, act in zip(self.layers, self.acts):\n            x = act(layer(x))\n        x = self.output(x)\n        return x\n\nmodel = NeuralNetClassifier(\n    module=SonarClassifier,\n    max_epochs=150,\n    batch_size=10,\n    module__n_layers=2\n)\n```", "```py\nprint(model.initialize())\n```", "```py\n<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n  module_=SonarClassifier(\n    (layer0): Linear(in_features=60, out_features=60, bias=True)\n    (act0): ReLU()\n    (layer1): Linear(in_features=60, out_features=60, bias=True)\n    (act1): ReLU()\n    (output): Linear(in_features=60, out_features=1, bias=True)\n  ),\n)\n```", "```py\nparam_grid = {\n    'epochs': [10,20,30]\n}\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, Y)\n```", "```py\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom skorch import NeuralNetClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# load the dataset, split into input (X) and output (y) variables\ndataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\nX = dataset[:,0:8]\ny = dataset[:,8]\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n\n# PyTorch classifier\nclass PimaClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = nn.Linear(8, 12)\n        self.act = nn.ReLU()\n        self.output = nn.Linear(12, 1)\n        self.prob = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.act(self.layer(x))\n        x = self.prob(self.output(x))\n        return x\n\n# create model with skorch\nmodel = NeuralNetClassifier(\n    PimaClassifier,\n    criterion=nn.BCELoss,\n    optimizer=optim.Adam,\n    verbose=False\n)\n\n# define the grid search parameters\nparam_grid = {\n    'batch_size': [10, 20, 40, 60, 80, 100],\n    'max_epochs': [10, 50, 100]\n}\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, y)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n```", "```py\nBest: 0.714844 using {'batch_size': 10, 'max_epochs': 100}\n0.665365 (0.020505) with: {'batch_size': 10, 'max_epochs': 10}\n0.588542 (0.168055) with: {'batch_size': 10, 'max_epochs': 50}\n0.714844 (0.032369) with: {'batch_size': 10, 'max_epochs': 100}\n0.671875 (0.022326) with: {'batch_size': 20, 'max_epochs': 10}\n0.696615 (0.008027) with: {'batch_size': 20, 'max_epochs': 50}\n0.714844 (0.019918) with: {'batch_size': 20, 'max_epochs': 100}\n0.666667 (0.009744) with: {'batch_size': 40, 'max_epochs': 10}\n0.687500 (0.033603) with: {'batch_size': 40, 'max_epochs': 50}\n0.707031 (0.024910) with: {'batch_size': 40, 'max_epochs': 100}\n0.667969 (0.014616) with: {'batch_size': 60, 'max_epochs': 10}\n0.694010 (0.036966) with: {'batch_size': 60, 'max_epochs': 50}\n0.694010 (0.042473) with: {'batch_size': 60, 'max_epochs': 100}\n0.670573 (0.023939) with: {'batch_size': 80, 'max_epochs': 10}\n0.674479 (0.020752) with: {'batch_size': 80, 'max_epochs': 50}\n0.703125 (0.026107) with: {'batch_size': 80, 'max_epochs': 100}\n0.680990 (0.014382) with: {'batch_size': 100, 'max_epochs': 10}\n0.670573 (0.013279) with: {'batch_size': 100, 'max_epochs': 50}\n0.687500 (0.017758) with: {'batch_size': 100, 'max_epochs': 100}\n```", "```py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom skorch import NeuralNetClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# load the dataset, split into input (X) and output (y) variables\ndataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\nX = dataset[:,0:8]\ny = dataset[:,8]\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n\n# PyTorch classifier\nclass PimaClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = nn.Linear(8, 12)\n        self.act = nn.ReLU()\n        self.output = nn.Linear(12, 1)\n        self.prob = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.act(self.layer(x))\n        x = self.prob(self.output(x))\n        return x\n\n# create model with skorch\nmodel = NeuralNetClassifier(\n    PimaClassifier,\n    criterion=nn.BCELoss,\n    max_epochs=100,\n    batch_size=10,\n    verbose=False\n)\n\n# define the grid search parameters\nparam_grid = {\n    'optimizer': [optim.SGD, optim.RMSprop, optim.Adagrad, optim.Adadelta,\n                  optim.Adam, optim.Adamax, optim.NAdam],\n}\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, y)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n```", "```py\nBest: 0.721354 using {'optimizer': <class 'torch.optim.adamax.Adamax'>}\n0.674479 (0.036828) with: {'optimizer': <class 'torch.optim.sgd.SGD'>}\n0.700521 (0.043303) with: {'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n0.682292 (0.027126) with: {'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n0.572917 (0.051560) with: {'optimizer': <class 'torch.optim.adadelta.Adadelta'>}\n0.714844 (0.030758) with: {'optimizer': <class 'torch.optim.adam.Adam'>}\n0.721354 (0.019225) with: {'optimizer': <class 'torch.optim.adamax.Adamax'>}\n0.709635 (0.024360) with: {'optimizer': <class 'torch.optim.nadam.NAdam'>}\n```", "```py\noptimizer = optim.SGD(lr=0.001, momentum=0.9)\n```", "```py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom skorch import NeuralNetClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# load the dataset, split into input (X) and output (y) variables\ndataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\nX = dataset[:,0:8]\ny = dataset[:,8]\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n\n# PyTorch classifier\nclass PimaClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = nn.Linear(8, 12)\n        self.act = nn.ReLU()\n        self.output = nn.Linear(12, 1)\n        self.prob = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.act(self.layer(x))\n        x = self.prob(self.output(x))\n        return x\n\n# create model with skorch\nmodel = NeuralNetClassifier(\n    PimaClassifier,\n    criterion=nn.BCELoss,\n    optimizer=optim.SGD,\n    max_epochs=100,\n    batch_size=10,\n    verbose=False\n)\n\n# define the grid search parameters\nparam_grid = {\n    'optimizer__lr': [0.001, 0.01, 0.1, 0.2, 0.3],\n    'optimizer__momentum': [0.0, 0.2, 0.4, 0.6, 0.8, 0.9],\n}\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, y)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n```", "```py\nBest: 0.682292 using {'optimizer__lr': 0.001, 'optimizer__momentum': 0.9}\n0.648438 (0.016877) with: {'optimizer__lr': 0.001, 'optimizer__momentum': 0.0}\n0.671875 (0.017758) with: {'optimizer__lr': 0.001, 'optimizer__momentum': 0.2}\n0.674479 (0.022402) with: {'optimizer__lr': 0.001, 'optimizer__momentum': 0.4}\n0.677083 (0.011201) with: {'optimizer__lr': 0.001, 'optimizer__momentum': 0.6}\n0.679688 (0.027621) with: {'optimizer__lr': 0.001, 'optimizer__momentum': 0.8}\n0.682292 (0.026557) with: {'optimizer__lr': 0.001, 'optimizer__momentum': 0.9}\n0.671875 (0.019918) with: {'optimizer__lr': 0.01, 'optimizer__momentum': 0.0}\n0.648438 (0.024910) with: {'optimizer__lr': 0.01, 'optimizer__momentum': 0.2}\n0.546875 (0.143454) with: {'optimizer__lr': 0.01, 'optimizer__momentum': 0.4}\n0.567708 (0.153668) with: {'optimizer__lr': 0.01, 'optimizer__momentum': 0.6}\n0.552083 (0.141790) with: {'optimizer__lr': 0.01, 'optimizer__momentum': 0.8}\n0.451823 (0.144561) with: {'optimizer__lr': 0.01, 'optimizer__momentum': 0.9}\n0.348958 (0.001841) with: {'optimizer__lr': 0.1, 'optimizer__momentum': 0.0}\n0.450521 (0.142719) with: {'optimizer__lr': 0.1, 'optimizer__momentum': 0.2}\n0.450521 (0.142719) with: {'optimizer__lr': 0.1, 'optimizer__momentum': 0.4}\n0.450521 (0.142719) with: {'optimizer__lr': 0.1, 'optimizer__momentum': 0.6}\n0.348958 (0.001841) with: {'optimizer__lr': 0.1, 'optimizer__momentum': 0.8}\n0.348958 (0.001841) with: {'optimizer__lr': 0.1, 'optimizer__momentum': 0.9}\n0.444010 (0.136265) with: {'optimizer__lr': 0.2, 'optimizer__momentum': 0.0}\n0.450521 (0.142719) with: {'optimizer__lr': 0.2, 'optimizer__momentum': 0.2}\n0.348958 (0.001841) with: {'optimizer__lr': 0.2, 'optimizer__momentum': 0.4}\n0.552083 (0.141790) with: {'optimizer__lr': 0.2, 'optimizer__momentum': 0.6}\n0.549479 (0.142719) with: {'optimizer__lr': 0.2, 'optimizer__momentum': 0.8}\n0.651042 (0.001841) with: {'optimizer__lr': 0.2, 'optimizer__momentum': 0.9}\n0.552083 (0.141790) with: {'optimizer__lr': 0.3, 'optimizer__momentum': 0.0}\n0.348958 (0.001841) with: {'optimizer__lr': 0.3, 'optimizer__momentum': 0.2}\n0.450521 (0.142719) with: {'optimizer__lr': 0.3, 'optimizer__momentum': 0.4}\n0.552083 (0.141790) with: {'optimizer__lr': 0.3, 'optimizer__momentum': 0.6}\n0.450521 (0.142719) with: {'optimizer__lr': 0.3, 'optimizer__momentum': 0.8}\n0.450521 (0.142719) with: {'optimizer__lr': 0.3, 'optimizer__momentum': 0.9}\n```", "```py\n# PyTorch classifier\nclass PimaClassifier(nn.Module):\n    def __init__(self, weight_init=torch.nn.init.xavier_uniform_):\n        super().__init__()\n        self.layer = nn.Linear(8, 12)\n        self.act = nn.ReLU()\n        self.output = nn.Linear(12, 1)\n        self.prob = nn.Sigmoid()\n        # manually init weights\n        weight_init(self.layer.weight)\n        weight_init(self.output.weight)\n\n    def forward(self, x):\n        x = self.act(self.layer(x))\n        x = self.prob(self.output(x))\n        return x\n```", "```py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.optim as optim\nfrom skorch import NeuralNetClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# load the dataset, split into input (X) and output (y) variables\ndataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\nX = dataset[:,0:8]\ny = dataset[:,8]\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n\n# PyTorch classifier\nclass PimaClassifier(nn.Module):\n    def __init__(self, weight_init=init.xavier_uniform_):\n        super().__init__()\n        self.layer = nn.Linear(8, 12)\n        self.act = nn.ReLU()\n        self.output = nn.Linear(12, 1)\n        self.prob = nn.Sigmoid()\n        # manually init weights\n        weight_init(self.layer.weight)\n        weight_init(self.output.weight)\n\n    def forward(self, x):\n        x = self.act(self.layer(x))\n        x = self.prob(self.output(x))\n        return x\n\n# create model with skorch\nmodel = NeuralNetClassifier(\n    PimaClassifier,\n    criterion=nn.BCELoss,\n    optimizer=optim.Adamax,\n    max_epochs=100,\n    batch_size=10,\n    verbose=False\n)\n\n# define the grid search parameters\nparam_grid = {\n    'module__weight_init': [init.uniform_, init.normal_, init.zeros_,\n                           init.xavier_normal_, init.xavier_uniform_,\n                           init.kaiming_normal_, init.kaiming_uniform_]\n}\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, y)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n```", "```py\nBest: 0.697917 using {'module__weight_init': <function kaiming_uniform_ at 0x112020c10>}\n0.348958 (0.001841) with: {'module__weight_init': <function uniform_ at 0x1120204c0>}\n0.602865 (0.061708) with: {'module__weight_init': <function normal_ at 0x112020550>}\n0.652344 (0.003189) with: {'module__weight_init': <function zeros_ at 0x112020820>}\n0.691406 (0.030758) with: {'module__weight_init': <function xavier_normal_ at 0x112020af0>}\n0.592448 (0.171589) with: {'module__weight_init': <function xavier_uniform_ at 0x112020a60>}\n0.563802 (0.152971) with: {'module__weight_init': <function kaiming_normal_ at 0x112020ca0>}\n0.697917 (0.013279) with: {'module__weight_init': <function kaiming_uniform_ at 0x112020c10>}\n```", "```py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.optim as optim\nfrom skorch import NeuralNetClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# load the dataset, split into input (X) and output (y) variables\ndataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\nX = dataset[:,0:8]\ny = dataset[:,8]\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n\n# PyTorch classifier\nclass PimaClassifier(nn.Module):\n    def __init__(self, activation=nn.ReLU):\n        super().__init__()\n        self.layer = nn.Linear(8, 12)\n        self.act = activation()\n        self.output = nn.Linear(12, 1)\n        self.prob = nn.Sigmoid()\n        # manually init weights\n        init.kaiming_uniform_(self.layer.weight)\n        init.kaiming_uniform_(self.output.weight)\n\n    def forward(self, x):\n        x = self.act(self.layer(x))\n        x = self.prob(self.output(x))\n        return x\n\n# create model with skorch\nmodel = NeuralNetClassifier(\n    PimaClassifier,\n    criterion=nn.BCELoss,\n    optimizer=optim.Adamax,\n    max_epochs=100,\n    batch_size=10,\n    verbose=False\n)\n\n# define the grid search parameters\nparam_grid = {\n    'module__activation': [nn.Identity, nn.ReLU, nn.ELU, nn.ReLU6,\n                           nn.GELU, nn.Softplus, nn.Softsign, nn.Tanh,\n                           nn.Sigmoid, nn.Hardsigmoid]\n}\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, y)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n```", "```py\nBest: 0.699219 using {'module__activation': <class 'torch.nn.modules.activation.ReLU'>}\n0.687500 (0.025315) with: {'module__activation': <class 'torch.nn.modules.linear.Identity'>}\n0.699219 (0.011049) with: {'module__activation': <class 'torch.nn.modules.activation.ReLU'>}\n0.674479 (0.035849) with: {'module__activation': <class 'torch.nn.modules.activation.ELU'>}\n0.621094 (0.063549) with: {'module__activation': <class 'torch.nn.modules.activation.ReLU6'>}\n0.674479 (0.017566) with: {'module__activation': <class 'torch.nn.modules.activation.GELU'>}\n0.558594 (0.149189) with: {'module__activation': <class 'torch.nn.modules.activation.Softplus'>}\n0.675781 (0.014616) with: {'module__activation': <class 'torch.nn.modules.activation.Softsign'>}\n0.619792 (0.018688) with: {'module__activation': <class 'torch.nn.modules.activation.Tanh'>}\n0.643229 (0.019225) with: {'module__activation': <class 'torch.nn.modules.activation.Sigmoid'>}\n0.636719 (0.022326) with: {'module__activation': <class 'torch.nn.modules.activation.Hardsigmoid'>}\n```", "```py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.optim as optim\nfrom skorch import NeuralNetClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# load the dataset, split into input (X) and output (y) variables\ndataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\nX = dataset[:,0:8]\ny = dataset[:,8]\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n\n# PyTorch classifier\nclass PimaClassifier(nn.Module):\n    def __init__(self, dropout_rate=0.5, weight_constraint=1.0):\n        super().__init__()\n        self.layer = nn.Linear(8, 12)\n        self.act = nn.ReLU()\n        self.dropout = nn.Dropout(dropout_rate)\n        self.output = nn.Linear(12, 1)\n        self.prob = nn.Sigmoid()\n        self.weight_constraint = weight_constraint\n        # manually init weights\n        init.kaiming_uniform_(self.layer.weight)\n        init.kaiming_uniform_(self.output.weight)\n\n    def forward(self, x):\n        # maxnorm weight before actual forward pass\n        with torch.no_grad():\n            norm = self.layer.weight.norm(2, dim=0, keepdim=True).clamp(min=self.weight_constraint / 2)\n            desired = torch.clamp(norm, max=self.weight_constraint)\n            self.layer.weight *= (desired / norm)\n        # actual forward pass\n        x = self.act(self.layer(x))\n        x = self.dropout(x)\n        x = self.prob(self.output(x))\n        return x\n\n# create model with skorch\nmodel = NeuralNetClassifier(\n    PimaClassifier,\n    criterion=nn.BCELoss,\n    optimizer=optim.Adamax,\n    max_epochs=100,\n    batch_size=10,\n    verbose=False\n)\n\n# define the grid search parameters\nparam_grid = {\n    'module__weight_constraint': [1.0, 2.0, 3.0, 4.0, 5.0],\n    'module__dropout_rate': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n}\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, y)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n```", "```py\nBest: 0.701823 using {'module__dropout_rate': 0.1, 'module__weight_constraint': 2.0}\n0.669271 (0.015073) with: {'module__dropout_rate': 0.0, 'module__weight_constraint': 1.0}\n0.692708 (0.035132) with: {'module__dropout_rate': 0.0, 'module__weight_constraint': 2.0}\n0.589844 (0.170180) with: {'module__dropout_rate': 0.0, 'module__weight_constraint': 3.0}\n0.561198 (0.151131) with: {'module__dropout_rate': 0.0, 'module__weight_constraint': 4.0}\n0.688802 (0.021710) with: {'module__dropout_rate': 0.0, 'module__weight_constraint': 5.0}\n0.697917 (0.009744) with: {'module__dropout_rate': 0.1, 'module__weight_constraint': 1.0}\n0.701823 (0.016367) with: {'module__dropout_rate': 0.1, 'module__weight_constraint': 2.0}\n0.694010 (0.010253) with: {'module__dropout_rate': 0.1, 'module__weight_constraint': 3.0}\n0.686198 (0.025976) with: {'module__dropout_rate': 0.1, 'module__weight_constraint': 4.0}\n0.679688 (0.026107) with: {'module__dropout_rate': 0.1, 'module__weight_constraint': 5.0}\n0.701823 (0.029635) with: {'module__dropout_rate': 0.2, 'module__weight_constraint': 1.0}\n0.682292 (0.014731) with: {'module__dropout_rate': 0.2, 'module__weight_constraint': 2.0}\n0.701823 (0.009744) with: {'module__dropout_rate': 0.2, 'module__weight_constraint': 3.0}\n0.701823 (0.026557) with: {'module__dropout_rate': 0.2, 'module__weight_constraint': 4.0}\n0.687500 (0.015947) with: {'module__dropout_rate': 0.2, 'module__weight_constraint': 5.0}\n0.686198 (0.006639) with: {'module__dropout_rate': 0.3, 'module__weight_constraint': 1.0}\n0.656250 (0.006379) with: {'module__dropout_rate': 0.3, 'module__weight_constraint': 2.0}\n0.565104 (0.155608) with: {'module__dropout_rate': 0.3, 'module__weight_constraint': 3.0}\n0.700521 (0.028940) with: {'module__dropout_rate': 0.3, 'module__weight_constraint': 4.0}\n0.669271 (0.012890) with: {'module__dropout_rate': 0.3, 'module__weight_constraint': 5.0}\n0.661458 (0.018688) with: {'module__dropout_rate': 0.4, 'module__weight_constraint': 1.0}\n0.669271 (0.017566) with: {'module__dropout_rate': 0.4, 'module__weight_constraint': 2.0}\n0.652344 (0.006379) with: {'module__dropout_rate': 0.4, 'module__weight_constraint': 3.0}\n0.680990 (0.037783) with: {'module__dropout_rate': 0.4, 'module__weight_constraint': 4.0}\n0.692708 (0.042112) with: {'module__dropout_rate': 0.4, 'module__weight_constraint': 5.0}\n0.666667 (0.006639) with: {'module__dropout_rate': 0.5, 'module__weight_constraint': 1.0}\n0.652344 (0.011500) with: {'module__dropout_rate': 0.5, 'module__weight_constraint': 2.0}\n0.662760 (0.007366) with: {'module__dropout_rate': 0.5, 'module__weight_constraint': 3.0}\n0.558594 (0.146610) with: {'module__dropout_rate': 0.5, 'module__weight_constraint': 4.0}\n0.552083 (0.141826) with: {'module__dropout_rate': 0.5, 'module__weight_constraint': 5.0}\n0.548177 (0.141826) with: {'module__dropout_rate': 0.6, 'module__weight_constraint': 1.0}\n0.653646 (0.013279) with: {'module__dropout_rate': 0.6, 'module__weight_constraint': 2.0}\n0.661458 (0.008027) with: {'module__dropout_rate': 0.6, 'module__weight_constraint': 3.0}\n0.553385 (0.142719) with: {'module__dropout_rate': 0.6, 'module__weight_constraint': 4.0}\n0.669271 (0.035132) with: {'module__dropout_rate': 0.6, 'module__weight_constraint': 5.0}\n0.662760 (0.015733) with: {'module__dropout_rate': 0.7, 'module__weight_constraint': 1.0}\n0.636719 (0.024910) with: {'module__dropout_rate': 0.7, 'module__weight_constraint': 2.0}\n0.550781 (0.146818) with: {'module__dropout_rate': 0.7, 'module__weight_constraint': 3.0}\n0.537760 (0.140094) with: {'module__dropout_rate': 0.7, 'module__weight_constraint': 4.0}\n0.542969 (0.138144) with: {'module__dropout_rate': 0.7, 'module__weight_constraint': 5.0}\n0.565104 (0.148654) with: {'module__dropout_rate': 0.8, 'module__weight_constraint': 1.0}\n0.657552 (0.008027) with: {'module__dropout_rate': 0.8, 'module__weight_constraint': 2.0}\n0.428385 (0.111418) with: {'module__dropout_rate': 0.8, 'module__weight_constraint': 3.0}\n0.549479 (0.142719) with: {'module__dropout_rate': 0.8, 'module__weight_constraint': 4.0}\n0.648438 (0.005524) with: {'module__dropout_rate': 0.8, 'module__weight_constraint': 5.0}\n0.540365 (0.136861) with: {'module__dropout_rate': 0.9, 'module__weight_constraint': 1.0}\n0.605469 (0.053083) with: {'module__dropout_rate': 0.9, 'module__weight_constraint': 2.0}\n0.553385 (0.139948) with: {'module__dropout_rate': 0.9, 'module__weight_constraint': 3.0}\n0.549479 (0.142719) with: {'module__dropout_rate': 0.9, 'module__weight_constraint': 4.0}\n0.595052 (0.075566) with: {'module__dropout_rate': 0.9, 'module__weight_constraint': 5.0}\n```", "```py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.optim as optim\nfrom skorch import NeuralNetClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# load the dataset, split into input (X) and output (y) variables\ndataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\nX = dataset[:,0:8]\ny = dataset[:,8]\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n\nclass PimaClassifier(nn.Module):\n    def __init__(self, n_neurons=12):\n        super().__init__()\n        self.layer = nn.Linear(8, n_neurons)\n        self.act = nn.ReLU()\n        self.dropout = nn.Dropout(0.1)\n        self.output = nn.Linear(n_neurons, 1)\n        self.prob = nn.Sigmoid()\n        self.weight_constraint = 2.0\n        # manually init weights\n        init.kaiming_uniform_(self.layer.weight)\n        init.kaiming_uniform_(self.output.weight)\n\n    def forward(self, x):\n        # maxnorm weight before actual forward pass\n        with torch.no_grad():\n            norm = self.layer.weight.norm(2, dim=0, keepdim=True).clamp(min=self.weight_constraint / 2)\n            desired = torch.clamp(norm, max=self.weight_constraint)\n            self.layer.weight *= (desired / norm)\n        # actual forward pass\n        x = self.act(self.layer(x))\n        x = self.dropout(x)\n        x = self.prob(self.output(x))\n        return x\n\n# create model with skorch\nmodel = NeuralNetClassifier(\n    PimaClassifier,\n    criterion=nn.BCELoss,\n    optimizer=optim.Adamax,\n    max_epochs=100,\n    batch_size=10,\n    verbose=False\n)\n\n# define the grid search parameters\nparam_grid = {\n    'module__n_neurons': [1, 5, 10, 15, 20, 25, 30]\n}\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, y)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n```", "```py\nBest: 0.708333 using {'module__n_neurons': 30}\n0.654948 (0.003683) with: {'module__n_neurons': 1}\n0.666667 (0.023073) with: {'module__n_neurons': 5}\n0.694010 (0.014382) with: {'module__n_neurons': 10}\n0.682292 (0.014382) with: {'module__n_neurons': 15}\n0.707031 (0.028705) with: {'module__n_neurons': 20}\n0.703125 (0.030758) with: {'module__n_neurons': 25}\n0.708333 (0.015733) with: {'module__n_neurons': 30}\n```"]