- en: A Gentle Introduction To Gradient Descent Procedure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://machinelearningmastery.com/a-gentle-introduction-to-gradient-descent-procedure/](https://machinelearningmastery.com/a-gentle-introduction-to-gradient-descent-procedure/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Gradient descent procedure is a method that holds paramount importance in machine
    learning. It is often used for minimizing error functions in classification and
    regression problems. It is also used in training neural networks, and deep learning
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, you will discover the gradient descent procedure.
  prefs: []
  type: TYPE_NORMAL
- en: 'After completing this tutorial, you will know:'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importance of gradient descent in machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: '[![A Gentle Introduction to gradient descent. Photo by Mehreen Saeed, some
    rights reserved.](../Images/ccb6c9a8177a2d9c7e554ace919b3efa.png)](https://machinelearningmastery.com/wp-content/uploads/2021/07/IMG_9313-scaled.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: A Gentle Introduction to gradient descent. Photo by Mehreen Saeed, some rights
    reserved.
  prefs: []
  type: TYPE_NORMAL
- en: Tutorial Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This tutorial is divided into two parts; they are:'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent procedure
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solved example of gradient descent procedure
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this tutorial the prerequisite knowledge of the following topics is assumed:'
  prefs: []
  type: TYPE_NORMAL
- en: A function of several variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Partial derivatives and gradient vectors](https://machinelearningmastery.com/a-gentle-introduction-to-partial-derivatives-and-gradient-vectors)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can review these concepts by clicking on the link given above.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent Procedure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The gradient descent procedure is an algorithm for finding the minimum of a
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have a function f(x), where x is a tuple of several variables,i.e.,
    x = (x_1, x_2, …x_n). Also, suppose that the gradient of f(x) is given by ∇f(x).
    We want to find the value of the variables (x_1, x_2, …x_n) that give us the minimum
    of the function. At any iteration t, we’ll denote the value of the tuple x by
    x[t]. So x[t][1] is the value of x_1 at iteration t, x[t][2] is the value of x_2
    at iteration t, e.t.c.
  prefs: []
  type: TYPE_NORMAL
- en: The Notation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have the following variables:'
  prefs: []
  type: TYPE_NORMAL
- en: t = Iteration number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T = Total iterations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: n = Total variables  in the domain of f  (also called the dimensionality of
    x)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: j = Iterator for variable number, e.g., x_j represents the jth variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ???? = Learning rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ∇f(x[t]) = Value of the gradient vector of f at iteration t
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Training Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The steps for the gradient descent algorithm are given below. This is also called
    the training method.
  prefs: []
  type: TYPE_NORMAL
- en: Choose a random initial point x_initial and set x[0] = x_initial
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For iterations t=1..T
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update x[t] = x[t-1] – ????∇f(x[t-1])
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is as simple as that!
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate ???? is a user defined variable for the gradient descent procedure.
    Its value lies in the range [0,1].
  prefs: []
  type: TYPE_NORMAL
- en: The above method says that at each iteration we have to update the value of
    x by taking a small step in the direction of the negative of the gradient vector.
    If ????=0, then there will be no change in x. If ????=1, then it is like taking
    a large step in the direction of the negative of the gradient of the vector. Normally,
    ???? is set to a small value like 0.05 or 0.1\. It can also be variable during
    the training procedure. So your algorithm can start with a large value (e.g. 0.8)
    and then reduce it to smaller values.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Get Started With Calculus for Machine Learning?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Take my free 7-day email crash course now (with sample code).
  prefs: []
  type: TYPE_NORMAL
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  prefs: []
  type: TYPE_NORMAL
- en: Example of Gradient Descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s find the minimum of the following function of two variables, whose graphs
    and contours are shown in the figure below:'
  prefs: []
  type: TYPE_NORMAL
- en: f(x,y) = x**x + 2y**y
  prefs: []
  type: TYPE_NORMAL
- en: '[![Graph and contours of f(x,y) = x*x + 2y*y](../Images/677e92ebb420e0bb877b853edb3a8b96.png)](https://machinelearningmastery.com/wp-content/uploads/2021/07/gradientDescent1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Graph and contours of f(x,y) = x*x + 2y*y
  prefs: []
  type: TYPE_NORMAL
- en: 'The general form of the gradient vector is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: ∇f(x,y) = 2xi + 4yj
  prefs: []
  type: TYPE_NORMAL
- en: Two iterations of the algorithm, T=2 and ????=0.1 are shown below
  prefs: []
  type: TYPE_NORMAL
- en: Initial t=0
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'x[0] = (4,3)     # This is just a randomly chosen point'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: At t = 1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: x[1] = x[0] – ????∇f(x[0])
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: x[1] = (4,3) – 0.1*(8,12)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: x[1] = (3.2,1.8)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: At t=2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: x[2] = x[1] – ????∇f(x[1])
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: x[2] = (3.2,1.8) – 0.1*(6.4,7.2)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: x[2] = (2.56,1.08)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If you keep running the above iterations, the procedure will eventually end
    up at the point where the function is minimum, i.e., (0,0).
  prefs: []
  type: TYPE_NORMAL
- en: 'At iteration t=1, the algorithm is illustrated in the figure below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![Illustration of gradient descent procedure](../Images/316758996fd4b1cb60527bcde73300c3.png)](https://machinelearningmastery.com/wp-content/uploads/2021/07/gradientDescent2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Illustration of gradient descent procedure
  prefs: []
  type: TYPE_NORMAL
- en: How Many Iterations to Run?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normally gradient descent is run till the value of x does not change or the
    change in x is below a certain threshold. The stopping criterion can also be a
    user defined maximum number of iterations (that we defined earlier as T).
  prefs: []
  type: TYPE_NORMAL
- en: Adding Momentum
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Gradient descent can run into problems such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Oscillate between two or more points
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get trapped in a local minimum
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Overshoot and miss the minimum point
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To take care of the above problems, a momentum term can be added to the update
    equation of gradient descent algorithm as:'
  prefs: []
  type: TYPE_NORMAL
- en: x[t] = x[t-1] – ????∇f(x[t-1]) + ????*Δx[t-1]
  prefs: []
  type: TYPE_NORMAL
- en: where Δx[t-1] represents the change in x, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: Δx[t] = x[t] – x[t-1]
  prefs: []
  type: TYPE_NORMAL
- en: The initial change at t=0 is a zero vector. For this problem Δx[0] = (0,0).
  prefs: []
  type: TYPE_NORMAL
- en: About Gradient Ascent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a related gradient ascent procedure, which finds the maximum of a function.
    In gradient descent we follow the direction of the rate of maximum decrease of
    a function. It is the direction of the negative gradient vector. Whereas, in gradient
    ascent we follow the direction of maximum rate of increase of a function, which
    is the direction pointed to by the positive gradient vector. We can also write
    a maximization problem in terms of a maximization problem by adding a negative
    sign to f(x), i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Why Is The Gradient Descent Important In Machine Learning?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The gradient descent algorithm is often employed in machine learning problems.
    In many classification and regression tasks, the mean square error function is
    used to fit a model to the data. The gradient descent procedure is used to identify
    the optimal model parameters that lead to the lowest mean square error.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient ascent is used similarly, for problems that involve maximizing a function.
  prefs: []
  type: TYPE_NORMAL
- en: Extensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section lists some ideas for extending the tutorial that you may wish to
    explore.
  prefs: []
  type: TYPE_NORMAL
- en: Hessian matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jacobian
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you explore any of these extensions, I’d love to know. Post your findings
    in the comments below.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section provides more resources on the topic if you are looking to go deeper.
  prefs: []
  type: TYPE_NORMAL
- en: Tutorials
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Derivatives](https://machinelearningmastery.com/a-gentle-introduction-to-function-derivatives)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Slopes and tangents](https://machinelearningmastery.com/a-gentle-introduction-to-slopes-and-tangents)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Gradient descent for machine learning](https://machinelearningmastery.com/gradient-descent-for-machine-learning/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What is gradient in machine learning](https://machinelearningmastery.com/gradient-in-machine-learning/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Partial derivatives and gradient vectors](https://machinelearningmastery.com/a-gentle-introduction-to-partial-derivatives-and-gradient-vectors)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Additional resources on [Calculus Books for Machine Learning](https://machinelearningmastery.com/calculus-books-for-machine-learning/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Books
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Thomas’ Calculus](https://amzn.to/35Yeolv), 14th edition, 2017\. (based on
    the original works of George B. Thomas, revised by Joel Hass, Christopher Heil,
    Maurice Weir)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Calculus](https://www.amazon.com/Calculus-3rd-Gilbert-Strang/dp/0980232759/ref=as_li_ss_tl?dchild=1&keywords=Gilbert+Strang+calculus&qid=1606171602&s=books&sr=1-1&linkCode=sl1&tag=inspiredalgor-20&linkId=423b93db012f7cc6bb92cb7494a3095f&language=en_US),
    3rd Edition, 2017\. (Gilbert Strang)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Calculus](https://amzn.to/3kS9I52), 8th edition, 2015\. (James Stewart)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this tutorial, you discovered the algorithm for gradient descent. Specifically,
    you learned:'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent procedure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to apply gradient descent procedure to find the minimum of a function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to transform a maximization problem into a minimization problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you have any questions?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ask your questions in the comments below and I will do my best to answer.
  prefs: []
  type: TYPE_NORMAL
