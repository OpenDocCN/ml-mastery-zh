["```py\nclass FeedForward(Layer):\n    def __init__(self, d_ff, d_model, **kwargs):\n        super(FeedForward, self).__init__(**kwargs)\n        self.fully_connected1 = Dense(d_ff)  # First fully connected layer\n        self.fully_connected2 = Dense(d_model)  # Second fully connected layer\n        self.activation = ReLU()  # ReLU activation layer\n        ...\n```", "```py\n...\ndef call(self, x):\n    # The input is passed into the two fully-connected layers, with a ReLU in between\n    x_fc1 = self.fully_connected1(x)\n\n    return self.fully_connected2(self.activation(x_fc1))\n```", "```py\nclass AddNormalization(Layer):\n    def __init__(self, **kwargs):\n        super(AddNormalization, self).__init__(**kwargs)\n        self.layer_norm = LayerNormalization()  # Layer normalization layer\n        ...\n```", "```py\n...\ndef call(self, x, sublayer_x):\n    # The sublayer input and output need to be of the same shape to be summed\n    add = x + sublayer_x\n\n    # Apply layer normalization to the sum\n    return self.layer_norm(add)\n```", "```py\nclass EncoderLayer(Layer):\n    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n        super(EncoderLayer, self).__init__(**kwargs)\n        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n        self.dropout1 = Dropout(rate)\n        self.add_norm1 = AddNormalization()\n        self.feed_forward = FeedForward(d_ff, d_model)\n        self.dropout2 = Dropout(rate)\n        self.add_norm2 = AddNormalization()\n        ...\n```", "```py\n...\ndef call(self, x, padding_mask, training):\n    # Multi-head attention layer\n    multihead_output = self.multihead_attention(x, x, x, padding_mask)\n    # Expected output shape = (batch_size, sequence_length, d_model)\n\n    # Add in a dropout layer\n    multihead_output = self.dropout1(multihead_output, training=training)\n\n    # Followed by an Add & Norm layer\n    addnorm_output = self.add_norm1(x, multihead_output)\n    # Expected output shape = (batch_size, sequence_length, d_model)\n\n    # Followed by a fully connected layer\n    feedforward_output = self.feed_forward(addnorm_output)\n    # Expected output shape = (batch_size, sequence_length, d_model)\n\n    # Add in another dropout layer\n    feedforward_output = self.dropout2(feedforward_output, training=training)\n\n    # Followed by another Add & Norm layer\n    return self.add_norm2(addnorm_output, feedforward_output)\n```", "```py\nclass Encoder(Layer):\n    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n        self.dropout = Dropout(rate)\n        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n        ...\n```", "```py\n...\ndef call(self, input_sentence, padding_mask, training):\n    # Generate the positional encoding\n    pos_encoding_output = self.pos_encoding(input_sentence)\n    # Expected output shape = (batch_size, sequence_length, d_model)\n\n    # Add in a dropout layer\n    x = self.dropout(pos_encoding_output, training=training)\n\n    # Pass on the positional encoded values to each encoder layer\n    for i, layer in enumerate(self.encoder_layer):\n        x = layer(x, padding_mask, training)\n\n    return x\n```", "```py\nfrom tensorflow.keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout\nfrom multihead_attention import MultiHeadAttention\nfrom positional_encoding import PositionEmbeddingFixedWeights\n\n# Implementing the Add & Norm Layer\nclass AddNormalization(Layer):\n    def __init__(self, **kwargs):\n        super(AddNormalization, self).__init__(**kwargs)\n        self.layer_norm = LayerNormalization()  # Layer normalization layer\n\n    def call(self, x, sublayer_x):\n        # The sublayer input and output need to be of the same shape to be summed\n        add = x + sublayer_x\n\n        # Apply layer normalization to the sum\n        return self.layer_norm(add)\n\n# Implementing the Feed-Forward Layer\nclass FeedForward(Layer):\n    def __init__(self, d_ff, d_model, **kwargs):\n        super(FeedForward, self).__init__(**kwargs)\n        self.fully_connected1 = Dense(d_ff)  # First fully connected layer\n        self.fully_connected2 = Dense(d_model)  # Second fully connected layer\n        self.activation = ReLU()  # ReLU activation layer\n\n    def call(self, x):\n        # The input is passed into the two fully-connected layers, with a ReLU in between\n        x_fc1 = self.fully_connected1(x)\n\n        return self.fully_connected2(self.activation(x_fc1))\n\n# Implementing the Encoder Layer\nclass EncoderLayer(Layer):\n    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n        super(EncoderLayer, self).__init__(**kwargs)\n        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n        self.dropout1 = Dropout(rate)\n        self.add_norm1 = AddNormalization()\n        self.feed_forward = FeedForward(d_ff, d_model)\n        self.dropout2 = Dropout(rate)\n        self.add_norm2 = AddNormalization()\n\n    def call(self, x, padding_mask, training):\n        # Multi-head attention layer\n        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n        # Expected output shape = (batch_size, sequence_length, d_model)\n\n        # Add in a dropout layer\n        multihead_output = self.dropout1(multihead_output, training=training)\n\n        # Followed by an Add & Norm layer\n        addnorm_output = self.add_norm1(x, multihead_output)\n        # Expected output shape = (batch_size, sequence_length, d_model)\n\n        # Followed by a fully connected layer\n        feedforward_output = self.feed_forward(addnorm_output)\n        # Expected output shape = (batch_size, sequence_length, d_model)\n\n        # Add in another dropout layer\n        feedforward_output = self.dropout2(feedforward_output, training=training)\n\n        # Followed by another Add & Norm layer\n        return self.add_norm2(addnorm_output, feedforward_output)\n\n# Implementing the Encoder\nclass Encoder(Layer):\n    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n        self.dropout = Dropout(rate)\n        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n\n    def call(self, input_sentence, padding_mask, training):\n        # Generate the positional encoding\n        pos_encoding_output = self.pos_encoding(input_sentence)\n        # Expected output shape = (batch_size, sequence_length, d_model)\n\n        # Add in a dropout layer\n        x = self.dropout(pos_encoding_output, training=training)\n\n        # Pass on the positional encoded values to each encoder layer\n        for i, layer in enumerate(self.encoder_layer):\n            x = layer(x, padding_mask, training)\n\n        return x\n```", "```py\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nd_model = 512  # Dimensionality of the model sub-layers' outputs\nn = 6  # Number of layers in the encoder stack\n\nbatch_size = 64  # Batch size from the training process\ndropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n...\n```", "```py\n...\nenc_vocab_size = 20 # Vocabulary size for the encoder\ninput_seq_length = 5  # Maximum length of the input sequence\n\ninput_seq = random.random((batch_size, input_seq_length))\n...\n```", "```py\n...\nencoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\nprint(encoder(input_seq, None, True))\n```", "```py\nfrom numpy import random\n\nenc_vocab_size = 20 # Vocabulary size for the encoder\ninput_seq_length = 5  # Maximum length of the input sequence\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nd_model = 512  # Dimensionality of the model sub-layers' outputs\nn = 6  # Number of layers in the encoder stack\n\nbatch_size = 64  # Batch size from the training process\ndropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n\ninput_seq = random.random((batch_size, input_seq_length))\n\nencoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\nprint(encoder(input_seq, None, True))\n```", "```py\ntf.Tensor(\n[[[-0.4214715  -1.1246173  -0.8444572  ...  1.6388322  -0.1890367\n    1.0173352 ]\n  [ 0.21662089 -0.61147404 -1.0946581  ...  1.4627445  -0.6000164\n   -0.64127874]\n  [ 0.46674493 -1.4155326  -0.5686513  ...  1.1790234  -0.94788337\n    0.1331717 ]\n  [-0.30638126 -1.9047263  -1.8556844  ...  0.9130118  -0.47863355\n    0.00976158]\n  [-0.22600567 -0.9702025  -0.91090447 ...  1.7457147  -0.139926\n   -0.07021569]]\n...\n\n [[-0.48047638 -1.1034104  -0.16164204 ...  1.5588069   0.08743562\n   -0.08847156]\n  [-0.61683714 -0.8403657  -1.0450369  ...  2.3587787  -0.76091915\n   -0.02891812]\n  [-0.34268388 -0.65042275 -0.6715749  ...  2.8530657  -0.33631966\n    0.5215888 ]\n  [-0.6288677  -1.0030932  -0.9749813  ...  2.1386387   0.0640307\n   -0.69504136]\n  [-1.33254    -1.2524267  -0.230098   ...  2.515467   -0.04207756\n   -0.3395423 ]]], shape=(64, 5, 512), dtype=float32)\n```"]