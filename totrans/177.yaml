- en: Further Stable Diffusion Pipeline with Diffusers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://machinelearningmastery.com/further-stable-diffusion-pipeline-with-diffusers/](https://machinelearningmastery.com/further-stable-diffusion-pipeline-with-diffusers/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There are many ways you can access Stable Diffusion models and generate high-quality
    images. One popular method is using the Diffusers Python library. It provides
    a simple interface to Stable Diffusion, making it easy to leverage these powerful
    AI image generation models. The `diffusers` lowers the barrier to using cutting-edge
    generative AI, enabling rapid experimentation and development. This library is
    very powerful. Not only you can use it to generate pictures from text prompts,
    but also to leverage LoRA and ControlNet to create a better picture.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post, you will learn about Hugging Face’s Diffusers, how to generate
    images, and how to apply various image generation techniques similar to Stable
    Diffusion WebUI. Specifically, you will learn how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Build a Diffusers Pipeline and generate a simple image with a prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading LoRA weights of fine-tuned models and generating IKEA-style images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build ControlNet OpenPose pipeline to generate an image using a reference image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kick-start your project** with my book [Mastering Digital Art with Stable
    Diffusion](https://machinelearningmastery.com/mastering-digital-art-with-stable-diffusion/).
    It provides **self-study tutorials** with **working code**.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/34c4912f68b0bec21520237b0a1fc7ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Further Stable Diffusion Pipeline with Diffusers
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Felicia Buitenwerf](https://unsplash.com/photos/white-and-black-light-bulb-8xFgmFnOnAg).
    Some rights reserved.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This post is in three parts; they are:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Diffusers on Google Colab
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading LoRA Weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ControlNet OpenPose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Diffusers on Google Colab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hugging Face’s diffusers is a Python library that allows you to access pre-trained
    diffusion models for generating realistic images, audio, and 3D molecular structures.
    You can use it for simple inference or train your own diffusion model. What’s
    special about this library is that with just a few lines of code, you can download
    the model from Hugging Face Hub and use it to generate images, similar to the
    Stable Diffusion WebUI.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of setting up locally, you will use Google Colab’s free GPU-based Notebook.
    To do so, go to [https://colab.research.google.com/](https://colab.research.google.com/)
    and create a new notebook. To access the GPU, you must head to “Runtime” → “Change
    runtime type” and select “T4 GPU” option.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d453ae11b11152c6017994290692e9a0.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting a GPU on Google Colab
  prefs: []
  type: TYPE_NORMAL
- en: Using Colab saves you from the burden of owning a GPU device to run Stable Diffusion
    efficiently. By the nature of a Jupyter notebook, you just need to keep all the
    following code in their own cell to run. It would be convenient for you to experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, install all necessary Python libraries to run `diffusers` pipeline.
    You need to create a notebook cell with the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the colab notebook, the `!` at the beginning of the line means this is a
    system command, not a Python code.
  prefs: []
  type: TYPE_NORMAL
- en: To generate an image using a prompt, you must first create a Diffusion pipeline.
    In the following, you will download and use Stable Diffusion XL with “float 16”
    type to save memory. Then, you will set up a pipeline to use the GPU as an accelerator.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To generate the image of a young woman, you will provide the same generic prompt
    to the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, you got exceptional results with a few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1e6045c73d34ce4f01d60520c127531.png)'
  prefs: []
  type: TYPE_IMG
- en: Image as generated using diffusers library with Stable Diffusion XL pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to Stable Diffusion WebUI, you can provide a positive prompt, a negative
    prompt, inference steps, set random seed, change the size, and guidance scale
    to generate the image as you wished:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The image is perfect, and it looks like a digital artist spent almost 200 hours
    creating it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73fd244c562d96bb4fd921f7f89e18f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Another picture generated by the Stable Diffusion XL pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Loading LoRA Weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Not only you can invoke the pipeline directly, you can also load LoRA weights
    into your pipeline. LoRA weights are model adapters that are fine-tuned for specific
    types of images. They can be attached to the base model to produce custom results.
    In the following, you will use LoRA weights to generate images in the style of
    IKEA instructional images.
  prefs: []
  type: TYPE_NORMAL
- en: You will download and load the LoRA adapter `ostris/ikea-instructions-lora-sdxl`
    by providing the Hugging Face link, the location of the adapter in the repository,
    and the name of the adapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To generate an IKEA-style image, you will provide a simple prompt, inference
    step, scale argument, and manual seed to the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You created a super villain that comes with instructions. Although not perfect,
    it can be used to generate custom images for your work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6bcc1d5a4d8f670dd886abcfe055d8ef.png)'
  prefs: []
  type: TYPE_IMG
- en: An IKEA style picture generated using a LoRA
  prefs: []
  type: TYPE_NORMAL
- en: ControlNet OpenPose
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s see another extension. You will now use the ControlNet OpenPose model
    to generate a control image using the reference image. ControlNet is a type of
    neural network architecture that controls diffusion models by adding extra conditions.
  prefs: []
  type: TYPE_NORMAL
- en: You will install `controlnet_aux` for the detecting pose of the body in the
    image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You will then build the ControlNet pipeline by loading the model from Hugging
    Face Hub in fp16 type. After that, you will load the free image from Pexels.com
    into our environment using the link.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: To display a grid of images, you will create a Python function that takes a
    list of images and displays them in a grid in a Colab notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the next step, you will build the OpenPose detector pipeline and feed it
    the image that you loaded. To see the original image and OpenPose image side by
    side, you will use the `image_grid` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The detector has successfully generated the structure of the human pose.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e944bb08888a21f5c184b10ef1dfbbd8.png)'
  prefs: []
  type: TYPE_IMG
- en: The original image and the detected pose. Note both pictures are in 1:1 aspect
    ratio to match the default in Stable Diffusion
  prefs: []
  type: TYPE_NORMAL
- en: Now, you will combine everything together. You will create Stable Diffusion
    1.5 text to image pipeline and provide a ControlNet OpenPose model. You are using
    the fp16 variant for memory optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You will generate four images using the same positive and negative prompts and
    display them in a grid. Note that you provide the pose image instead of the original
    image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The results are fantastic. All of the women are dancing in the same pose. There
    are a few deformities, but you cannot expect much from stable diffusion 1.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6cf788480411ef491e669204b9fcd00.png)'
  prefs: []
  type: TYPE_IMG
- en: Four images were generated using ControlNet pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Further Readings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section provides more resources on the topic if you want to go deeper.
  prefs: []
  type: TYPE_NORMAL
- en: '[diffusers API manual](https://huggingface.co/docs/diffusers/main/en/index)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[DiffusionPipeline API](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AutoPipelineForText2Image API](https://huggingface.co/docs/diffusers/en/api/pipelines/auto_pipeline)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[controlnet_aux Python package](https://github.com/huggingface/controlnet_aux)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this post, you learned about the Hugging Face Diffuser library and how to
    use it to generate high quality and custom images. Specifically, you covered:'
  prefs: []
  type: TYPE_NORMAL
- en: What is Diffusers, and how does it work?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to apply advanced settings and negative prompts to generate consistent images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to load LoRA weights to generate IKEA-style images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to control Stable Diffusion output using the ControlNet OpenPose model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
