["```py\nimport torch\nimport torch.nn as nn\n\nmodel = nn.Sequential(...)\n```", "```py\nmodel = nn.Sequential(\n    nn.Linear(764, 100),\n    nn.ReLU(),\n    nn.Linear(100, 50),\n    nn.ReLU(),\n    nn.Linear(50, 10),\n    nn.Sigmoid()\n)\n```", "```py\nfrom collections import OrderedDict\nimport torch.nn as nn\n\nmodel = nn.Sequential(OrderedDict([\n    ('dense1', nn.Linear(764, 100)),\n    ('act1', nn.ReLU()),\n    ('dense2', nn.Linear(100, 50)),\n    ('act2', nn.ReLU()),\n    ('output', nn.Linear(50, 10)),\n    ('outact', nn.Sigmoid()),\n]))\n```", "```py\nmodel = nn.Sequential()\nmodel.add_module(\"dense1\", nn.Linear(8, 12))\nmodel.add_module(\"act1\", nn.ReLU())\nmodel.add_module(\"dense2\", nn.Linear(12, 8))\nmodel.add_module(\"act2\", nn.ReLU())\nmodel.add_module(\"output\", nn.Linear(8, 1))\nmodel.add_module(\"outact\", nn.Sigmoid())\n```", "```py\nnn.Linear(764, 100, device=\"cpu\")\n```", "```py\nnn.Linear(764, 100, device=\"cuda:0\")\n```", "```py\nnn.Linear(764, 100, dtype=torch.float16)\n```", "```py\nloss_fn = nn.CrossEntropyLoss()\nloss = loss_fn(output, label)\n```", "```py\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n```", "```py\nfor n in range(num_epochs):\n    y_pred = model(X)\n    loss = loss_fn(y_pred, y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```", "```py\nprint(model)\n```", "```py\nSequential(\n  (0): Linear(in_features=8, out_features=12, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=12, out_features=8, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=8, out_features=1, bias=True)\n  (5): Sigmoid()\n)\n```", "```py\ntorch.save(model, \"my_model.pickle\")\n```", "```py\nmodel = torch.load(\"my_model.pickle\")\n```", "```py\ntorch.save(model.state_dict(), \"my_model.pickle\")\n```", "```py\nmodel = nn.Sequential(...)\nmodel.load_state_dict(torch.load(\"my_model.pickle\"))\n```"]