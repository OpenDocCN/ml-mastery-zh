["```py\nimport pathlib\n\nimport tensorflow as tf\n\n# download dataset provided by Anki: https://www.manythings.org/anki/\ntext_file = tf.keras.utils.get_file(\n    fname=\"fra-eng.zip\",\n    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n    extract=True,\n)\n# show where the file is located now\ntext_file = pathlib.Path(text_file).parent / \"fra.txt\"\nprint(text_file)\n```", "```py\n<English sentence><tab character><French sentence>\n```", "```py\nimport pathlib\nimport pickle\nimport random\nimport re\nimport unicodedata\n\nimport tensorflow as tf\n\n# download dataset provided by Anki: https://www.manythings.org/anki/\ntext_file = tf.keras.utils.get_file(\n    fname=\"fra-eng.zip\",\n    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n    extract=True,\n)\ntext_file = pathlib.Path(text_file).parent / \"fra.txt\"\n\ndef normalize(line):\n    \"\"\"Normalize a line of text and split into two at the tab character\"\"\"\n    line = unicodedata.normalize(\"NFKC\", line.strip().lower())\n    line = re.sub(r\"^([^ \\w])(?!\\s)\", r\"\\1 \", line)\n    line = re.sub(r\"(\\s[^ \\w])(?!\\s)\", r\"\\1 \", line)\n    line = re.sub(r\"(?!\\s)([^ \\w])$\", r\" \\1\", line)\n    line = re.sub(r\"(?!\\s)([^ \\w]\\s)\", r\" \\1\", line)\n    eng, fra = line.split(\"\\t\")\n    fra = \"[start] \" + fra + \" [end]\"\n    return eng, fra\n\n# normalize each line and separate into English and French\nwith open(text_file) as fp:\n    text_pairs = [normalize(line) for line in fp]\n\n# print some samples\nfor _ in range(5):\n    print(random.choice(text_pairs))\n\nwith open(\"text_pairs.pickle\", \"wb\") as fp:\n    pickle.dump(text_pairs, fp)\n```", "```py\n('where did you put your key ?', '[start] où est-ce tu as mis ta clé  ?  [end]')\n('you missed a spot .', '[start] tu as loupé une tache . [end]')\n(\"i think we're being followed .\", '[start] je pense que nous sommes suivis . [end]')\n('i bought a cactus .', \"[start] j'ai acheté un cactus . [end]\")\n('i have more than enough .', \"[start] j'en ai plus que marre . [end]\")\n```", "```py\nimport pickle\n\nwith open(\"text_pairs.pickle\", \"rb\") as fp:\n    text_pairs = pickle.load(fp)\n\n# count tokens\neng_tokens, fra_tokens = set(), set()\neng_maxlen, fra_maxlen = 0, 0\nfor eng, fra in text_pairs:\n    eng_tok, fra_tok = eng.split(), fra.split()\n    eng_maxlen = max(eng_maxlen, len(eng_tok))\n    fra_maxlen = max(fra_maxlen, len(fra_tok))\n    eng_tokens.update(eng_tok)\n    fra_tokens.update(fra_tok)\nprint(f\"Total English tokens: {len(eng_tokens)}\")\nprint(f\"Total French tokens: {len(fra_tokens)}\")\nprint(f\"Max English length: {eng_maxlen}\")\nprint(f\"Max French length: {fra_maxlen}\")\nprint(f\"{len(text_pairs)} total pairs\")\n```", "```py\nimport pickle\n\nimport matplotlib.pyplot as plt\n\nwith open(\"text_pairs.pickle\", \"rb\") as fp:\n    text_pairs = pickle.load(fp)\n\n# histogram of sentence length in tokens\nen_lengths = [len(eng.split()) for eng, fra in text_pairs]\nfr_lengths = [len(fra.split()) for eng, fra in text_pairs]\n\nplt.hist(en_lengths, label=\"en\", color=\"red\", alpha=0.33)\nplt.hist(fr_lengths, label=\"fr\", color=\"blue\", alpha=0.33)\nplt.yscale(\"log\")     # sentence length fits Benford\"s law\nplt.ylim(plt.ylim())  # make y-axis consistent for both plots\nplt.plot([max(en_lengths), max(en_lengths)], plt.ylim(), color=\"red\")\nplt.plot([max(fr_lengths), max(fr_lengths)], plt.ylim(), color=\"blue\")\nplt.legend()\nplt.title(\"Examples count vs Token length\")\nplt.show()\n```", "```py\nimport pickle\nimport random\n\nfrom tensorflow.keras.layers import TextVectorization\n\n# Load normalized sentence pairs\nwith open(\"text_pairs.pickle\", \"rb\") as fp:\n    text_pairs = pickle.load(fp)\n\n# train-test-val split of randomized sentence pairs\nrandom.shuffle(text_pairs)\nn_val = int(0.15*len(text_pairs))\nn_train = len(text_pairs) - 2*n_val\ntrain_pairs = text_pairs[:n_train]\nval_pairs = text_pairs[n_train:n_train+n_val]\ntest_pairs = text_pairs[n_train+n_val:]\n\n# Parameter determined after analyzing the input data\nvocab_size_en = 10000\nvocab_size_fr = 20000\nseq_length = 20\n\n# Create vectorizer\neng_vectorizer = TextVectorization(\n    max_tokens=vocab_size_en,\n    standardize=None,\n    split=\"whitespace\",\n    output_mode=\"int\",\n    output_sequence_length=seq_length,\n)\nfra_vectorizer = TextVectorization(\n    max_tokens=vocab_size_fr,\n    standardize=None,\n    split=\"whitespace\",\n    output_mode=\"int\",\n    output_sequence_length=seq_length + 1\n)\n\n# train the vectorization layer using training dataset\ntrain_eng_texts = [pair[0] for pair in train_pairs]\ntrain_fra_texts = [pair[1] for pair in train_pairs]\neng_vectorizer.adapt(train_eng_texts)\nfra_vectorizer.adapt(train_fra_texts)\n\n# save for subsequent steps\nwith open(\"vectorize.pickle\", \"wb\") as fp:\n    data = {\n        \"train\": train_pairs,\n        \"val\":   val_pairs,\n        \"test\":  test_pairs,\n        \"engvec_config\":  eng_vectorizer.get_config(),\n        \"engvec_weights\": eng_vectorizer.get_weights(),\n        \"fravec_config\":  fra_vectorizer.get_config(),\n        \"fravec_weights\": fra_vectorizer.get_weights(),\n    }\n    pickle.dump(data, fp)\n```", "```py\nimport pickle\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization\n\n# load text data and vectorizer weights\nwith open(\"vectorize.pickle\", \"rb\") as fp:\n    data = pickle.load(fp)\n\ntrain_pairs = data[\"train\"]\nval_pairs = data[\"val\"]\ntest_pairs = data[\"test\"]   # not used\n\neng_vectorizer = TextVectorization.from_config(data[\"engvec_config\"])\neng_vectorizer.set_weights(data[\"engvec_weights\"])\nfra_vectorizer = TextVectorization.from_config(data[\"fravec_config\"])\nfra_vectorizer.set_weights(data[\"fravec_weights\"])\n\n# set up Dataset object\ndef format_dataset(eng, fra):\n    \"\"\"Take an English and a French sentence pair, convert into input and target.\n    The input is a dict with keys `encoder_inputs` and `decoder_inputs`, each\n    is a vector, corresponding to English and French sentences respectively.\n    The target is also vector of the French sentence, advanced by 1 token. All\n    vector are in the same length.\n\n    The output will be used for training the transformer model. In the model we\n    will create, the input tensors are named `encoder_inputs` and `decoder_inputs`\n    which should be matched to the keys in the dictionary for the source part\n    \"\"\"\n    eng = eng_vectorizer(eng)\n    fra = fra_vectorizer(fra)\n    source = {\"encoder_inputs\": eng,\n              \"decoder_inputs\": fra[:, :-1]}\n    target = fra[:, 1:]\n    return (source, target)\n\ndef make_dataset(pairs, batch_size=64):\n    \"\"\"Create TensorFlow Dataset for the sentence pairs\"\"\"\n    # aggregate sentences using zip(*pairs)\n    eng_texts, fra_texts = zip(*pairs)\n    # convert them into list, and then create tensors\n    dataset = tf.data.Dataset.from_tensor_slices((list(eng_texts), list(fra_texts)))\n    return dataset.shuffle(2048) \\\n                  .batch(batch_size).map(format_dataset) \\\n                  .prefetch(16).cache()\n\ntrain_ds = make_dataset(train_pairs)\nval_ds = make_dataset(val_pairs)\n\n# test the dataset\nfor inputs, targets in train_ds.take(1):\n    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n    print(f'inputs[\"encoder_inputs\"][0]: {inputs[\"encoder_inputs\"][0]}')\n    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n    print(f'inputs[\"decoder_inputs\"][0]: {inputs[\"decoder_inputs\"][0]}')\n    print(f\"targets.shape: {targets.shape}\")\n    print(f\"targets[0]: {targets[0]}\")\n```", "```py\ninputs[\"encoder_inputs\"].shape: (64, 20)\ninputs[\"encoder_inputs\"][0]: [142   8 263 979   2   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0]\ninputs[\"decoder_inputs\"].shape: (64, 20)\ninputs[\"decoder_inputs\"][0]: [   2   15 2496  190    4    3    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0]\ntargets.shape: (64, 20)\ntargets[0]: [  15 2496  190    4    3    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0]\n```", "```py\nimport pickle\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef pos_enc_matrix(L, d, n=10000):\n    \"\"\"Create positional encoding matrix\n\n    Args:\n        L: Input dimension (length)\n        d: Output dimension (depth), even only\n        n: Constant for the sinusoidal functions\n\n    Returns:\n        numpy matrix of floats of dimension L-by-d. At element (k,2i) the value\n        is sin(k/n^(2i/d)) while at element (k,2i+1) the value is cos(k/n^(2i/d))\n    \"\"\"\n    assert d % 2 == 0, \"Output dimension needs to be an even integer\"\n    d2 = d//2\n    P = np.zeros((L, d))\n    k = np.arange(L).reshape(-1, 1)     # L-column vector\n    i = np.arange(d2).reshape(1, -1)    # d-row vector\n    denom = np.power(n, -i/d2)          # n**(-2*i/d)\n    args = k * denom                    # (L,d) matrix\n    P[:, ::2] = np.sin(args)\n    P[:, 1::2] = np.cos(args)\n    return P\n\n# Plot the positional encoding matrix\npos_matrix = pos_enc_matrix(L=2048, d=512)\nassert pos_matrix.shape == (2048, 512)\nplt.pcolormesh(pos_matrix, cmap='RdBu')\nplt.xlabel('Depth')\nplt.ylabel('Position')\nplt.colorbar()\nplt.show()\n\nwith open(\"posenc-2048-512.pickle\", \"wb\") as fp:\n    pickle.dump(pos_matrix, fp)\n```", "```py\nimport pickle\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nwith open(\"posenc-2048-512.pickle\", \"rb\") as fp:\n    pos_matrix = pickle.load(fp)\nassert pos_matrix.shape == (2048, 512)\n# Plot the positional encoding matrix, alternative way\nplt.pcolormesh(np.hstack([pos_matrix[:, ::2], pos_matrix[:, 1::2]]), cmap='RdBu')\nplt.xlabel('Depth')\nplt.ylabel('Position')\nplt.colorbar()\nplt.show()\n```", "```py\n...\nplt.plot(pos_matrix[:, 155], label=\"high freq\")\nplt.plot(pos_matrix[:, 300], label=\"low freq\")\nplt.legend()\nplt.show()\n```", "```py\nimport pickle\nimport matplotlib.pyplot as plt\n\nwith open(\"posenc-2048-512.pickle\", \"rb\") as fp:\n    pos_matrix = pickle.load(fp)\nassert pos_matrix.shape == (2048, 512)\n# Plot two curves from different position\nplt.plot(pos_matrix[100], alpha=0.66, color=\"red\", label=\"position 100\")\nplt.legend()\nplt.show()\n```", "```py\nimport pickle\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nwith open(\"posenc-2048-512.pickle\", \"rb\") as fp:\n    pos_matrix = pickle.load(fp)\nassert pos_matrix.shape == (2048, 512)\n# Show the dot product between different normalized positional vectors\npos_matrix /= np.linalg.norm(pos_matrix, axis=1, keepdims=True)\np = pos_matrix[789]  # all vectors compare to vector at position 789\ndots = pos_matrix @ p\nplt.plot(dots)\nplt.ylim([0, 1])\nplt.show()\n```", "```py\nimport numpy as np\nimport tensorflow as tf\n\ndef pos_enc_matrix(L, d, n=10000):\n    \"\"\"Create positional encoding matrix\n\n    Args:\n        L: Input dimension (length)\n        d: Output dimension (depth), even only\n        n: Constant for the sinusoidal functions\n\n    Returns:\n        numpy matrix of floats of dimension L-by-d. At element (k,2i) the value\n        is sin(k/n^(2i/d)) while at element (k,2i+1) the value is cos(k/n^(2i/d))\n    \"\"\"\n    assert d % 2 == 0, \"Output dimension needs to be an even integer\"\n    d2 = d//2\n    P = np.zeros((L, d))\n    k = np.arange(L).reshape(-1, 1)     # L-column vector\n    i = np.arange(d2).reshape(1, -1)    # d-row vector\n    denom = np.power(n, -i/d2)          # n**(-2*i/d)\n    args = k * denom                    # (L,d) matrix\n    P[:, ::2] = np.sin(args)\n    P[:, 1::2] = np.cos(args)\n    return P\n\nclass PositionalEmbedding(tf.keras.layers.Layer):\n    \"\"\"Positional embedding layer. Assume tokenized input, transform into\n    embedding and returns positional-encoded output.\"\"\"\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        \"\"\"\n        Args:\n            sequence_length: Input sequence length\n            vocab_size: Input vocab size, for setting up embedding matrix\n            embed_dim: Embedding vector size, for setting up embedding matrix\n        \"\"\"\n        super().__init__(**kwargs)\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim     # d_model in paper\n        # token embedding layer: Convert integer token to D-dim float vector\n        self.token_embeddings = tf.keras.layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim, mask_zero=True\n        )\n        # positional embedding layer: a matrix of hard-coded sine values\n        matrix = pos_enc_matrix(sequence_length, embed_dim)\n        self.position_embeddings = tf.constant(matrix, dtype=\"float32\")\n\n    def call(self, inputs):\n        \"\"\"Input tokens convert into embedding vectors then superimposed\n        with position vectors\"\"\"\n        embedded_tokens = self.token_embeddings(inputs)\n        return embedded_tokens + self.position_embeddings\n\n    # this layer is using an Embedding layer, which can take a mask\n    # see https://www.tensorflow.org/guide/keras/masking_and_padding#passing_mask_tensors_directly_to_layers\n    def compute_mask(self, *args, **kwargs):\n        return self.token_embeddings.compute_mask(*args, **kwargs)\n\n    def get_config(self):\n        # to make save and load a model using custom layer possible\n        config = super().get_config()\n        config.update({\n            \"sequence_length\": self.sequence_length,\n            \"vocab_size\": self.vocab_size,\n            \"embed_dim\": self.embed_dim,\n        })\n        return config\n```", "```py\n# From Lesson 03:\n# train_ds = make_dataset(train_pairs)\n\nvocab_size_en = 10000\nseq_length = 20\n\n# test the dataset\nfor inputs, targets in train_ds.take(1):\n    print(inputs[\"encoder_inputs\"])\n    embed_en = PositionalEmbedding(seq_length, vocab_size_en, embed_dim=512)\n    en_emb = embed_en(inputs[\"encoder_inputs\"])\n    print(en_emb.shape)\n    print(en_emb._keras_mask)\n```", "```py\ntf.Tensor(\n[[  10 4288  607 ...    0    0    0]\n [  28   14    4 ...    0    0    0]\n [  63   23  430 ...    2    0    0]\n ...\n [ 136  315  100 ...    0    0    0]\n [   3   20   19 ...    0    0    0]\n [  44   16    6 ...    0    0    0]], shape=(64, 20), dtype=int64)\n(64, 20, 512)\ntf.Tensor(\n[[ True  True  True ... False False False]\n [ True  True  True ... False False False]\n [ True  True  True ...  True False False]\n ...\n [ True  True  True ... False False False]\n [ True  True  True ... False False False]\n [ True  True  True ... False False False]], shape=(64, 20), dtype=bool)\n```", "```py\nimport tensorflow as tf\n\ndef self_attention(input_shape, prefix=\"att\", mask=False, **kwargs):\n    \"\"\"Self-attention layers at transformer encoder and decoder. Assumes its\n    input is the output from positional encoding layer.\n\n    Args:\n        prefix (str): The prefix added to the layer names\n        masked (bool): whether to use causal mask. Should be False on encoder and\n                       True on decoder. When True, a mask will be applied such that\n                       each location only has access to the locations before it.\n    \"\"\"\n    # create layers\n    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n                                   name=f\"{prefix}_in1\")\n    attention = tf.keras.layers.MultiHeadAttention(name=f\"{prefix}_attn1\", **kwargs)\n    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm1\")\n    add = tf.keras.layers.Add(name=f\"{prefix}_add1\")\n    # functional API to connect input to output\n    attout = attention(query=inputs, value=inputs, key=inputs,\n                       use_causal_mask=mask)\n    outputs = norm(add([inputs, attout]))\n    # create model and return\n    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f\"{prefix}_att\")\n    return model\n\nseq_length = 20\nkey_dim = 128\nnum_heads = 8\n\nmodel = self_attention(input_shape=(seq_length, key_dim),\n                       num_heads=num_heads, key_dim=key_dim)\ntf.keras.utils.plot_model(model, \"self-attention.png\",\n                          show_shapes=True, show_dtype=True, show_layer_names=True,\n                          rankdir='BT', show_layer_activations=True)\n```", "```py\nimport tensorflow as tf\n\ndef cross_attention(input_shape, context_shape, prefix=\"att\", **kwargs):\n    \"\"\"Cross-attention layers at transformer decoder. Assumes its\n    input is the output from positional encoding layer at decoder\n    and context is the final output from encoder.\n\n    Args:\n        prefix (str): The prefix added to the layer names\n    \"\"\"\n    # create layers\n    context = tf.keras.layers.Input(shape=context_shape, dtype='float32',\n                                    name=f\"{prefix}_ctx2\")\n    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n                                   name=f\"{prefix}_in2\")\n    attention = tf.keras.layers.MultiHeadAttention(name=f\"{prefix}_attn2\", **kwargs)\n    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm2\")\n    add = tf.keras.layers.Add(name=f\"{prefix}_add2\")\n    # functional API to connect input to output\n    attout = attention(query=inputs, value=context, key=context)\n    outputs = norm(add([attout, inputs]))\n    # create model and return\n    model = tf.keras.Model(inputs=[(context, inputs)], outputs=outputs,\n                           name=f\"{prefix}_cross\")\n    return model\n\nseq_length = 20\nkey_dim = 128\nnum_heads = 8\n\nmodel = cross_attention(input_shape=(seq_length, key_dim),\n                        context_shape=(seq_length, key_dim),\n                        num_heads=num_heads, key_dim=key_dim)\ntf.keras.utils.plot_model(model, \"cross-attention.png\",\n                          show_shapes=True, show_dtype=True, show_layer_names=True,\n                          rankdir='BT', show_layer_activations=True)\n```", "```py\nimport tensorflow as tf\n\ndef feed_forward(input_shape, model_dim, ff_dim, dropout=0.1, prefix=\"ff\"):\n    \"\"\"Feed-forward layers at transformer encoder and decoder. Assumes its\n    input is the output from an attention layer with add & norm, the output\n    is the output of one encoder or decoder block\n\n    Args:\n        model_dim (int): Output dimension of the feed-forward layer, which\n                         is also the output dimension of the encoder/decoder\n                         block\n        ff_dim (int): Internal dimension of the feed-forward layer\n        dropout (float): Dropout rate\n        prefix (str): The prefix added to the layer names\n    \"\"\"\n    # create layers\n    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n                                   name=f\"{prefix}_in3\")\n    dense1 = tf.keras.layers.Dense(ff_dim, name=f\"{prefix}_ff1\", activation=\"relu\")\n    dense2 = tf.keras.layers.Dense(model_dim, name=f\"{prefix}_ff2\")\n    drop = tf.keras.layers.Dropout(dropout, name=f\"{prefix}_drop\")\n    add = tf.keras.layers.Add(name=f\"{prefix}_add3\")\n    # functional API to connect input to output\n    ffout = drop(dense2(dense1(inputs)))\n    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm3\")\n    outputs = norm(add([inputs, ffout]))\n    # create model and return\n    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f\"{prefix}_ff\")\n    return model\n\nseq_length = 20\nkey_dim = 128\nff_dim = 512\n\nmodel = feed_forward(input_shape=(seq_length, key_dim),\n                     model_dim=key_dim, ff_dim=ff_dim)\ntf.keras.utils.plot_model(model, \"feedforward.png\",\n                          show_shapes=True, show_dtype=True, show_layer_names=True,\n                          rankdir='BT', show_layer_activations=True)\n```", "```py\nimport tensorflow as tf\n\n# the building block functions from Lesson 06\nfrom lesson_06 import self_attention, feed_forward\n\ndef encoder(input_shape, key_dim, ff_dim, dropout=0.1, prefix=\"enc\", **kwargs):\n    \"\"\"One encoder unit. The input and output are in the same shape so we can\n    daisy chain multiple encoder units into one larger encoder\"\"\"\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f\"{prefix}_in0\"),\n        self_attention(input_shape, prefix=prefix, key_dim=key_dim, mask=False, **kwargs),\n        feed_forward(input_shape, key_dim, ff_dim, dropout, prefix),\n    ], name=prefix)\n    return model\n\nseq_length = 20\nkey_dim = 128\nff_dim = 512\nnum_heads = 8\n\nmodel = encoder(input_shape=(seq_length, key_dim), key_dim=key_dim, ff_dim=ff_dim,\n                num_heads=num_heads)\ntf.keras.utils.plot_model(model, \"encoder.png\",\n                          show_shapes=True, show_dtype=True, show_layer_names=True,\n                          rankdir='BT', show_layer_activations=True)\n```", "```py\nimport tensorflow as tf\n\n# the three building block functions from Lesson 06\nfrom lesson_06 import self_attention, cross_attention, feed_forward\n\ndef decoder(input_shape, key_dim, ff_dim, dropout=0.1, prefix=\"dec\", **kwargs):\n    \"\"\"One decoder unit. The input and output are in the same shape so we can\n    daisy chain multiple decoder units into one larger decoder. The context\n    vector is also assumed to be the same shape for convenience\"\"\"\n    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n                                   name=f\"{prefix}_in0\")\n    context = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n                                    name=f\"{prefix}_ctx0\")\n    attmodel = self_attention(input_shape, key_dim=key_dim, mask=True,\n                              prefix=prefix, **kwargs)\n    crossmodel = cross_attention(input_shape, input_shape, key_dim=key_dim,\n                                 prefix=prefix, **kwargs)\n    ffmodel = feed_forward(input_shape, key_dim, ff_dim, dropout, prefix)\n    x = attmodel(inputs)\n    x = crossmodel([(context, x)])\n    output = ffmodel(x)\n    model = tf.keras.Model(inputs=[(inputs, context)], outputs=output, name=prefix)\n    return model\n\nseq_length = 20\nkey_dim = 128\nff_dim = 512\nnum_heads = 8\n\nmodel = decoder(input_shape=(seq_length, key_dim), key_dim=key_dim, ff_dim=ff_dim,\n                num_heads=num_heads)\ntf.keras.utils.plot_model(model, \"decoder.png\",\n                          show_shapes=True, show_dtype=True, show_layer_names=True,\n                          rankdir='BT', show_layer_activations=True)\n```", "```py\nimport tensorflow as tf\n\n# the positional embedding layer from Lesson 05\nfrom lesson_05 import PositionalEmbedding\n# the building block functions from Lesson 07\nfrom lesson_07 import encoder, decoder\n\ndef transformer(num_layers, num_heads, seq_len, key_dim, ff_dim, vocab_size_src,\n                vocab_size_tgt, dropout=0.1, name=\"transformer\"):\n    embed_shape = (seq_len, key_dim)  # output shape of the positional embedding layer\n    # set up layers\n    input_enc = tf.keras.layers.Input(shape=(seq_len,), dtype=\"int32\",\n                                      name=\"encoder_inputs\")\n    input_dec = tf.keras.layers.Input(shape=(seq_len,), dtype=\"int32\",\n                                      name=\"decoder_inputs\")\n    embed_enc = PositionalEmbedding(seq_len, vocab_size_src, key_dim, name=\"embed_enc\")\n    embed_dec = PositionalEmbedding(seq_len, vocab_size_tgt, key_dim, name=\"embed_dec\")\n    encoders = [encoder(input_shape=embed_shape, key_dim=key_dim,\n                        ff_dim=ff_dim, dropout=dropout, prefix=f\"enc{i}\",\n                        num_heads=num_heads)\n                for i in range(num_layers)]\n    decoders = [decoder(input_shape=embed_shape, key_dim=key_dim,\n                        ff_dim=ff_dim, dropout=dropout, prefix=f\"dec{i}\",\n                        num_heads=num_heads)\n                for i in range(num_layers)]\n    final = tf.keras.layers.Dense(vocab_size_tgt, name=\"linear\")\n    # build output\n    x1 = embed_enc(input_enc)\n    x2 = embed_dec(input_dec)\n    for layer in encoders:\n        x1 = layer(x1)\n    for layer in decoders:\n        x2 = layer([x2, x1])\n    output = final(x2)\n    # XXX keep this try-except block\n    try:\n        del output._keras_mask\n    except AttributeError:\n        pass\n    model = tf.keras.Model(inputs=[input_enc, input_dec], outputs=output, name=name)\n    return model\n\nseq_len = 20\nnum_layers = 4\nnum_heads = 8\nkey_dim = 128\nff_dim = 512\ndropout = 0.1\nvocab_size_en = 10000\nvocab_size_fr = 20000\nmodel = transformer(num_layers, num_heads, seq_len, key_dim, ff_dim,\n                    vocab_size_en, vocab_size_fr, dropout)\ntf.keras.utils.plot_model(model, \"transformer.png\",\n                          show_shapes=True, show_dtype=True, show_layer_names=True,\n                          rankdir='BT', show_layer_activations=True)\n```", "```py\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    \"Custom learning rate for Adam optimizer\"\n    def __init__(self, key_dim, warmup_steps=4000):\n        super().__init__()\n        self.key_dim = key_dim\n        self.warmup_steps = warmup_steps\n        self.d = tf.cast(self.key_dim, tf.float32)\n\n    def __call__(self, step):\n        step = tf.cast(step, dtype=tf.float32)\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n        return tf.math.rsqrt(self.d) * tf.math.minimum(arg1, arg2)\n\n    def get_config(self):\n        # to make save and load a model using custom layer possible0\n        config = {\n            \"key_dim\": self.key_dim,\n            \"warmup_steps\": self.warmup_steps,\n        }\n        return config\n\nkey_dim = 128\nlr = CustomSchedule(key_dim)\noptimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n\nplt.plot(lr(tf.range(50000, dtype=tf.float32)))\nplt.ylabel('Learning Rate')\nplt.xlabel('Train Step')\nplt.show()\n```", "```py\ndef masked_loss(label, pred):\n    mask = label != 0\n\n    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True, reduction='none')\n    loss = loss_object(label, pred)\n\n    mask = tf.cast(mask, dtype=loss.dtype)\n    loss *= mask\n    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n    return loss\n\ndef masked_accuracy(label, pred):\n    pred = tf.argmax(pred, axis=2)\n    label = tf.cast(label, pred.dtype)\n    match = label == pred\n\n    mask = label != 0\n\n    match = match & mask\n\n    match = tf.cast(match, dtype=tf.float32)\n    mask = tf.cast(mask, dtype=tf.float32)\n    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n```", "```py\nvocab_size_en = 10000\nvocab_size_fr = 20000\nseq_len = 20\nnum_layers = 4\nnum_heads = 8\nkey_dim = 128\nff_dim = 512\ndropout = 0.1\nmodel = transformer(num_layers, num_heads, seq_len, key_dim, ff_dim,\n                    vocab_size_en, vocab_size_fr, dropout)\nlr = CustomSchedule(key_dim)\noptimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\nmodel.compile(loss=masked_loss, optimizer=optimizer, metrics=[masked_accuracy])\nmodel.summary()\n```", "```py\nModel: \"transformer\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to\n==================================================================================================\n encoder_inputs (InputLayer)    [(None, 20)]         0           []\n\n embed_enc (PositionalEmbedding  (None, 20, 128)     1280000     ['encoder_inputs[0][0]']\n )\n\n enc0 (Sequential)              (None, 20, 128)      659712      ['embed_enc[0][0]']\n\n enc1 (Sequential)              (None, 20, 128)      659712      ['enc0[0][0]']\n\n decoder_inputs (InputLayer)    [(None, 20)]         0           []\n\n enc2 (Sequential)              (None, 20, 128)      659712      ['enc1[0][0]']\n\n embed_dec (PositionalEmbedding  (None, 20, 128)     2560000     ['decoder_inputs[0][0]']\n )\n\n enc3 (Sequential)              (None, 20, 128)      659712      ['enc2[0][0]']\n\n dec0 (Functional)              (None, 20, 128)      1187456     ['embed_dec[0][0]',\n                                                                  'enc3[0][0]']\n\n dec1 (Functional)              (None, 20, 128)      1187456     ['dec0[0][0]',\n                                                                  'enc3[0][0]']\n\n dec2 (Functional)              (None, 20, 128)      1187456     ['dec1[0][0]',\n                                                                  'enc3[0][0]']\n\n dec3 (Functional)              (None, 20, 128)      1187456     ['dec2[0][0]',\n                                                                  'enc3[0][0]']\n\n linear (Dense)                 (None, 20, 20000)    2580000     ['dec3[0][0]']\n\n==================================================================================================\nTotal params: 13,808,672\nTrainable params: 13,808,672\nNon-trainable params: 0\n__________________________________________________________________________________________________\n```", "```py\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n# the dataset objects from Lesson 03\nfrom lesson_03 import train_ds, val_ds\n# the building block functions from Lesson 08\nfrom lesson_08 import transformer\n# the learning rate schedule, loss, and accuracy functions from Lesson 09\nfrom lesson_09 import CustomSchedule, masked_loss, masked_accuracy\n\n# Create and train the model\nseq_len = 20\nnum_layers = 4\nnum_heads = 8\nkey_dim = 128\nff_dim = 512\ndropout = 0.1\nvocab_size_en = 10000\nvocab_size_fr = 20000\nmodel = transformer(num_layers, num_heads, seq_len, key_dim, ff_dim,\n                    vocab_size_en, vocab_size_fr, dropout)\nlr = CustomSchedule(key_dim)\noptimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\nmodel.compile(loss=masked_loss, optimizer=optimizer, metrics=[masked_accuracy])\nepochs = 20\nhistory = model.fit(train_ds, epochs=epochs, validation_data=val_ds)\n\n# Save the trained model\nmodel.save(\"eng-fra-transformer.h5\")\n\n# Plot the loss and accuracy history\nfig, axs = plt.subplots(2, figsize=(6, 8), sharex=True)\nfig.suptitle('Traininig history')\nx = list(range(1, epochs+1))\naxs[0].plot(x, history.history[\"loss\"], alpha=0.5, label=\"loss\")\naxs[0].plot(x, history.history[\"val_loss\"], alpha=0.5, label=\"val_loss\")\naxs[0].set_ylabel(\"Loss\")\naxs[0].legend(loc=\"upper right\")\naxs[1].plot(x, history.history[\"masked_accuracy\"], alpha=0.5, label=\"acc\")\naxs[1].plot(x, history.history[\"val_masked_accuracy\"], alpha=0.5, label=\"val_acc\")\naxs[1].set_ylabel(\"Accuracy\")\naxs[1].set_xlabel(\"epoch\")\naxs[1].legend(loc=\"lower right\")\nplt.show()\n```", "```py\nimport pickle\nimport random\n\nimport numpy as np\nimport tensorflow as tf\n\n# the dataset objects from Lesson 03\nfrom lesson_03 import test_pairs, eng_vectorizer, fra_vectorizer\n# the positional embedding layer from Lesson 05\nfrom lesson_05 import PositionalEmbedding\n# the learning rate schedule, loss, and accuracy functions from Lesson 09\nfrom lesson_09 import CustomSchedule, masked_loss, masked_accuracy\n\n# Load the trained model\ncustom_objects = {\"PositionalEmbedding\": PositionalEmbedding,\n                  \"CustomSchedule\": CustomSchedule,\n                  \"masked_loss\": masked_loss,\n                  \"masked_accuracy\": masked_accuracy}\nwith tf.keras.utils.custom_object_scope(custom_objects):\n    model = tf.keras.models.load_model(\"eng-fra-transformer.h5\")\n\n# training parameters used\nseq_len = 20\nvocab_size_en = 10000\nvocab_size_fr = 20000\n\ndef translate(sentence):\n    \"\"\"Create the translated sentence\"\"\"\n    enc_tokens = eng_vectorizer([sentence])\n    lookup = list(fra_vectorizer.get_vocabulary())\n    start_sentinel, end_sentinel = \"[start]\", \"[end]\"\n    output_sentence = [start_sentinel]\n    # generate the translated sentence word by word\n    for i in range(seq_len):\n        vector = fra_vectorizer([\" \".join(output_sentence)])\n        assert vector.shape == (1, seq_len+1)\n        dec_tokens = vector[:, :-1]\n        assert dec_tokens.shape == (1, seq_len)\n        pred = model([enc_tokens, dec_tokens])\n        assert pred.shape == (1, seq_len, vocab_size_fr)\n        word = lookup[np.argmax(pred[0, i, :])]\n        output_sentence.append(word)\n        if word == end_sentinel:\n            break\n    return output_sentence\n\ntest_count = 20\nfor n in range(test_count):\n    english_sentence, french_sentence = random.choice(test_pairs)\n    translated = translate(english_sentence)\n    print(f\"Test {n}:\")\n    print(f\"{english_sentence}\")\n    print(f\"== {french_sentence}\")\n    print(f\"-> {' '.join(translated)}\")\n    print()\n```", "```py\nTest 2:\nit rained for three days .\n== [start] il a plu pendant trois jours . [end]\n-> [start] il a plu pendant trois jours . [end]\n\nTest 3:\ntwo people say they heard a gunshot .\n== [start] deux personnes disent qu'elles ont entendu une détonation . [end]\n-> [start] deux personnes disent qu'ils ont entendu un coup de feu . [end]\n\nTest 4:\ni'm not dead yet .\n== [start] je ne suis pas encore mort . [end]\n-> [start] je ne suis pas encore mort . [end]\n\nTest 5:\ni want us to get back together .\n== [start] je veux que nous nous remettions ensemble . [end]\n-> [start] je veux que nous nous [UNK] ensemble . [end]\n```"]