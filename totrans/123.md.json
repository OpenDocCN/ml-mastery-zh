["```py\nimport matplotlib.pyplot as plt\nimport torchvision\n\ntrain = torchvision.datasets.MNIST('./data', train=True, download=True)\n\nfig, ax = plt.subplots(4, 4, sharex=True, sharey=True)\nfor i in range(4):\n    for j in range(4):\n        ax[i][j].imshow(train.data[4*i+j], cmap=\"gray\")\nplt.show()\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\n\n# Load MNIST data\ntrain = torchvision.datasets.MNIST('data', train=True, download=True)\ntest = torchvision.datasets.MNIST('data', train=True, download=True)\nprint(train.data.shape, train.targets.shape)\nprint(test.data.shape, test.targets.shape)\n```", "```py\ntorch.Size([60000, 28, 28]) torch.Size([60000])\ntorch.Size([10000, 28, 28]) torch.Size([10000])\n```", "```py\n# each sample becomes a vector of values 0-1\nX_train = train.data.reshape(-1, 784).float() / 255.0\ny_train = train.targets\nX_test = test.data.reshape(-1, 784).float() / 255.0\ny_test = test.targets\n```", "```py\nclass Baseline(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(784, 784)\n        self.act1 = nn.ReLU()\n        self.layer2 = nn.Linear(784, 10)\n\n    def forward(self, x):\n        x = self.act1(self.layer1(x))\n        x = self.layer2(x)\n        return x\n```", "```py\nmodel = Baseline()\n\noptimizer = optim.SGD(model.parameters(), lr=0.01)\nloss_fn = nn.CrossEntropyLoss()\nloader = torch.utils.data.DataLoader(list(zip(X_train, y_train)), shuffle=True, batch_size=100)\n\nn_epochs = 10\nfor epoch in range(n_epochs):\n    model.train()\n    for X_batch, y_batch in loader:\n        y_pred = model(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    # Validation\n    model.eval()\n    y_pred = model(X_test)\n    acc = (torch.argmax(y_pred, 1) == y_test).float().mean()\n    print(\"Epoch %d: model accuracy %.2f%%\" % (epoch, acc*100))\n```", "```py\nEpoch 0: model accuracy 84.11%\nEpoch 1: model accuracy 87.53%\nEpoch 2: model accuracy 89.01%\nEpoch 3: model accuracy 89.76%\nEpoch 4: model accuracy 90.29%\nEpoch 5: model accuracy 90.69%\nEpoch 6: model accuracy 91.10%\nEpoch 7: model accuracy 91.48%\nEpoch 8: model accuracy 91.74%\nEpoch 9: model accuracy 91.96%\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\n\n# Load MNIST data\ntrain = torchvision.datasets.MNIST('data', train=True, download=True)\ntest = torchvision.datasets.MNIST('data', train=True, download=True)\n\n# each sample becomes a vector of values 0-1\nX_train = train.data.reshape(-1, 784).float() / 255.0\ny_train = train.targets\nX_test = test.data.reshape(-1, 784).float() / 255.0\ny_test = test.targets\n\nclass Baseline(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(784, 784)\n        self.act1 = nn.ReLU()\n        self.layer2 = nn.Linear(784, 10)\n\n    def forward(self, x):\n        x = self.act1(self.layer1(x))\n        x = self.layer2(x)\n        return x\n\nmodel = Baseline()\n\noptimizer = optim.SGD(model.parameters(), lr=0.01)\nloss_fn = nn.CrossEntropyLoss()\nloader = torch.utils.data.DataLoader(list(zip(X_train, y_train)), shuffle=True, batch_size=100)\n\nn_epochs = 10\nfor epoch in range(n_epochs):\n    model.train()\n    for X_batch, y_batch in loader:\n        y_pred = model(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    # Validation\n    model.eval()\n    y_pred = model(X_test)\n    acc = (torch.argmax(y_pred, 1) == y_test).float().mean()\n    print(\"Epoch %d: model accuracy %.2f%%\" % (epoch, acc*100))\n```", "```py\n...\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize((0,), (128,)),\n])\ntrain = torchvision.datasets.MNIST('data', train=True, download=True, transform=transform)\ntest = torchvision.datasets.MNIST('data', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=100)\ntestloader = torch.utils.data.DataLoader(test, shuffle=True, batch_size=100)\n```", "```py\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(1, 10, kernel_size=5, stride=1, padding=2)\n        self.relu1 = nn.ReLU()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n        self.dropout = nn.Dropout(0.2)\n        self.flat = nn.Flatten()\n        self.fc = nn.Linear(27*27*10, 128)\n        self.relu2 = nn.ReLU()\n        self.output = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.relu1(self.conv(x))\n        x = self.pool(x)\n        x = self.dropout(x)\n        x = self.relu2(self.fc(self.flat(x)))\n        x = self.output(x)\n        return x\n\nmodel = CNN()\n\noptimizer = optim.Adam(model.parameters(), lr=0.01)\nloss_fn = nn.CrossEntropyLoss()\n\nn_epochs = 10\nfor epoch in range(n_epochs):\n    model.train()\n    for X_batch, y_batch in trainloader:\n        y_pred = model(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    # Validation\n    model.eval()\n    acc = 0\n    count = 0\n    for X_batch, y_batch in testloader:\n        y_pred = model(X_batch)\n        acc += (torch.argmax(y_pred, 1) == y_batch).float().sum()\n        count += len(y_batch)\n    acc = acc / count\n    print(\"Epoch %d: model accuracy %.2f%%\" % (epoch, acc*100))\n```", "```py\nEpoch 0: model accuracy 81.74%\nEpoch 1: model accuracy 85.38%\nEpoch 2: model accuracy 86.37%\nEpoch 3: model accuracy 87.75%\nEpoch 4: model accuracy 88.00%\nEpoch 5: model accuracy 88.17%\nEpoch 6: model accuracy 88.81%\nEpoch 7: model accuracy 88.34%\nEpoch 8: model accuracy 88.86%\nEpoch 9: model accuracy 88.75%\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\n\n# Load MNIST data\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize((0,), (128,)),\n])\ntrain = torchvision.datasets.MNIST('data', train=True, download=True, transform=transform)\ntest = torchvision.datasets.MNIST('data', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=100)\ntestloader = torch.utils.data.DataLoader(test, shuffle=True, batch_size=100)\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(1, 10, kernel_size=5, stride=1, padding=2)\n        self.relu1 = nn.ReLU()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n        self.dropout = nn.Dropout(0.2)\n        self.flat = nn.Flatten()\n        self.fc = nn.Linear(27*27*10, 128)\n        self.relu2 = nn.ReLU()\n        self.output = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.relu1(self.conv(x))\n        x = self.pool(x)\n        x = self.dropout(x)\n        x = self.relu2(self.fc(self.flat(x)))\n        x = self.output(x)\n        return x\n\nmodel = CNN()\n\noptimizer = optim.Adam(model.parameters())\nloss_fn = nn.CrossEntropyLoss()\n\nn_epochs = 10\nfor epoch in range(n_epochs):\n    model.train()\n    for X_batch, y_batch in trainloader:\n        y_pred = model(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    # Validation\n    model.eval()\n    acc = 0\n    count = 0\n    for X_batch, y_batch in testloader:\n        y_pred = model(X_batch)\n        acc += (torch.argmax(y_pred, 1) == y_batch).float().sum()\n        count += len(y_batch)\n    acc = acc / count\n    print(\"Epoch %d: model accuracy %.2f%%\" % (epoch, acc*100))\n```", "```py\nclass LeNet5(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n        self.act1 = nn.Tanh()\n        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)\n        self.act2 = nn.Tanh()\n        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n\n        self.conv3 = nn.Conv2d(16, 120, kernel_size=5, stride=1, padding=0)\n        self.act3 = nn.Tanh()\n\n        self.flat = nn.Flatten()\n        self.fc1 = nn.Linear(1*1*120, 84)\n        self.act4 = nn.Tanh()\n        self.fc2 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # input 1x28x28, output 6x28x28\n        x = self.act1(self.conv1(x))\n        # input 6x28x28, output 6x14x14\n        x = self.pool1(x)\n        # input 6x14x14, output 16x10x10\n        x = self.act2(self.conv2(x))\n        # input 16x10x10, output 16x5x5\n        x = self.pool2(x)\n        # input 16x5x5, output 120x1x1\n        x = self.act3(self.conv3(x))\n        # input 120x1x1, output 84\n        x = self.act4(self.fc1(self.flat(x)))\n        # input 84, output 10\n        x = self.fc2(x)\n        return x\n```", "```py\n...\nmodel = LeNet5()\n\noptimizer = optim.Adam(model.parameters())\nloss_fn = nn.CrossEntropyLoss()\n\nn_epochs = 10\nfor epoch in range(n_epochs):\n    model.train()\n    for X_batch, y_batch in trainloader:\n        y_pred = model(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    # Validation\n    model.eval()\n    acc = 0\n    count = 0\n    for X_batch, y_batch in testloader:\n        y_pred = model(X_batch)\n        acc += (torch.argmax(y_pred, 1) == y_batch).float().sum()\n        count += len(y_batch)\n    acc = acc / count\n    print(\"Epoch %d: model accuracy %.2f%%\" % (epoch, acc*100))\n```", "```py\nEpoch 0: model accuracy 89.46%\nEpoch 1: model accuracy 93.14%\nEpoch 2: model accuracy 94.69%\nEpoch 3: model accuracy 95.84%\nEpoch 4: model accuracy 96.43%\nEpoch 5: model accuracy 96.99%\nEpoch 6: model accuracy 97.14%\nEpoch 7: model accuracy 97.66%\nEpoch 8: model accuracy 98.05%\nEpoch 9: model accuracy 98.22%\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\n\n# Load MNIST data\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize((0,), (128,)),\n])\ntrain = torchvision.datasets.MNIST('data', train=True, download=True, transform=transform)\ntest = torchvision.datasets.MNIST('data', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=100)\ntestloader = torch.utils.data.DataLoader(test, shuffle=True, batch_size=100)\n\nclass LeNet5(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n        self.act1 = nn.Tanh()\n        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)\n        self.act2 = nn.Tanh()\n        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n\n        self.conv3 = nn.Conv2d(16, 120, kernel_size=5, stride=1, padding=0)\n        self.act3 = nn.Tanh()\n\n        self.flat = nn.Flatten()\n        self.fc1 = nn.Linear(1*1*120, 84)\n        self.act4 = nn.Tanh()\n        self.fc2 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # input 1x28x28, output 6x28x28\n        x = self.act1(self.conv1(x))\n        # input 6x28x28, output 6x14x14\n        x = self.pool1(x)\n        # input 6x14x14, output 16x10x10\n        x = self.act2(self.conv2(x))\n        # input 16x10x10, output 16x5x5\n        x = self.pool2(x)\n        # input 16x5x5, output 120x1x1\n        x = self.act3(self.conv3(x))\n        # input 120x1x1, output 84\n        x = self.act4(self.fc1(self.flat(x)))\n        # input 84, output 10\n        x = self.fc2(x)\n        return x\n\nmodel = LeNet5()\n\noptimizer = optim.Adam(model.parameters())\nloss_fn = nn.CrossEntropyLoss()\n\nn_epochs = 10\nfor epoch in range(n_epochs):\n    model.train()\n    for X_batch, y_batch in trainloader:\n        y_pred = model(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    # Validation\n    model.eval()\n    acc = 0\n    count = 0\n    for X_batch, y_batch in testloader:\n        y_pred = model(X_batch)\n        acc += (torch.argmax(y_pred, 1) == y_batch).float().sum()\n        count += len(y_batch)\n    acc = acc / count\n    print(\"Epoch %d: model accuracy %.2f%%\" % (epoch, acc*100))\n```"]