["```py\nclass MultiHeadAttention(Layer):\n    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n        super(MultiHeadAttention, self).__init__(**kwargs)\n        self.attention = DotProductAttention()  # Scaled dot product attention \n        self.heads = h  # Number of attention heads to use\n        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n        self.d_v = d_v  # Dimensionality of the linearly projected values\n        self.W_q = Dense(d_k)  # Learned projection matrix for the queries\n        self.W_k = Dense(d_k)  # Learned projection matrix for the keys\n        self.W_v = Dense(d_v)  # Learned projection matrix for the values\n        self.W_o = Dense(d_model)  # Learned projection matrix for the multi-head output\n        ...\n```", "```py\nfrom tensorflow import matmul, math, cast, float32\nfrom tensorflow.keras.layers import Layer\nfrom keras.backend import softmax\n\n# Implementing the Scaled-Dot Product Attention\nclass DotProductAttention(Layer):\n    def __init__(self, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n\n    def call(self, queries, keys, values, d_k, mask=None):\n        # Scoring the queries against the keys after transposing the latter, and scaling\n        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n\n        # Apply mask to the attention scores\n        if mask is not None:\n            scores += -1e9 * mask\n\n        # Computing the weights by a softmax operation\n        weights = softmax(scores)\n\n        # Computing the attention by a weighted sum of the value vectors\n        return matmul(weights, values)\n```", "```py\ndef reshape_tensor(self, x, heads, flag):\n    if flag:\n        # Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1)\n        x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n        x = transpose(x, perm=(0, 2, 1, 3))\n    else:\n        # Reverting the reshaping and transposing operations: (batch_size, seq_length, d_model)\n        x = transpose(x, perm=(0, 2, 1, 3))\n        x = reshape(x, shape=(shape(x)[0], shape(x)[1], -1))\n    return x\n```", "```py\ndef call(self, queries, keys, values, mask=None):\n    # Rearrange the queries to be able to compute all heads in parallel\n    q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n\n    # Rearrange the keys to be able to compute all heads in parallel\n    k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n\n    # Rearrange the values to be able to compute all heads in parallel\n    v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n\n    # Compute the multi-head attention output using the reshaped queries, keys and values\n    o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n    ...\n```", "```py\n...\n# Rearrange back the output into concatenated form\noutput = self.reshape_tensor(o_reshaped, self.heads, False)\n# Resulting tensor shape: (batch_size, input_seq_length, d_v)\n\n# Apply one final linear projection to the output to generate the multi-head attention\n# Resulting tensor shape: (batch_size, input_seq_length, d_model)\nreturn self.W_o(output)\n```", "```py\nfrom tensorflow import math, matmul, reshape, shape, transpose, cast, float32\nfrom tensorflow.keras.layers import Dense, Layer\nfrom keras.backend import softmax\n\n# Implementing the Scaled-Dot Product Attention\nclass DotProductAttention(Layer):\n    def __init__(self, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n\n    def call(self, queries, keys, values, d_k, mask=None):\n        # Scoring the queries against the keys after transposing the latter, and scaling\n        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n\n        # Apply mask to the attention scores\n        if mask is not None:\n            scores += -1e9 * mask\n\n        # Computing the weights by a softmax operation\n        weights = softmax(scores)\n\n        # Computing the attention by a weighted sum of the value vectors\n        return matmul(weights, values)\n\n# Implementing the Multi-Head Attention\nclass MultiHeadAttention(Layer):\n    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n        super(MultiHeadAttention, self).__init__(**kwargs)\n        self.attention = DotProductAttention()  # Scaled dot product attention\n        self.heads = h  # Number of attention heads to use\n        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n        self.d_v = d_v  # Dimensionality of the linearly projected values\n        self.d_model = d_model  # Dimensionality of the model\n        self.W_q = Dense(d_k)  # Learned projection matrix for the queries\n        self.W_k = Dense(d_k)  # Learned projection matrix for the keys\n        self.W_v = Dense(d_v)  # Learned projection matrix for the values\n        self.W_o = Dense(d_model)  # Learned projection matrix for the multi-head output\n\n    def reshape_tensor(self, x, heads, flag):\n        if flag:\n            # Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1)\n            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n            x = transpose(x, perm=(0, 2, 1, 3))\n        else:\n            # Reverting the reshaping and transposing operations: (batch_size, seq_length, d_k)\n            x = transpose(x, perm=(0, 2, 1, 3))\n            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n        return x\n\n    def call(self, queries, keys, values, mask=None):\n        # Rearrange the queries to be able to compute all heads in parallel\n        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n\n        # Rearrange the keys to be able to compute all heads in parallel\n        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n\n        # Rearrange the values to be able to compute all heads in parallel\n        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n\n        # Compute the multi-head attention output using the reshaped queries, keys and values\n        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n\n        # Rearrange back the output into concatenated form\n        output = self.reshape_tensor(o_reshaped, self.heads, False)\n        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n\n        # Apply one final linear projection to the output to generate the multi-head attention\n        # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n        return self.W_o(output)\n```", "```py\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_model = 512  # Dimensionality of the model sub-layers' outputs\nbatch_size = 64  # Batch size from the training process\n...\n```", "```py\n...\ninput_seq_length = 5  # Maximum length of the input sequence\n\nqueries = random.random((batch_size, input_seq_length, d_k))\nkeys = random.random((batch_size, input_seq_length, d_k))\nvalues = random.random((batch_size, input_seq_length, d_v))\n...\n```", "```py\n...\nmultihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n...\n```", "```py\n...\nprint(multihead_attention(queries, keys, values))\n```", "```py\nfrom numpy import random\n\ninput_seq_length = 5  # Maximum length of the input sequence\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_model = 512  # Dimensionality of the model sub-layers' outputs\nbatch_size = 64  # Batch size from the training process\n\nqueries = random.random((batch_size, input_seq_length, d_k))\nkeys = random.random((batch_size, input_seq_length, d_k))\nvalues = random.random((batch_size, input_seq_length, d_v))\n\nmultihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\nprint(multihead_attention(queries, keys, values))\n```", "```py\ntf.Tensor(\n[[[-0.02185373  0.32784638  0.15958631 ... -0.0353895   0.6645204\n   -0.2588266 ]\n  [-0.02272229  0.32292002  0.16208754 ... -0.03644213  0.66478664\n   -0.26139447]\n  [-0.01876744  0.32900316  0.16190802 ... -0.03548665  0.6645842\n   -0.26155376]\n  [-0.02193783  0.32687354  0.15801215 ... -0.03232524  0.6642926\n   -0.25795174]\n  [-0.02224652  0.32437912  0.1596448  ... -0.0340827   0.6617497\n   -0.26065096]]\n ...\n\n [[ 0.05414441  0.27019292  0.1845745  ...  0.0809482   0.63738805\n   -0.34231138]\n  [ 0.05546578  0.27191412  0.18483458 ...  0.08379208  0.6366671\n   -0.34372014]\n  [ 0.05190979  0.27185103  0.18378328 ...  0.08341806  0.63851804\n   -0.3422392 ]\n  [ 0.05437043  0.27318984  0.18792395 ...  0.08043509  0.6391771\n   -0.34357914]\n  [ 0.05406848  0.27073097  0.18579456 ...  0.08388947  0.6376929\n   -0.34230167]]], shape=(64, 5, 512), dtype=float32)\n```"]