- en: A Bird’s Eye View of Research on Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://machinelearningmastery.com/a-birds-eye-view-of-research-on-attention/](https://machinelearningmastery.com/a-birds-eye-view-of-research-on-attention/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Attention is a concept that is scientifically studied across multiple disciplines,
    including psychology, neuroscience, and, more recently, machine learning. While
    all disciplines may have produced their own definitions for attention, one core
    quality they can all agree on is that attention is a mechanism for making both
    biological and artificial neural systems more flexible.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, you will discover an overview of the research advances on
    attention.
  prefs: []
  type: TYPE_NORMAL
- en: 'After completing this tutorial, you will know:'
  prefs: []
  type: TYPE_NORMAL
- en: The concept of attention that is of significance to different scientific disciplines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How attention is revolutionizing machine learning, specifically in the domains
    of natural language processing and computer vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kick-start your project** with my book [Building Transformer Models with
    Attention](https://machinelearningmastery.com/transformer-models-with-attention/).
    It provides **self-study tutorials** with **working code** to guide you into building
    a fully-working transformer model that can'
  prefs: []
  type: TYPE_NORMAL
- en: '*translate sentences from one language to another*...'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/9ded8ab94506cf197f8f9f12b23ecb8d.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_cover-scaled.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: A bird’s-eye view of research on attention
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Chris Lawton](https://unsplash.com/photos/6tfO1M8_gas), some rights
    reserved.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tutorial Overview**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This tutorial is divided into two parts; they are:'
  prefs: []
  type: TYPE_NORMAL
- en: The Concept of Attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention in Machine Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention in Natural Language Processing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention in Computer Vision
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Concept of Attention**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Research on attention finds its origin in the field of psychology.
  prefs: []
  type: TYPE_NORMAL
- en: '*The scientific study of attention began in psychology, where careful behavioral
    experimentation can give rise to precise demonstrations of the tendencies and
    abilities of attention in different circumstances. *'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*–* [Attention in Psychology, Neuroscience, and Machine Learning](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full),
    2020.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Observations derived from such studies could help researchers infer the mental
    processes underlying such behavioral patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the different fields of psychology, neuroscience, and, more recently,
    machine learning have all produced their own definitions of attention, there is
    one core quality that is of great significance to all:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Attention is the flexible control of limited computational resources. *'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*–* [Attention in Psychology, Neuroscience, and Machine Learning](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full),
    2020.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With this in mind, the following sections review the role of attention in revolutionizing
    the field of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Attention in Machine Learning**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of attention in machine learning is *very* loosely inspired by the
    psychological mechanisms of attention in the human brain.
  prefs: []
  type: TYPE_NORMAL
- en: '*The use of attention mechanisms in artificial neural networks came about —
    much like the apparent need for attention in the brain — as a means of making
    neural systems more flexible. *'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*–* [Attention in Psychology, Neuroscience, and Machine Learning](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full),
    2020.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The idea is to be able to work with an artificial neural network that can perform
    well on tasks where the input may be of variable length, size, or structure or
    even handle several different tasks. It is in this spirit that attention mechanisms
    in machine learning are said to inspire themselves from psychology rather than
    because they replicate the biology of the human brain.
  prefs: []
  type: TYPE_NORMAL
- en: '*In the form of attention originally developed for ANNs, attention mechanisms
    worked within an encoder-decoder framework and in the context of sequence models
    …*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*–* [Attention in Psychology, Neuroscience, and Machine Learning](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full),
    2020.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The task of the [encoder](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/)
    is to generate a vector representation of the input, whereas the task of the [decoder](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/)
    is to transform this vector representation into an output. The attention mechanism
    connects the two.
  prefs: []
  type: TYPE_NORMAL
- en: There have been different propositions of neural network architectures that
    implement attention mechanisms, which are also tied to the specific applications
    in which they find their use. Natural language processing (NLP) and computer vision
    are among the most popular applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**Attention in Natural Language Processing**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An early application for attention in NLP was machine translation, where the
    goal was to translate an input sentence in a source language to an output sentence
    in a target language. Within this context, the encoder would generate a set of
    *context* vectors, one for each word in the source sentence. The decoder, on the
    other hand, would read the context vectors to generate an output sentence in the
    target language, one word at a time.
  prefs: []
  type: TYPE_NORMAL
- en: '*In the traditional encoder-decoder framework without attention, the encoder
    produced a fixed-length vector that was independent of the length or features
    of the input and static during the course of decoding.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*–* [Attention in Psychology, Neuroscience, and Machine Learning](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full),
    2020.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Representing the input by a fixed-length vector was especially problematic for
    long sequences or sequences that were complex in structure since the dimensionality
    of their representation was forced to be the same as with shorter or simpler sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '*For example, in some languages, such as Japanese, the last word might be very
    important to predict the first word, while translating English to French might
    be easier as the order of the sentences (how the sentence is organized) is more
    similar to each other. *'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*–* [Attention in Psychology, Neuroscience, and Machine Learning](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full),
    2020.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This created a bottleneck whereby the decoder has limited access to the information
    provided by the input—that which is available within the fixed-length encoding
    vector. On the other hand, preserving the length of the input sequence during
    the encoding process could make it possible for the decoder to utilize its most
    relevant parts in a flexible manner.
  prefs: []
  type: TYPE_NORMAL
- en: The latter is how the attention mechanism operates.
  prefs: []
  type: TYPE_NORMAL
- en: '*Attention helps determine which of these vectors should be used to generate
    the output. Because the output sequence is dynamically generated one element at
    a time, attention can dynamically highlight different encoded vectors at each
    time point. This allows the decoder to flexibly utilize the most relevant parts
    of the input sequence.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – Page 186, [Deep Learning Essentials](https://www.amazon.com/Deep-Learning-Essentials-hands-fundamentals/dp/1785880365),
    2018.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'One of the earliest works in machine translation that sought to address the
    bottleneck problem created by fixed-length vectors was by [Bahdanau et al. (2014)](https://arxiv.org/abs/1409.0473).
    In their work, Bahdanau et al. employed the use of Recurrent Neural Networks (RNNs)
    for both encoding and decoding tasks: the encoder employs a bi-directional RNN
    to generate a sequence of *annotations* that each contain a summary of both preceding
    and succeeding words that can be mapped into a *context* vector through a weighted
    sum; the decoder then generates an output based on these annotations and the hidden
    states of another RNN. Since the context vector is computed by a weighted sum
    of the annotations, then Bahdanau et al.’s attention mechanism is an example of
    [*soft attention*](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/).'
  prefs: []
  type: TYPE_NORMAL
- en: Another of the earliest works was by [Sutskever et al. (2014)](https://arxiv.org/abs/1409.3215).
    They, alternatively, made use of multilayered Long Short-Term Memory (LSTM) to
    encode a vector representing the input sequence and another LSTM to decode the
    vector into a target sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '[Luong et al. (2015)](https://arxiv.org/abs/1508.04025) introduced the idea
    of *global* versus *local* attention. In their work, they described a global attention
    model as one that, when deriving the context vector, considers all the hidden
    states of the encoder. The computation of the global context vector is, therefore,
    based on a weighted average of *all* the words in the source sequence. Luong et
    al. mentioned that this is computationally expensive and could potentially make
    global attention difficult to be applied to long sequences. Local attention is
    proposed to address this problem by focusing on a smaller subset of the words
    in the source sequence per target word. Luong et al. explained that local attention
    trades off the [*soft*](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/)
    and [*hard*](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/)
    attentional models of [Xu et al. (2016)](https://arxiv.org/abs/1502.03044) (we
    will refer to this paper again in the next section) by being less computationally
    expensive than the soft attention but easier to train than the hard attention.'
  prefs: []
  type: TYPE_NORMAL
- en: More recently, [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) proposed
    an entirely different architecture that has steered the field of machine translation
    in a new direction. Termed *Transformer*, their architecture dispenses with any
    recurrence and convolutions altogether but implements a *self-attention* mechanism.
    Words in the source sequence are first encoded in parallel to generate key, query,
    and value representations. The keys and queries are combined to generate attention
    weightings that capture how each word relates to the others in the sequence. These
    attention weightings are then used to scale the values, in order to retain focus
    on the important words and drown out the irrelevant ones.
  prefs: []
  type: TYPE_NORMAL
- en: '*The output is computed as a weighted sum of the values, where the weight assigned
    to each value is computed by a compatibility function of the query with the corresponding
    key.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf), 2017.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[![](../Images/5cc2fa00063bfd70298252dce57dbdcd.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer architecture
  prefs: []
  type: TYPE_NORMAL
- en: Taken from “Attention Is All You Need”
  prefs: []
  type: TYPE_NORMAL
- en: At the time, the proposed Transformer architecture established a new state-of-the-art
    process for English-to-German and English-to-French translation tasks. It was
    reportedly also faster to train than architectures based on recurrent or convolutional
    layers. Subsequently, the method called BERT by [Devlin et al. (2019)](https://arxiv.org/abs/1810.04805)
    built on Vaswani et al.’s work by proposing a multi-layer bi-directional architecture.
  prefs: []
  type: TYPE_NORMAL
- en: As we shall see shortly, the uptake of the Transformer architecture was not
    only rapid in the domain of NLP but also in the computer vision domain.
  prefs: []
  type: TYPE_NORMAL
- en: '**Attention in Computer Vision**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In computer vision, attention has found its way into several applications, such
    as in the domains of image classification, image segmentation, and image captioning.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we had to reframe the encoder-decoder model to the task of image
    captioning, then the encoder could be a Convolutional Neural Network (CNN) that
    captured the salient visual cues in the images into a vector representation. And
    the decoder could be an RNN or LSTM that transformed the vector representation
    into an output.
  prefs: []
  type: TYPE_NORMAL
- en: '*Also, as in the neuroscience literature, these attentional processes can be
    divided into spatial and feature-based attention. *'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*–* [Attention in Psychology, Neuroscience, and Machine Learning](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full),
    2020.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In *spatial* attention, different spatial locations are attributed different
    weights. However, these same weights are retained across all feature channels
    at the different spatial locations.
  prefs: []
  type: TYPE_NORMAL
- en: One of the fundamental image captioning approaches working with spatial attention
    has been proposed by [Xu et al. (2016)](https://arxiv.org/abs/1502.03044). Their
    model incorporates a CNN as an encoder that extracts a set of feature vectors
    (or *annotation* vectors), with each vector corresponding to a different part
    of the image to allow the decoder to focus selectively on specific image parts.
    The decoder is an LSTM that generates a caption based on a context vector, the
    previously hidden state, and the previously generated words. Xu et al. investigate
    the use of [*hard attention*](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/)
    as an alternative to [soft attention](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/)
    in computing their context vector. Here, soft attention places weights *softly*
    on all patches of the source image, whereas hard attention attends to a single
    patch alone while disregarding the rest. They report that, in their work, hard
    attention performs better.
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/c12d91e0601ae834180a0c76b377b649.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Model for image caption generation
  prefs: []
  type: TYPE_NORMAL
- en: 'Taken from “Show, Attend and Tell: Neural Image Caption Generation with Visual
    Attention”'
  prefs: []
  type: TYPE_NORMAL
- en: '*Feature* attention, in comparison, permits individual feature maps to be attributed
    their own weight values. One such example, also applied to image captioning, is
    the encoder-decoder framework of [Chen et al. (2018)](https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_SCA-CNN_Spatial_and_CVPR_2017_paper.pdf),
    which incorporates spatial and channel-wise attentions in the same CNN.'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly to how the Transformer has quickly become the standard architecture
    for NLP tasks, it has also been recently taken up and adapted by the computer
    vision community.
  prefs: []
  type: TYPE_NORMAL
- en: The earliest work to do so was proposed by [Dosovitskiy et al. (2020)](https://arxiv.org/abs/2010.11929),
    who applied their *Vision Transformer* (ViT) to an image classification task.
    They argued that the long-standing reliance on CNNs for image classification was
    not necessary, and the same task could be accomplished by a pure transformer.
    Dosovitskiy et al. reshape an input image into a sequence of flattened 2D image
    patches, which they subsequently embed by a trainable linear projection to generate
    the *patch embeddings*. These patch embeddings, together with their *position
    embeddings*, to retain positional information, are fed into the encoder part of
    the Transformer architecture, whose output is subsequently fed into a Multilayer
    Perceptron (MLP) for classification.
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/d7e4076797bdf6cc13b250e5a7312329.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_3.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The Vision Transformer architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'Taken from “An Image is Worth 16×16 Words: Transformers for Image Recognition
    at Scale”'
  prefs: []
  type: TYPE_NORMAL
- en: '*Inspired by ViT, and the fact that attention-based architectures are an intuitive
    choice for modelling long-range contextual relationships in video, we develop
    several transformer-based models for video classification.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '– [ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691), 2021.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Arnab et al. (2021)](https://arxiv.org/abs/2103.15691) subsequently extended
    the ViT model to ViViT, which exploits the spatiotemporal information contained
    within videos for the task of video classification. Their method explores different
    approaches of extracting the spatiotemporal data, such as by sampling and embedding
    each frame independently or by extracting non-overlapping tubelets (an image patch
    that spans across several image frames, creating a *tube*) and embedding each
    one in turn. They also investigate different methods of factorizing the spatial
    and temporal dimensions of the input video for increased efficiency and scalability.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/3dbe074038add2c8e6771d951e9c9505.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_4.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The Video Vision Transformer architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'Taken from “ViViT: A Video Vision Transformer”'
  prefs: []
  type: TYPE_NORMAL
- en: Correctness · Re
  prefs: []
  type: TYPE_NORMAL
- en: In its first application for image classification, the Vision Transformer is
    already being applied to several other computer vision domains, such as [action
    localization](https://arxiv.org/abs/2106.08061), [gaze estimation](https://arxiv.org/abs/2105.14424),
    and [image generation](https://arxiv.org/abs/2107.04589). This surge of interest
    among computer vision practitioners suggests an exciting near future, where we’ll
    be seeing more adaptations and applications of the Transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Get Started With Building Transformer Models with Attention?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Take my free 12-day email crash course now (with sample code).
  prefs: []
  type: TYPE_NORMAL
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Reading**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section provides more resources on the topic if you are looking to go deeper.
  prefs: []
  type: TYPE_NORMAL
- en: '**Books**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Deep Learning Essentials](https://www.amazon.com/Deep-Learning-Essentials-hands-fundamentals/dp/1785880365),
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Papers**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Attention in Psychology, Neuroscience, and Machine Learning](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full),
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473),
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215),
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025),
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Attention Is All You Need](https://arxiv.org/abs/1706.03762), 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805),
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044),
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks for
    Image Captioning](https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_SCA-CNN_Spatial_and_CVPR_2017_paper.pdf),
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929),
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691), 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example Applications:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Relation Modeling in Spatio-Temporal Action Localization](https://arxiv.org/abs/2106.08061),
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Gaze Estimation using Transformer](https://arxiv.org/abs/2105.14424), 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ViTGAN: Training GANs with Vision Transformers](https://arxiv.org/abs/2107.04589),
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this tutorial, you discovered an overview of the research advances on attention.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, you learned:'
  prefs: []
  type: TYPE_NORMAL
- en: The concept of attention that is of significance to different scientific disciplines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How attention is revolutionizing machine learning, specifically in the domains
    of natural language processing and computer vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you have any questions?
  prefs: []
  type: TYPE_NORMAL
- en: Ask your questions in the comments below, and I will do my best to answer.
  prefs: []
  type: TYPE_NORMAL
