- en: A Bird’s Eye View of Research on Attention
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对注意力研究的总体概述
- en: 原文：[https://machinelearningmastery.com/a-birds-eye-view-of-research-on-attention/](https://machinelearningmastery.com/a-birds-eye-view-of-research-on-attention/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/a-birds-eye-view-of-research-on-attention/](https://machinelearningmastery.com/a-birds-eye-view-of-research-on-attention/)
- en: Attention is a concept that is scientifically studied across multiple disciplines,
    including psychology, neuroscience, and, more recently, machine learning. While
    all disciplines may have produced their own definitions for attention, one core
    quality they can all agree on is that attention is a mechanism for making both
    biological and artificial neural systems more flexible.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力是一个在多个学科中科学研究的概念，包括心理学、神经科学，以及最近的机器学习。虽然各个学科可能对注意力有不同的定义，但它们都一致认为，注意力是使生物和人工神经系统更具灵活性的机制。
- en: In this tutorial, you will discover an overview of the research advances on
    attention.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你将发现关于注意力研究进展的概述。
- en: 'After completing this tutorial, you will know:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本教程后，你将了解到：
- en: The concept of attention that is of significance to different scientific disciplines
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对不同科学学科具有重要意义的注意力概念
- en: How attention is revolutionizing machine learning, specifically in the domains
    of natural language processing and computer vision
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力如何在机器学习中引发革命，特别是在自然语言处理和计算机视觉领域
- en: '**Kick-start your project** with my book [Building Transformer Models with
    Attention](https://machinelearningmastery.com/transformer-models-with-attention/).
    It provides **self-study tutorials** with **working code** to guide you into building
    a fully-working transformer model that can'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**启动你的项目**，请参阅我的书籍 [《使用注意力构建变换器模型》](https://machinelearningmastery.com/transformer-models-with-attention/)。它提供了**自学教程**和**工作代码**，指导你构建一个完全可用的变换器模型。'
- en: '*translate sentences from one language to another*...'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*将句子从一种语言翻译成另一种语言*...'
- en: Let’s get started.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: '[![](../Images/9ded8ab94506cf197f8f9f12b23ecb8d.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_cover-scaled.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/9ded8ab94506cf197f8f9f12b23ecb8d.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_cover-scaled.jpg)'
- en: A bird’s-eye view of research on attention
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对注意力研究的总体概述
- en: Photo by [Chris Lawton](https://unsplash.com/photos/6tfO1M8_gas), some rights
    reserved.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Chris Lawton](https://unsplash.com/photos/6tfO1M8_gas) 提供，部分权利保留。
- en: '**Tutorial Overview**'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**教程概述**'
- en: 'This tutorial is divided into two parts; they are:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程分为两个部分，分别是：
- en: The Concept of Attention
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力的概念
- en: Attention in Machine Learning
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习中的注意力
- en: Attention in Natural Language Processing
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理中的注意力
- en: Attention in Computer Vision
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉中的注意力
- en: '**The Concept of Attention**'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**注意力的概念**'
- en: Research on attention finds its origin in the field of psychology.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对注意力的研究源于心理学领域。
- en: '*The scientific study of attention began in psychology, where careful behavioral
    experimentation can give rise to precise demonstrations of the tendencies and
    abilities of attention in different circumstances. *'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*对注意力的科学研究始于心理学，通过细致的行为实验可以精准展示注意力在不同情况下的倾向和能力。*'
- en: ''
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*–* [Attention in Psychology, Neuroscience, and Machine Learning](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full),
    2020.'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*–* [心理学、神经科学和机器学习中的注意力](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full)，2020年。'
- en: Observations derived from such studies could help researchers infer the mental
    processes underlying such behavioral patterns.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些研究中得出的观察结果可以帮助研究人员推断出这些行为模式背后的心理过程。
- en: 'While the different fields of psychology, neuroscience, and, more recently,
    machine learning have all produced their own definitions of attention, there is
    one core quality that is of great significance to all:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管心理学、神经科学以及最近的机器学习领域都对注意力有各自的定义，但有一个核心特质对所有领域都具有重要意义：
- en: '*Attention is the flexible control of limited computational resources. *'
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*注意力是对有限计算资源的灵活控制。*'
- en: ''
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*–* [Attention in Psychology, Neuroscience, and Machine Learning](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full),
    2020.'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*–* [心理学、神经科学和机器学习中的注意力](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full)，2020年。'
- en: With this in mind, the following sections review the role of attention in revolutionizing
    the field of machine learning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，接下来的部分将回顾注意力在引领机器学习领域革命中的角色。
- en: '**Attention in Machine Learning**'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**机器学习中的注意力**'
- en: The concept of attention in machine learning is *very* loosely inspired by the
    psychological mechanisms of attention in the human brain.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的注意力概念*非常*松散地受到人脑注意力心理机制的启发。
- en: '*The use of attention mechanisms in artificial neural networks came about —
    much like the apparent need for attention in the brain — as a means of making
    neural systems more flexible. *'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*注意力机制在人工神经网络中的使用出现了——就像大脑中对注意力的明显需求一样——作为使神经系统更加灵活的一种手段。*'
- en: ''
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*–* [Attention in Psychology, Neuroscience, and Machine Learning](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full),
    2020.'
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*–* [心理学、神经科学与机器学习中的注意力](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full)，2020。'
- en: The idea is to be able to work with an artificial neural network that can perform
    well on tasks where the input may be of variable length, size, or structure or
    even handle several different tasks. It is in this spirit that attention mechanisms
    in machine learning are said to inspire themselves from psychology rather than
    because they replicate the biology of the human brain.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是能够处理一种能够在输入可能具有不同长度、大小或结构，甚至处理几个不同任务的人工神经网络。正是在这种精神下，机器学习中的注意力机制被认为是从心理学中获得灵感的，而不是因为它们复制了人脑的生物学。
- en: '*In the form of attention originally developed for ANNs, attention mechanisms
    worked within an encoder-decoder framework and in the context of sequence models
    …*'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*在最初为人工神经网络（ANNs）开发的注意力形式中，注意力机制在编码器-解码器框架和序列模型的背景下工作……*'
- en: ''
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*–* [Attention in Psychology, Neuroscience, and Machine Learning](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full),
    2020.'
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*–* [心理学、神经科学与机器学习中的注意力](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full)，2020。'
- en: The task of the [encoder](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/)
    is to generate a vector representation of the input, whereas the task of the [decoder](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/)
    is to transform this vector representation into an output. The attention mechanism
    connects the two.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[编码器](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/)的任务是生成输入的向量表示，而[解码器](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/)的任务是将这个向量表示转换为输出。注意力机制将二者连接起来。'
- en: There have been different propositions of neural network architectures that
    implement attention mechanisms, which are also tied to the specific applications
    in which they find their use. Natural language processing (NLP) and computer vision
    are among the most popular applications.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有不同的神经网络架构提议实现注意力机制，这些架构也与其应用的特定领域相关。自然语言处理（NLP）和计算机视觉是最受欢迎的应用之一。
- en: '**Attention in Natural Language Processing**'
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**自然语言处理中的注意力**'
- en: An early application for attention in NLP was machine translation, where the
    goal was to translate an input sentence in a source language to an output sentence
    in a target language. Within this context, the encoder would generate a set of
    *context* vectors, one for each word in the source sentence. The decoder, on the
    other hand, would read the context vectors to generate an output sentence in the
    target language, one word at a time.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，早期的注意力应用是机器翻译，其目标是将源语言中的输入句子翻译为目标语言中的输出句子。在这个背景下，编码器会生成一组*上下文*向量，每个词一个。解码器则读取这些上下文向量，以逐字生成目标语言中的输出句子。
- en: '*In the traditional encoder-decoder framework without attention, the encoder
    produced a fixed-length vector that was independent of the length or features
    of the input and static during the course of decoding.*'
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*在没有注意力的传统编码器-解码器框架中，编码器生成一个固定长度的向量，该向量与输入的长度或特征无关，并且在解码过程中保持静态。*'
- en: ''
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*–* [Attention in Psychology, Neuroscience, and Machine Learning](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full),
    2020.'
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*–* [心理学、神经科学与机器学习中的注意力](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full)，2020。'
- en: Representing the input by a fixed-length vector was especially problematic for
    long sequences or sequences that were complex in structure since the dimensionality
    of their representation was forced to be the same as with shorter or simpler sequences.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用固定长度向量表示输入在处理长序列或结构复杂的序列时尤为棘手，因为这些序列的表示维度必须与较短或较简单序列的表示维度相同。
- en: '*For example, in some languages, such as Japanese, the last word might be very
    important to predict the first word, while translating English to French might
    be easier as the order of the sentences (how the sentence is organized) is more
    similar to each other. *'
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*例如，在某些语言中，如日语，最后一个词可能对预测第一个词非常重要，而将英语翻译成法语可能更容易，因为句子的顺序（句子的组织方式）更相似。*'
- en: ''
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*–* [Attention in Psychology, Neuroscience, and Machine Learning](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full),
    2020.'
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*–* [心理学、神经科学和机器学习中的注意力](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full)，2020年。'
- en: This created a bottleneck whereby the decoder has limited access to the information
    provided by the input—that which is available within the fixed-length encoding
    vector. On the other hand, preserving the length of the input sequence during
    the encoding process could make it possible for the decoder to utilize its most
    relevant parts in a flexible manner.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这造成了一个瓶颈，即解码器对由输入提供的信息的访问受到限制，即在固定长度编码向量内可用的信息。另一方面，在编码过程中保持输入序列的长度不变可以使解码器能够灵活地利用其最相关的部分。
- en: The latter is how the attention mechanism operates.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机制是如何运作的。
- en: '*Attention helps determine which of these vectors should be used to generate
    the output. Because the output sequence is dynamically generated one element at
    a time, attention can dynamically highlight different encoded vectors at each
    time point. This allows the decoder to flexibly utilize the most relevant parts
    of the input sequence.*'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*注意帮助确定应该使用这些向量中的哪一个来生成输出。由于输出序列是逐个元素动态生成的，注意力可以在每个时间点动态突出显示不同的编码向量。这使得解码器能够灵活地利用输入序列中最相关的部分。*'
- en: ''
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – Page 186, [Deep Learning Essentials](https://www.amazon.com/Deep-Learning-Essentials-hands-fundamentals/dp/1785880365),
    2018.
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – 第186页，[深度学习基础](https://www.amazon.com/Deep-Learning-Essentials-hands-fundamentals/dp/1785880365)，2018年。
- en: 'One of the earliest works in machine translation that sought to address the
    bottleneck problem created by fixed-length vectors was by [Bahdanau et al. (2014)](https://arxiv.org/abs/1409.0473).
    In their work, Bahdanau et al. employed the use of Recurrent Neural Networks (RNNs)
    for both encoding and decoding tasks: the encoder employs a bi-directional RNN
    to generate a sequence of *annotations* that each contain a summary of both preceding
    and succeeding words that can be mapped into a *context* vector through a weighted
    sum; the decoder then generates an output based on these annotations and the hidden
    states of another RNN. Since the context vector is computed by a weighted sum
    of the annotations, then Bahdanau et al.’s attention mechanism is an example of
    [*soft attention*](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期的机器翻译工作中，试图解决由固定长度向量引起的瓶颈问题的工作之一是由[Bahdanau et al. (2014)](https://arxiv.org/abs/1409.0473)完成的。在他们的工作中，Bahdanau等人使用递归神经网络（RNNs）进行编码和解码任务：编码器采用双向RNN生成一系列*注释*，每个注释包含前后单词的摘要，可以通过加权和映射到*上下文*向量；解码器然后基于这些注释和另一个RNN的隐藏状态生成输出。由于上下文向量是通过注释的加权和计算得到的，因此Bahdanau等人的注意机制是[*软注意*](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/)的一个例子。
- en: Another of the earliest works was by [Sutskever et al. (2014)](https://arxiv.org/abs/1409.3215).
    They, alternatively, made use of multilayered Long Short-Term Memory (LSTM) to
    encode a vector representing the input sequence and another LSTM to decode the
    vector into a target sequence.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项早期工作是由[Sutskever et al. (2014)](https://arxiv.org/abs/1409.3215)完成的。他们选择使用多层长短期记忆（LSTM）来编码表示输入序列的向量，并使用另一个LSTM来将该向量解码为目标序列。
- en: '[Luong et al. (2015)](https://arxiv.org/abs/1508.04025) introduced the idea
    of *global* versus *local* attention. In their work, they described a global attention
    model as one that, when deriving the context vector, considers all the hidden
    states of the encoder. The computation of the global context vector is, therefore,
    based on a weighted average of *all* the words in the source sequence. Luong et
    al. mentioned that this is computationally expensive and could potentially make
    global attention difficult to be applied to long sequences. Local attention is
    proposed to address this problem by focusing on a smaller subset of the words
    in the source sequence per target word. Luong et al. explained that local attention
    trades off the [*soft*](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/)
    and [*hard*](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/)
    attentional models of [Xu et al. (2016)](https://arxiv.org/abs/1502.03044) (we
    will refer to this paper again in the next section) by being less computationally
    expensive than the soft attention but easier to train than the hard attention.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[Luong 等人（2015）](https://arxiv.org/abs/1508.04025) 引入了*全局*与*局部*注意力的概念。在他们的工作中，他们将全局注意力模型描述为在推导上下文向量时考虑编码器的所有隐藏状态。因此，全局上下文向量的计算基于*所有*源序列中的词的加权平均。Luong
    等人提到，这在计算上是昂贵的，并且可能使全局注意力难以应用于长序列。局部注意力被提出以解决这个问题，通过专注于每个目标词的源序列中的较小子集。Luong 等人解释说，局部注意力在计算上比软注意力更便宜，但比硬注意力更易于训练。'
- en: More recently, [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) proposed
    an entirely different architecture that has steered the field of machine translation
    in a new direction. Termed *Transformer*, their architecture dispenses with any
    recurrence and convolutions altogether but implements a *self-attention* mechanism.
    Words in the source sequence are first encoded in parallel to generate key, query,
    and value representations. The keys and queries are combined to generate attention
    weightings that capture how each word relates to the others in the sequence. These
    attention weightings are then used to scale the values, in order to retain focus
    on the important words and drown out the irrelevant ones.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 更近期，[Vaswani 等人（2017）](https://arxiv.org/abs/1706.03762)提出了一种完全不同的架构，已经引导了机器翻译领域的一个新方向。这个被称为*Transformer*的架构完全舍弃了递归和卷积，但实现了*自注意力*机制。源序列中的词首先被并行编码以生成键、查询和值表示。键和值被组合以生成注意力权重，从而捕捉每个词与序列中其他词的关系。这些注意力权重随后用于缩放值，以便保持对重要词的关注，并消除无关的词。
- en: '*The output is computed as a weighted sum of the values, where the weight assigned
    to each value is computed by a compatibility function of the query with the corresponding
    key.*'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*输出是通过对值的加权求和来计算的，其中分配给每个值的权重是通过查询与相应键的兼容性函数计算的。*'
- en: ''
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf), 2017.
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – [《Attention Is All You Need》](https://arxiv.org/pdf/1706.03762.pdf)，2017年。
- en: '[![](../Images/5cc2fa00063bfd70298252dce57dbdcd.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/5cc2fa00063bfd70298252dce57dbdcd.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png)'
- en: The Transformer architecture
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构
- en: Taken from “Attention Is All You Need”
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 摘自《Attention Is All You Need》
- en: At the time, the proposed Transformer architecture established a new state-of-the-art
    process for English-to-German and English-to-French translation tasks. It was
    reportedly also faster to train than architectures based on recurrent or convolutional
    layers. Subsequently, the method called BERT by [Devlin et al. (2019)](https://arxiv.org/abs/1810.04805)
    built on Vaswani et al.’s work by proposing a multi-layer bi-directional architecture.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当时，提出的 Transformer 架构为英语到德语和英语到法语的翻译任务建立了新的最先进的过程。报告称，它的训练速度也比基于递归或卷积层的架构更快。随后，由[Devlin
    等人（2019）](https://arxiv.org/abs/1810.04805)提出的方法 BERT 基于 Vaswani 等人的工作，提出了一个多层双向架构。
- en: As we shall see shortly, the uptake of the Transformer architecture was not
    only rapid in the domain of NLP but also in the computer vision domain.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们很快将看到的那样，Transformer 架构的接受不仅在 NLP 领域迅速增长，而且在计算机视觉领域也迅速扩展。
- en: '**Attention in Computer Vision**'
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**计算机视觉中的注意力**'
- en: In computer vision, attention has found its way into several applications, such
    as in the domains of image classification, image segmentation, and image captioning.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，注意力机制已经在多个应用领域找到了它的位置，例如在图像分类、图像分割和图像描述领域。
- en: For example, if we had to reframe the encoder-decoder model to the task of image
    captioning, then the encoder could be a Convolutional Neural Network (CNN) that
    captured the salient visual cues in the images into a vector representation. And
    the decoder could be an RNN or LSTM that transformed the vector representation
    into an output.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们需要将编码器-解码器模型重新构建用于图像描述任务，那么编码器可以是一个卷积神经网络（CNN），它将图像中的显著视觉线索转化为向量表示。解码器则可以是一个RNN或LSTM，将向量表示转换为输出。
- en: '*Also, as in the neuroscience literature, these attentional processes can be
    divided into spatial and feature-based attention. *'
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*此外，正如神经科学文献中所述，这些注意力过程可以分为空间注意力和基于特征的注意力。*'
- en: ''
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*–* [Attention in Psychology, Neuroscience, and Machine Learning](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full),
    2020.'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*–* [心理学、神经科学和机器学习中的注意力](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full)，2020。'
- en: In *spatial* attention, different spatial locations are attributed different
    weights. However, these same weights are retained across all feature channels
    at the different spatial locations.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在*空间*注意力中，不同的空间位置被赋予不同的权重。然而，这些相同的权重在不同的空间位置的所有特征通道中保持不变。
- en: One of the fundamental image captioning approaches working with spatial attention
    has been proposed by [Xu et al. (2016)](https://arxiv.org/abs/1502.03044). Their
    model incorporates a CNN as an encoder that extracts a set of feature vectors
    (or *annotation* vectors), with each vector corresponding to a different part
    of the image to allow the decoder to focus selectively on specific image parts.
    The decoder is an LSTM that generates a caption based on a context vector, the
    previously hidden state, and the previously generated words. Xu et al. investigate
    the use of [*hard attention*](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/)
    as an alternative to [soft attention](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/)
    in computing their context vector. Here, soft attention places weights *softly*
    on all patches of the source image, whereas hard attention attends to a single
    patch alone while disregarding the rest. They report that, in their work, hard
    attention performs better.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一种基于空间注意力的基本图像描述方法由[Xu et al. (2016)](https://arxiv.org/abs/1502.03044)提出。他们的模型将CNN作为编码器，提取一组特征向量（或*注释*向量），每个向量对应于图像的不同部分，以便解码器能够有选择地关注特定的图像部分。解码器是一个LSTM，根据上下文向量、先前的隐藏状态和先前生成的单词生成描述。Xu
    et al.研究了将[*硬注意力*](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/)作为计算其上下文向量的[软注意力](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/)的替代方法。在这里，软注意力在源图像的所有区域上*柔和地*施加权重，而硬注意力则只关注单个区域，同时忽略其余部分。他们报告说，在他们的工作中，硬注意力表现更好。
- en: '[![](../Images/c12d91e0601ae834180a0c76b377b649.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_2.png)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/c12d91e0601ae834180a0c76b377b649.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_2.png)'
- en: Model for image caption generation
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图像描述生成模型
- en: 'Taken from “Show, Attend and Tell: Neural Image Caption Generation with Visual
    Attention”'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 摘自《展示、注意和讲述：带有视觉注意力的神经图像描述生成》
- en: '*Feature* attention, in comparison, permits individual feature maps to be attributed
    their own weight values. One such example, also applied to image captioning, is
    the encoder-decoder framework of [Chen et al. (2018)](https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_SCA-CNN_Spatial_and_CVPR_2017_paper.pdf),
    which incorporates spatial and channel-wise attentions in the same CNN.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 相较之下，*特征*注意力允许各个特征图赋予自身的权重值。一个这样的例子，亦应用于图像描述，是[Chen et al. (2018)](https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_SCA-CNN_Spatial_and_CVPR_2017_paper.pdf)的编码器-解码器框架，它在同一个CNN中结合了空间和通道注意力。
- en: Similarly to how the Transformer has quickly become the standard architecture
    for NLP tasks, it has also been recently taken up and adapted by the computer
    vision community.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 与Transformer迅速成为NLP任务的标准架构类似，它最近也被计算机视觉领域采纳并加以改编。
- en: The earliest work to do so was proposed by [Dosovitskiy et al. (2020)](https://arxiv.org/abs/2010.11929),
    who applied their *Vision Transformer* (ViT) to an image classification task.
    They argued that the long-standing reliance on CNNs for image classification was
    not necessary, and the same task could be accomplished by a pure transformer.
    Dosovitskiy et al. reshape an input image into a sequence of flattened 2D image
    patches, which they subsequently embed by a trainable linear projection to generate
    the *patch embeddings*. These patch embeddings, together with their *position
    embeddings*, to retain positional information, are fed into the encoder part of
    the Transformer architecture, whose output is subsequently fed into a Multilayer
    Perceptron (MLP) for classification.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最早这样做的工作是由 [Dosovitskiy 等人 (2020)](https://arxiv.org/abs/2010.11929) 提出的，他们将
    *Vision Transformer* (ViT) 应用于图像分类任务。他们认为，长期以来对 CNN 的依赖并不是必要的，纯变换器也可以完成同样的任务。Dosovitskiy
    等人将输入图像重塑为一系列展平的2D图像补丁，然后通过可训练的线性投影将其嵌入，以生成 *补丁嵌入*。这些补丁嵌入与 *位置嵌入* 一起，以保留位置信息，被输入到变换器架构的编码器部分，编码器的输出随后被输入到多层感知机
    (MLP) 进行分类。
- en: '[![](../Images/d7e4076797bdf6cc13b250e5a7312329.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_3.png)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/d7e4076797bdf6cc13b250e5a7312329.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_3.png)'
- en: The Vision Transformer architecture
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Vision Transformer 架构
- en: 'Taken from “An Image is Worth 16×16 Words: Transformers for Image Recognition
    at Scale”'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '摘自《An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale》'
- en: '*Inspired by ViT, and the fact that attention-based architectures are an intuitive
    choice for modelling long-range contextual relationships in video, we develop
    several transformer-based models for video classification.*'
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*受 ViT 启发，并且基于注意力的架构在建模视频中的长程上下文关系时直观有效，我们开发了几个基于变换器的视频分类模型。*'
- en: ''
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '– [ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691), 2021.'
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '– [ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691)，2021年。'
- en: '[Arnab et al. (2021)](https://arxiv.org/abs/2103.15691) subsequently extended
    the ViT model to ViViT, which exploits the spatiotemporal information contained
    within videos for the task of video classification. Their method explores different
    approaches of extracting the spatiotemporal data, such as by sampling and embedding
    each frame independently or by extracting non-overlapping tubelets (an image patch
    that spans across several image frames, creating a *tube*) and embedding each
    one in turn. They also investigate different methods of factorizing the spatial
    and temporal dimensions of the input video for increased efficiency and scalability.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[Arnab 等人 (2021)](https://arxiv.org/abs/2103.15691) 随后将 ViT 模型扩展为 ViViT，该模型利用视频中的时空信息进行视频分类任务。他们的方法探索了提取时空数据的不同方法，例如通过独立采样和嵌入每一帧，或提取不重叠的管段（一个跨越多个图像帧的图像补丁，形成一个
    *管道*）并逐一嵌入。他们还研究了对输入视频的空间和时间维度进行分解的不同方法，以提高效率和可扩展性。'
- en: '[![](../Images/3dbe074038add2c8e6771d951e9c9505.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_4.png)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/3dbe074038add2c8e6771d951e9c9505.png)](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_4.png)'
- en: The Video Vision Transformer architecture
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 视频视觉变换器架构
- en: 'Taken from “ViViT: A Video Vision Transformer”'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '摘自《ViViT: A Video Vision Transformer》'
- en: Correctness · Re
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正确性 · Re
- en: In its first application for image classification, the Vision Transformer is
    already being applied to several other computer vision domains, such as [action
    localization](https://arxiv.org/abs/2106.08061), [gaze estimation](https://arxiv.org/abs/2105.14424),
    and [image generation](https://arxiv.org/abs/2107.04589). This surge of interest
    among computer vision practitioners suggests an exciting near future, where we’ll
    be seeing more adaptations and applications of the Transformer architecture.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在首次应用于图像分类的情况下，Vision Transformer 已经被应用于多个其他计算机视觉领域，如 [动作定位](https://arxiv.org/abs/2106.08061)、[注视估计](https://arxiv.org/abs/2105.14424)
    和 [图像生成](https://arxiv.org/abs/2107.04589)。这种计算机视觉从业者的兴趣激增，预示着一个激动人心的近未来，我们将看到更多对变换器架构的适应和应用。
- en: Want to Get Started With Building Transformer Models with Attention?
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 想开始构建带有注意力机制的变换器模型吗？
- en: Take my free 12-day email crash course now (with sample code).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在就参加我的免费12天电子邮件速成课程（包含示例代码）。
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 点击注册，并获取课程的免费PDF电子书版本。
- en: '**Further Reading**'
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: This section provides more resources on the topic if you are looking to go deeper.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了更多关于该主题的资源，如果你想深入了解。
- en: '**Books**'
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**书籍**'
- en: '[Deep Learning Essentials](https://www.amazon.com/Deep-Learning-Essentials-hands-fundamentals/dp/1785880365),
    2018.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习要点](https://www.amazon.com/Deep-Learning-Essentials-hands-fundamentals/dp/1785880365)，2018年。'
- en: '**Papers**'
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**论文**'
- en: '[Attention in Psychology, Neuroscience, and Machine Learning](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full),
    2020.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[心理学、神经科学和机器学习中的注意力](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full)，2020年。'
- en: '[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473),
    2014.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过联合学习对齐和翻译的神经机器翻译](https://arxiv.org/abs/1409.0473)，2014年。'
- en: '[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215),
    2014.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[序列到序列学习与神经网络](https://arxiv.org/abs/1409.3215)，2014年。'
- en: '[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025),
    2015.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基于注意力的神经机器翻译的有效方法](https://arxiv.org/abs/1508.04025)，2015年。'
- en: '[Attention Is All You Need](https://arxiv.org/abs/1706.03762), 2017.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[注意力机制是你所需的一切](https://arxiv.org/abs/1706.03762)，2017年。'
- en: '[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805),
    2019.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BERT：深度双向变换器的预训练用于语言理解](https://arxiv.org/abs/1810.04805)，2019年。'
- en: '[Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044),
    2016.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[展示、关注和讲述：使用视觉注意力的神经图像描述生成](https://arxiv.org/abs/1502.03044)，2016年。'
- en: '[SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks for
    Image Captioning](https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_SCA-CNN_Spatial_and_CVPR_2017_paper.pdf),
    2018.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SCA-CNN：用于图像描述的卷积网络中的空间和通道注意力](https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_SCA-CNN_Spatial_and_CVPR_2017_paper.pdf)，2018年。'
- en: '[An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929),
    2020.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一张图像值16×16个词：用于大规模图像识别的变换器](https://arxiv.org/abs/2010.11929)，2020年。'
- en: '[ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691), 2021.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ViViT：视频视觉变换器](https://arxiv.org/abs/2103.15691)，2021年。'
- en: '**Example Applications:**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例应用：**'
- en: '[Relation Modeling in Spatio-Temporal Action Localization](https://arxiv.org/abs/2106.08061),
    2021.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[时空动作定位中的关系建模](https://arxiv.org/abs/2106.08061)，2021年。'
- en: '[Gaze Estimation using Transformer](https://arxiv.org/abs/2105.14424), 2021.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用变换器的注视估计](https://arxiv.org/abs/2105.14424)，2021年。'
- en: '[ViTGAN: Training GANs with Vision Transformers](https://arxiv.org/abs/2107.04589),
    2021.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ViTGAN：使用视觉变换器训练GANs](https://arxiv.org/abs/2107.04589)，2021年。'
- en: '**Summary**'
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this tutorial, you discovered an overview of the research advances on attention.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你了解了关于注意力的研究进展概述。
- en: 'Specifically, you learned:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，你学到了：
- en: The concept of attention that is of significance to different scientific disciplines
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力的概念对不同科学学科的重要性
- en: How attention is revolutionizing machine learning, specifically in the domains
    of natural language processing and computer vision
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力如何在机器学习中引发革命，特别是在自然语言处理和计算机视觉领域
- en: Do you have any questions?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你有什么问题吗？
- en: Ask your questions in the comments below, and I will do my best to answer.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 请在下方评论中提出你的问题，我会尽力回答。
