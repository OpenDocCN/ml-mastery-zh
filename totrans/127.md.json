["```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Read data\ndata = fetch_california_housing()\nX, y = data.data, data.target\n\n# train-test split for model evaluation\nX_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n\n# Standardizing data\nscaler = StandardScaler()\nscaler.fit(X_train_raw)\nX_train = scaler.transform(X_train_raw)\nX_test = scaler.transform(X_test_raw)\n\n# Convert to 2D PyTorch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n\n# Define the model\nmodel = nn.Sequential(\n    nn.Linear(8, 24),\n    nn.ReLU(),\n    nn.Linear(24, 12),\n    nn.ReLU(),\n    nn.Linear(12, 6),\n    nn.ReLU(),\n    nn.Linear(6, 1)\n)\n\n# loss function and optimizer\nloss_fn = nn.MSELoss()  # mean square error\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nn_epochs = 100   # number of epochs to run\nbatch_size = 32  # size of each batch\nbatch_start = torch.arange(0, len(X_train), batch_size)\n\nfor epoch in range(n_epochs):\n    for start in batch_start:\n        # take a batch\n        X_batch = X_train[start:start+batch_size]\n        y_batch = y_train[start:start+batch_size]\n        # forward pass\n        y_pred = model(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        # update weights\n        optimizer.step()\n```", "```py\nmse_history = []\n\nfor epoch in range(n_epochs):\n    for start in batch_start:\n        # take a batch\n        X_batch = X_train[start:start+batch_size]\n        y_batch = y_train[start:start+batch_size]\n        # forward pass\n        y_pred = model(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        mse_history.append(float(loss))\n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        # update weights\n        optimizer.step()\n```", "```py\nmae_fn = nn.L1Loss()  # create a function to compute MAE\ntrain_mse_history = []\ntest_mse_history = []\ntest_mae_history = []\n\nfor epoch in range(n_epochs):\n    model.train()\n    for start in batch_start:\n        # take a batch\n        X_batch = X_train[start:start+batch_size]\n        y_batch = y_train[start:start+batch_size]\n        # forward pass\n        y_pred = model(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        train_mse_history.append(float(loss))\n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        # update weights\n        optimizer.step()\n    # validate model on test set\n    model.eval()\n    with torch.no_grad():\n        y_pred = model(X_test)\n        mse = loss_fn(y_pred, y_test)\n        mae = mae_fn(y_pred, y_test)\n        test_mse_history.append(float(mse))\n        test_mae_history.append(float(mae))\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Read data\ndata = fetch_california_housing()\nX, y = data.data, data.target\n\n# train-test split for model evaluation\nX_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n\n# Standardizing data\nscaler = StandardScaler()\nscaler.fit(X_train_raw)\nX_train = scaler.transform(X_train_raw)\nX_test = scaler.transform(X_test_raw)\n\n# Convert to 2D PyTorch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n\n# Define the model\nmodel = nn.Sequential(\n    nn.Linear(8, 24),\n    nn.ReLU(),\n    nn.Linear(24, 12),\n    nn.ReLU(),\n    nn.Linear(12, 6),\n    nn.ReLU(),\n    nn.Linear(6, 1)\n)\n\n# loss function, metrics, and optimizer\nloss_fn = nn.MSELoss()  # mean square error\nmae_fn = nn.L1Loss()  # mean absolute error\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nn_epochs = 100   # number of epochs to run\nbatch_size = 32  # size of each batch\nbatch_start = torch.arange(0, len(X_train), batch_size)\n\ntrain_mse_history = []\ntest_mse_history = []\ntest_mae_history = []\n\nfor epoch in range(n_epochs):\n    model.train()\n    epoch_mse = []\n    for start in batch_start:\n        # take a batch\n        X_batch = X_train[start:start+batch_size]\n        y_batch = y_train[start:start+batch_size]\n        # forward pass\n        y_pred = model(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        epoch_mse.append(float(loss))\n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        # update weights\n        optimizer.step()\n    mean_mse = sum(epoch_mse) / len(epoch_mse)\n    train_mse_history.append(mean_mse)\n    # validate model on test set\n    model.eval()\n    with torch.no_grad():\n        y_pred = model(X_test)\n        mse = loss_fn(y_pred, y_test)\n        mae = mae_fn(y_pred, y_test)\n        test_mse_history.append(float(mse))\n        test_mae_history.append(float(mae))\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.plot(np.sqrt(train_mse_history), label=\"Train RMSE\")\nplt.plot(np.sqrt(test_mse_history), label=\"Test RMSE\")\nplt.plot(test_mae_history, label=\"Test MAE\")\nplt.xlabel(\"epochs\")\nplt.legend()\nplt.show()\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Read data\ndata = fetch_california_housing()\nX, y = data.data, data.target\n\n# train-test split for model evaluation\nX_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n\n# Standardizing data\nscaler = StandardScaler()\nscaler.fit(X_train_raw)\nX_train = scaler.transform(X_train_raw)\nX_test = scaler.transform(X_test_raw)\n\n# Convert to 2D PyTorch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n\n# Define the model\nmodel = nn.Sequential(\n    nn.Linear(8, 24),\n    nn.ReLU(),\n    nn.Linear(24, 12),\n    nn.ReLU(),\n    nn.Linear(12, 6),\n    nn.ReLU(),\n    nn.Linear(6, 1)\n)\n\n# loss function, metrics, and optimizer\nloss_fn = nn.MSELoss()  # mean square error\nmae_fn = nn.L1Loss()  # mean absolute error\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nn_epochs = 100   # number of epochs to run\nbatch_size = 32  # size of each batch\nbatch_start = torch.arange(0, len(X_train), batch_size)\n\ntrain_mse_history = []\ntest_mse_history = []\ntest_mae_history = []\n\nfor epoch in range(n_epochs):\n    model.train()\n    epoch_mse = []\n    for start in batch_start:\n        # take a batch\n        X_batch = X_train[start:start+batch_size]\n        y_batch = y_train[start:start+batch_size]\n        # forward pass\n        y_pred = model(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        epoch_mse.append(float(loss))\n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        # update weights\n        optimizer.step()\n    mean_mse = sum(epoch_mse) / len(epoch_mse)\n    train_mse_history.append(mean_mse)\n    # validate model on test set\n    model.eval()\n    with torch.no_grad():\n        y_pred = model(X_test)\n        mse = loss_fn(y_pred, y_test)\n        mae = mae_fn(y_pred, y_test)\n        test_mse_history.append(float(mse))\n        test_mae_history.append(float(mae))\n\nplt.plot(np.sqrt(train_mse_history), label=\"Train RMSE\")\nplt.plot(np.sqrt(test_mse_history), label=\"Test RMSE\")\nplt.plot(test_mae_history, label=\"Test MAE\")\nplt.xlabel(\"epochs\")\nplt.legend()\nplt.show()\n```"]