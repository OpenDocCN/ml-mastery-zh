- en: Neural Network with More Hidden Neurons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://machinelearningmastery.com/neural-network-with-more-hidden-neurons/](https://machinelearningmastery.com/neural-network-with-more-hidden-neurons/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The traditional model of neural network is called multilayer perceptrons. They
    are usually made up of a series of interconnected layers. The input layer is where
    the data enters the network, and the output layer is where the network delivers
    the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input layer is usually connected to one or more hidden layers, which modify
    and process the data before it reaches the output layer. The hidden layers are
    what make neural networks so powerful: they can learn complicated functions that
    might be difficult for a programmer to specify in code.'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous tutorial, we built a neural network with only a couple of hidden
    neurons. Here, you will implement a neural network by adding more hidden neurons
    to it. This will estimate more complex function for us in order to fit the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'During the implementation process, you’ll learn:'
  prefs: []
  type: TYPE_NORMAL
- en: How to build a neural network with more hidden neurons in PyTorch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to estimate complex functions using neural networks by adding more hidden
    neurons to the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to train a neural network in PyTorch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kick-start your project** with my book [Deep Learning with PyTorch](https://machinelearningmastery.com/deep-learning-with-pytorch/).
    It provides **self-study tutorials** with **working code**.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.![](../Images/0a0df0be009ba84b451a0c0758afb706.png)
  prefs: []
  type: TYPE_NORMAL
- en: Neural Network with More Hidden Neurons.
  prefs: []
  type: TYPE_NORMAL
- en: Picture by [Kdwk Leung](https://unsplash.com/photos/Lu2NgRt7p_g). Some rights
    reserved.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This tutorial is in three parts; they are
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the Data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build the Model Architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the Model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s build a `Data` class that extends the `Dataset` class from PyTorch. You
    use it to create a dataset of 100 synthetic values ranging from $-50$ to $50$.
    The `x` tensor stores the values in the specified range, while the `y` tensor
    is a corresponding tensor of zeros with the same shape as `x`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you use a for loop to set the values in `x` and `y` tensors based on the
    values in `x`. If a value in `x` is between $-20$ and $20$, the corresponding
    value in `y` is set to 1 and if a value in `x` is between $-30$ and $-20$ or between
    $20$ and $30$, the corresponding value in `y` is set to 0\. Similarly, If a value
    in `x` is between $-40$ and $-30$ or between $30$ and $40$, the corresponding
    value in `y` is set to 1\. Otherwise, the corresponding value in `y` is set to
    0.
  prefs: []
  type: TYPE_NORMAL
- en: In the `Data` class, the `__getitem__()` method has been used to retrieve the
    `x` and `y` values at a specified index in the dataset. The `__len__()` method
    returns the length of the dataset. With these, you can obtain a sample from the
    dataset using `data[i]` and tell the size of the dataset using `len(data)`. This
    class can be used to create a data object that can be passed to a PyTorch data
    loader to train a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Note that you are building this complex data object to see how well our neural
    network with more hidden neurons estimates the function. Here is how the code
    of the data object will look like.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let’s instantiate a data object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: And, write a function to visualize this data, which will also be useful when
    you train the model later.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this function, you can see the data looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a6ae432bfb8664e8e340d87a4e8bf3a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Want to Get Started With Deep Learning with PyTorch?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Take my free email crash course now (with sample code).
  prefs: []
  type: TYPE_NORMAL
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  prefs: []
  type: TYPE_NORMAL
- en: Build the Model Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Below, you will define a `NeuralNetwork` class to build a custom model architecture
    using `nn.Module` from PyTorch. This class represents a simple neural network
    with an input layer, a hidden layer, and an output layer.
  prefs: []
  type: TYPE_NORMAL
- en: The `__init__()` method is used to initialize the neural network by defining
    the layers in the network. The forward method is used to define the forward pass
    through the network. In this case, a sigmoid activation function is applied to
    the output of both input and output layers. This means that the output of the
    network will be a value between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you will create an instance of the `NeuralNetwork` class and store
    it in the `model` variable. The model is initialized with an input layer having
    1 input neuron, a hidden layer having 15 hidden neurons, and an output layer having
    1 output neuron. This model is now ready to be trained on some data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Train the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s define the criterion, optimizer, and dataloader. You should use binary
    cross entropy loss as the dataset is a classification with two classes. Adam optimizer
    is used, with a batch size of 32\. The learning rate is set to 0.01 which determines
    how model weights are updated during training. The loss function is used to evaluate
    the model performance, the optimizer updates the weights, and the data loader
    divides the data into batches for efficient processing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s build a training loop for 7000 epochs and visualize the results during
    training. You’ll see how well our model estimates the data points as the training
    progresses.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: When you run this loop, you will see at the first epoch, the neural network
    modelled the dataset poorly, like the following:![](../Images/a675eee83978de4a004b788d04a9e0c3.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'But the accuracy improved as the training progressed. After the training loop
    completed, we can see the result as the neural network modelled the data like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0cf77d920f4e0504e79a1264c0a2cd4e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and the corresponding history of loss metric can be plot like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/530f84d67b9270c5ec76573285bdbfba.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, our model estimated the function quite well but not perfect.
    The input of range 20 to 40, for example, isn’t predicted right. You may try to
    expand the network to add one more layer, such as the following, and see if it
    will make any difference.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Putting everything together, the following is the complete code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this tutorial, you learned how we estimate complex functions by introducing
    more neurons into the neural networks. Particularly, you learned:'
  prefs: []
  type: TYPE_NORMAL
- en: How to build a neural network with more hidden neurons in PyTorch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to estimate complex functions using neural networks by adding more hidden
    neurons to the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to train a neural network in PyTorch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
