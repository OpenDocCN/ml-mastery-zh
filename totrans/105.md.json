["```py\nimport datetime\n\ntimestamp = datetime.datetime.now()  # Get the current date and time\nx = 0    # initialize x to zero\n```", "```py\nimport scipy.stats\n\nz_alpha = scipy.stats.norm.ppf(0.975)  # Call the inverse CDF of standard normal\n```", "```py\ndef adadelta(objective, derivative, bounds, n_iter, rho, ep=1e-3):\n    # generate an initial point\n    solution = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n    # lists to hold the average square gradients for each variable and\n    # average parameter updates\n    sq_grad_avg = [0.0 for _ in range(bounds.shape[0])]\n    sq_para_avg = [0.0 for _ in range(bounds.shape[0])]\n    # run the gradient descent\n    for it in range(n_iter):\n        gradient = derivative(solution[0], solution[1])\n        # update the moving average of the squared partial derivatives\n        for i in range(gradient.shape[0]):\n            sg = gradient[i]**2.0\n            sq_grad_avg[i] = (sq_grad_avg[i] * rho) + (sg * (1.0-rho))\n        # build a solution one variable at a time\n        new_solution = list()\n        for i in range(solution.shape[0]):\n            # calculate the step size for this variable\n            alpha = (ep + sqrt(sq_para_avg[i])) / (ep + sqrt(sq_grad_avg[i]))\n            # calculate the change and update the moving average of the squared change\n            change = alpha * gradient[i]\n            sq_para_avg[i] = (sq_para_avg[i] * rho) + (change**2.0 * (1.0-rho))\n            # calculate the new position in this variable and store as new solution\n            value = solution[i] - change\n            new_solution.append(value)\n        # evaluate candidate point\n        solution = asarray(new_solution)\n        solution_eval = objective(solution[0], solution[1])\n        # report progress\n        print('>%d f(%s) = %.5f' % (it, solution, solution_eval))\n    return [solution, solution_eval]\n```", "```py\n# TODO replace Keras code below with Tensorflow\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\n\nmodel = Sequential()\nmodel.add(Conv2D(1, (3,3), strides=(2, 2), input_shape=(8, 8, 1)))\nmodel.summary()\n...\n```", "```py\nTcpSocketBase::~TcpSocketBase (void)\n{\n  NS_LOG_FUNCTION (this);\n  m_node = nullptr;\n  if (m_endPoint != nullptr)\n    {\n      NS_ASSERT (m_tcp != nullptr);\n      /*\n       * Upon Bind, an Ipv4Endpoint is allocated and set to m_endPoint, and\n       * DestroyCallback is set to TcpSocketBase::Destroy. If we called\n       * m_tcp->DeAllocate, it will destroy its Ipv4EndpointDemux::DeAllocate,\n       * which in turn destroys my m_endPoint, and in turn invokes\n       * TcpSocketBase::Destroy to nullify m_node, m_endPoint, and m_tcp.\n       */\n      NS_ASSERT (m_endPoint != nullptr);\n      m_tcp->DeAllocate (m_endPoint);\n      NS_ASSERT (m_endPoint == nullptr);\n    }\n  if (m_endPoint6 != nullptr)\n    {\n      NS_ASSERT (m_tcp != nullptr);\n      NS_ASSERT (m_endPoint6 != nullptr);\n      m_tcp->DeAllocate (m_endPoint6);\n      NS_ASSERT (m_endPoint6 == nullptr);\n    }\n  m_tcp = 0;\n  CancelAllTimers ();\n}\n```", "```py\nasync def main(indir):\n    # Scan dirs for files and populate a list\n    filepaths = []\n    for path, dirs, files in os.walk(indir):\n        for basename in files:\n            filepath = os.path.join(path, basename)\n            filepaths.append(filepath)\n\n    \"\"\"Create the \"process pool\" of 4 and run asyncio.\n    The processes will execute the worker function\n    concurrently with each file path as parameter\n    \"\"\"\n    loop = asyncio.get_running_loop()\n    with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n        futures = [loop.run_in_executor(executor, func, f) for f in filepaths]\n        for fut in asyncio.as_completed(futures):\n            try:\n                filepath = await fut\n                print(filepath)\n            except Exception as exc:\n                print(\"failed one job\")\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\n\"\"\"\nX, y = make_classification(n_samples=5000, n_features=2, n_informative=2,\n                           n_redundant=0, n_repeated=0, n_classes=2,\n                           n_clusters_per_class=1,\n                           weights=[0.01, 0.05, 0.94],\n                           class_sep=0.8, random_state=0)\n\"\"\"\nimport pickle\nwith open(\"dataset.pickle\", \"wb\") as fp:\n    X, y = pickle.load(fp)\n\nclf = LogisticRegression(random_state=0).fit(X, y)\n...\n```", "```py\ndef square(x):\n    \"\"\"Just to compute the square of a value\n\n    Args:\n        x (int or float): A numerical value\n\n    Returns:\n        int or float: The square of x\n    \"\"\"\n    return x * x\n```", "```py\nprint(\"Function name:\", square.__name__)\nprint(\"Docstring:\", square.__doc__)\n```", "```py\nFunction name: square\nDocstring: Just to compute the square of a value\n\n    Args:\n        x (int or float): A numerical value\n\n    Returns:\n        int or float: The square of x\n```", "```py\ndef square(x):\n    \"\"\"Just to compupte the square of a value\n\n    Parameters\n    ----------\n    x : int or float\n        A numerical value\n\n    Returns\n    -------\n    int or float\n        The square of `x`\n    \"\"\"\n    return x * x\n```", "```py\ndef square(x: int) -> int:\n    return x * x\n```", "```py\ndef square(x: int) -> int:\n    value: int = x * x\n    return value\n```", "```py\nfrom typing import Any, Union, List\n\ndef square(x: Union[int, float]) -> Union[int, float]:\n    return x * x\n\ndef append(x: List[Any], y: Any) -> None:\n    x.append(y)\n```", "```py\nn: int = 3.5\nn = \"assign a string\"\n```", "```py\ntest.py:1: error: Incompatible types in assignment (expression has type \"float\", variable has type \"int\")\ntest.py:2: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\nFound 2 errors in 1 file (checked 1 source file)\n```", "```py\nfrom typing import List, Tuple, Generator\nimport pandas as pd\nimport numpy as np\n\nTrainingSampleGenerator = Generator[Tuple[np.ndarray,np.ndarray], None, None]\n\ndef lstm_gen(data: pd.DataFrame,\n             timesteps: int,\n             batch_size: int) -> TrainingSampleGenerator:\n    \"\"\"Generator to produce random samples for LSTM training\n\n    Args:\n        data: DataFrame of data with datetime index in chronological order,\n              samples are drawn from this\n        timesteps: Number of time steps for each sample, data will be\n                   produced from a window of such length\n        batch_size: Number of samples in each batch\n\n    Yields:\n        ndarray, ndarray: The (X,Y) training samples drawn on a random window\n        from the input data\n    \"\"\"\n    input_columns = [c for c in data.columns if c != \"target\"]\n    batch: List[Tuple[pd.DataFrame, pd.Series]] = []\n    while True:\n        # pick one start time and security\n        while True:\n            # Start from a random point from the data and clip a window\n            row = data[\"target\"].sample()\n            starttime = row.index[0]\n            window: pd.DataFrame = data[starttime:].iloc[:timesteps]\n            # If we are at the end of the DataFrame, we can't get a full\n            # window and we must start over\n            if len(window) == timesteps:\n                break\n        # Extract the input and output\n        y = window[\"target\"]\n        X = window[input_columns]\n        batch.append((X, y))\n        # If accumulated enough for one batch, dispatch\n        if len(batch) == batch_size:\n            X, y = zip(*batch)\n            yield np.array(X).astype(\"float32\"), np.array(y).astype(\"float32\")\n            batch = []\n```"]