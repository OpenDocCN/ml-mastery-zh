- en: Understand Model Behavior During Training by Visualizing Metrics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过可视化指标了解训练期间的模型行为
- en: 原文：[https://machinelearningmastery.com/understand-model-behavior-during-training-by-visualizing-metrics/](https://machinelearningmastery.com/understand-model-behavior-during-training-by-visualizing-metrics/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/understand-model-behavior-during-training-by-visualizing-metrics/](https://machinelearningmastery.com/understand-model-behavior-during-training-by-visualizing-metrics/)
- en: 'You can learn a lot about neural networks and deep learning models by observing
    their performance over time during training. For example, if you see the training
    accuracy went worse with training epochs, you know you have issue with the optimization.
    Probably your learning rate is too fast. In this post, you will discover how you
    can review and visualize the performance of PyTorch models over time during training.
    After completing this post, you will know:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察神经网络和深度学习模型在训练期间的性能变化，您可以学到很多。例如，如果您发现训练精度随着训练轮数变差，您就知道优化存在问题。可能是学习率过快。在本文中，您将了解如何在训练过程中查看和可视化PyTorch模型的性能。完成本文后，您将了解：
- en: What metrics to collect during training
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练期间收集哪些指标
- en: How to plot the metrics on training and validation datasets from training
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何绘制训练和验证数据集上的指标
- en: How to interpret the plot to tell about the model and training progress
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何解释图表以了解模型和训练进展
- en: '**Kick-start your project** with my book [Deep Learning with PyTorch](https://machinelearningmastery.com/deep-learning-with-pytorch/).
    It provides **self-study tutorials** with **working code**.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**启动您的项目**，使用我的书籍[《使用PyTorch进行深度学习》](https://machinelearningmastery.com/deep-learning-with-pytorch/)。它提供了**自学教程**和**可工作代码**。'
- en: Let’s get started.![](../Images/b2859a3ed041bcffaae754c3450932ce.png)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！[](../Images/b2859a3ed041bcffaae754c3450932ce.png)
- en: Understand Model Behavior During Training by Visualizing Metrics
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通过可视化指标了解训练期间的模型行为
- en: Photo by [Alison Pang](https://unsplash.com/photos/bnEgE5Aigns). Some rights
    reserved.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Alison Pang](https://unsplash.com/photos/bnEgE5Aigns)提供。部分权利保留。
- en: Overview
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'This chapter is in two parts; they are:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章分为两部分；它们是：
- en: Collecting Metrics from a Training Loop
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从训练循环中收集指标
- en: Plotting the Training History
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制训练历史
- en: Collecting Metrics from a Training Loop
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从训练循环中收集指标
- en: In deep learning, training a model with gradient descent algorithm means to
    take a forward pass to infer loss metric from the input using the model and a
    loss function, then a backward pass to compute the gradient from the loss metric,
    and a update process to apply the gradient to update the model parameters. While
    these are the basic steps you must take, you can do a bit more along the process
    to collect additional information.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，使用梯度下降算法训练模型意味着进行前向传递，使用模型和损失函数推断输入的损失指标，然后进行反向传递以计算从损失指标得出的梯度，并且更新过程应用梯度以更新模型参数。虽然这些是你必须采取的基本步骤，但你可以在整个过程中做更多事情来收集额外的信息。
- en: A model that trained correctly should expect the loss metric to decrease, as
    the loss is the objective to optimize. The loss metric to use should depends on
    the problem.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正确训练的模型应该期望损失指标减少，因为损失是要优化的目标。应该根据问题使用的损失指标来决定。
- en: For regression problems, the closer the model’s prediction to the actual value
    the better. Therefore you want to keep track on the mean square error (MSE), or
    sometimes root mean square error (RMSE), mean absolute error (MAE), or mean absolute
    percentage error (MAPE). Although not used as a loss metric, you may also interested
    in the maximum error produced by your model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，模型预测与实际值越接近越好。因此，您希望跟踪均方误差（MSE）、有时是均方根误差（RMSE）、平均绝对误差（MAE）或平均绝对百分比误差（MAPE）。虽然这些不被用作损失指标，但您可能还对模型产生的最大误差感兴趣。
- en: For classification problems, usually the loss metric is cross entropy. But the
    value of cross entropy is not very intuitive. Therefore you may also want to keep
    track on the accuracy of prediction, true positive rate, precision, recall, F1
    scores, and so on.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，通常损失指标是交叉熵。但是交叉熵的值并不直观。因此，您可能还希望跟踪预测准确率、真正例率、精确度、召回率、F1分数等。
- en: 'Collecting these metrics from a training loop is trivial. Let’s start with
    a basic regression example of deep learning using PyTorch with the California
    housing dataset:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练循环中收集这些指标是微不足道的。让我们从使用PyTorch和加利福尼亚房屋数据集的基本回归示例开始：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This implementation is primitive, but you obtained `loss` as a tensor in each
    step in the process which provides hints to the optimizer to improve the model.
    To know about the progress of the training, you can, of course, print this loss
    metric at every step. But you can also save this value so you can visualize it
    later. When you do that, beware that you do not want to save a tensor but simply
    its value. It is because the PyTorch tensor here remembers how it comes with its
    value so automatic differentiation can be done. These additional data are occupying
    memory but you do not need them.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现方法虽然原始，但在过程中你得到了每一步的`loss`作为张量，这为优化器提供了改进模型的提示。要了解训练的进展，当然可以在每一步打印这个损失度量。但你也可以保存这个值，这样稍后可以进行可视化。在这样做时，请注意不要保存张量，而只保存它的值。这是因为这里的PyTorch张量记得它是如何得到它的值的，所以可以进行自动微分。这些额外的数据占用了内存，但你并不需要它们。
- en: 'Hence you can modify the training loop to the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以修改训练循环如下：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In training a model, you should evaluate it with a test set which is segregated
    from the training set. Usually it is done once in an epoch, after all the training
    steps in that epoch. The test result can also be saved for visualization later.
    In fact, you can obtain multiple metrics from the test set if you want to. Hence
    you can add to the training loop as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，你应该使用与训练集分离的测试集来评估它。通常在一个时期内进行一次，即在该时期的所有训练步骤之后。测试结果也可以保存以便稍后进行可视化。事实上，如果需要，你可以从测试集获得多个指标。因此，你可以添加到训练循环中如下：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can define your own function to compute the metrics or use one that already
    implemented from PyTorch library. It is a good practice to switch the model to
    evaluation mode on evaluation. It is also good practice to run the evaluation
    under the `no_grad()` context, in which you explicitly tell PyTorch that you have
    no intention to run automatic differentiation on the tensors.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以定义自己的函数来计算指标，或者使用已经在PyTorch库中实现的函数。在评估时将模型切换到评估模式是一个好习惯。在`no_grad()`上下文中运行评估也是一个好习惯，这样你明确告诉PyTorch你没有打算在张量上运行自动微分。
- en: 'However, there is a problem in the code above: The MSE from training set is
    computed once per training step based on one batch while the metrics from the
    test set are computed once per epoch and based on the entire test set. They are
    not directly comparable. In fact, if you look a the MSE from training steps, you
    will find it **very noisy**. The better way is to summarize the MSE from the same
    epoch to one number (e.g., their mean) so you can compare to the test set’s data.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上述代码存在问题：训练集的MSE是基于一个批次计算一次训练步骤，而测试集的指标是基于整个测试集每个时期计算一次。它们不是直接可比较的。事实上，如果你查看训练步骤的MSE，你会发现它**非常嘈杂**。更好的方法是将同一时期的MSE总结为一个数字（例如，它们的平均值），这样你可以与测试集的数据进行比较。
- en: 'Making this change, following is the complete code:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 进行这些更改后，以下是完整的代码：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Want to Get Started With Deep Learning with PyTorch?
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 想要开始使用PyTorch进行深度学习吗？
- en: Take my free email crash course now (with sample code).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在就参加我的免费电子邮件速成课程（附带示例代码）。
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 点击注册并获取课程的免费PDF电子书版本。
- en: Plotting the Training History
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制训练历史
- en: 'In the code above, you collected metrics in a Python list, one each per epoch.
    Therefore, it is trivial to plot them into a line graph using matplotlib. Below
    is an example:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，你在Python列表中收集了每个时期的指标。因此，使用matplotlib将它们绘制成折线图是很简单的。下面是一个示例：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: It plots, for example, the following:![](../Images/9f6b4d8f55086d40c8f9d874b05c7c7c.png)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 它绘制了例如以下内容：![](../Images/9f6b4d8f55086d40c8f9d874b05c7c7c.png)
- en: 'Plots like this can provide an indication of useful things about the training
    of the model, such as:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的图表可以提供关于模型训练的有用信息，例如：
- en: Its speed of convergence over epochs (slope)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在时期间的收敛速度（斜率）
- en: Whether the model may have already converged (plateau of the line)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是否已经收敛（线的平台期）
- en: Whether the model may be over-learning the training data (inflection for validation
    line)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是否在过度学习训练数据（验证线的拐点）
- en: In a regression example like the above, the metrics MAE and MSE should both
    decrease if the model gets better. In a classification example, however, accuracy
    metric should increase while the cross entropy loss should decrease as more training
    has been done. This is what you are expected to see from the plot.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在如上回归示例中，如果模型变得更好，MAE 和 MSE 指标应该都下降。然而，在分类示例中，准确率指标应该增加，而交叉熵损失应该随着更多训练的进行而减少。这是你在图中期望看到的结果。
- en: These curves should eventually flatten, meaning you cannot improve the model
    any further based on the current dataset, model design, and algorithms. You want
    this to happen as soon as possible, so your model **converge** faster as your
    training is efficient. You also want the metric to flatten at a high accuracy
    or low loss region, so your model is effective in prediction.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些曲线最终应该平稳，意味着你无法根据当前数据集、模型设计和算法进一步改进模型。你希望这一点尽快发生，以便你的模型**收敛**更快，使训练更高效。你还希望指标在高准确率或低损失区域平稳，以便模型在预测中有效。
- en: The other property to watch for in the plots is how different are the metrics
    from training and validation. In the above, you see the training set’s RMSE is
    higher than test set’s RMSE at the beginning but very soon, the curves crossed
    and the test set’s RMSE is higher at the end. This is expected, as eventually
    the model will fit better to the training set but it is the test set that can
    predict how the model performs on future, unseen data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要关注的属性是训练和验证的指标差异。在上图中，你看到训练集的 RMSE 在开始时高于测试集的 RMSE，但很快曲线交叉，最后测试集的 RMSE
    更高。这是预期的，因为最终模型会更好地拟合训练集，但测试集可以预测模型在未来未见数据上的表现。
- en: You need to be careful to interpret the curves or metrics in a microscopic scale.
    In the plot above, you see that the training set’s RMSE is extremely large compare
    to that of test set’s in epoch 0\. Their difference may not be that drastic, but
    since you collected the training set’s RMSE by taking the MSE of each steps during
    the first epoch, your model probably not doing well in the first few steps but
    much better at the last few steps of the epoch. Taking average across all the
    steps may not be a fair comparison as the MSE from test set is based on the model
    after the last step.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要谨慎地在微观尺度上解释曲线或指标。在上图中，你会看到训练集的 RMSE 在第0轮时与测试集的 RMSE 相比极大。它们的差异可能并不那么显著，但由于你在第一个训练轮次中通过计算每个步骤的
    MSE 收集了训练集的 RMSE，你的模型可能在前几个步骤表现不好，但在训练轮次的最后几个步骤表现更好。在所有步骤上取平均可能不是一个公平的比较，因为测试集的
    MSE 基于最后一步后的模型。
- en: Your model is **overfit** if you see the training set’s metric is much better
    than that from test set. This can hint that you should stop your training at an
    earlier epoch or your model’s design need some regularization, such as dropout
    layer.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到训练集的指标远好于测试集，那么你的模型是**过拟合**的。这可能提示你应该在较早的训练轮次停止训练，或者模型设计需要一些正则化，例如 dropout
    层。
- en: In the plot above, while you collected mean square error (MSE) for the regression
    problem but you plotted root mean square error (RMSE) instead, so you can compare
    to the mean absolute error (MAE) in the same scale. Probably you should also collect
    the MAE of the training set as well. The two MAE curves should behave similarly
    to that of the RMSE curves.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，虽然你收集了回归问题的均方误差（MSE），但你绘制的是均方根误差（RMSE），以便你可以与均值绝对误差（MAE）在相同的尺度上进行比较。你可能还应该收集训练集的MAE。这两个MAE曲线应该与RMSE曲线的行为类似。
- en: 'Putting everything together, the following is the complete code:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容汇总，以下是完整的代码：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Further Readings
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: This section provides more resources on the topic if you are looking to go deeper.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了更多资源，供你深入了解该主题。
- en: APIs
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: APIs
- en: '[nn.L1Loss](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html)
    from PyTorch documentation'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[nn.L1Loss](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html)
    来自 PyTorch 文档'
- en: '[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)
    from PyTorch documentation'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)
    来自 PyTorch 文档'
- en: Summary
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, you discovered the importance of collecting and reviewing
    metrics while training your deep learning models. You learned:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你发现了在训练深度学习模型时收集和审查指标的重要性。你学到了：
- en: What metrics to look for during model training
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练过程中应关注哪些指标
- en: How to compute and collect metrics in a PyTorch training loop
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在 PyTorch 训练循环中计算和收集指标
- en: How to visualize the metrics from a training loop
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何从训练循环中可视化指标
- en: How to interpret the metrics to infer details about the training experience
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何解读指标以推断有关训练经验的详细信息
