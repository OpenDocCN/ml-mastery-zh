["```py\nfrom sklearn.datasets import make_circles\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Make data: Two circles on x-y plane as a classification problem\nX, y = make_circles(n_samples=1000, factor=0.5, noise=0.1)\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y.reshape(-1, 1), dtype=torch.float32)\n\nplt.figure(figsize=(8,6))\nplt.scatter(X[:,0], X[:,1], c=y)\nplt.show()\n```", "```py\nclass Model(nn.Module):\n    def __init__(self, activation=nn.ReLU):\n        super().__init__()\n        self.layer0 = nn.Linear(2,5)\n        self.act0 = activation()\n        self.layer1 = nn.Linear(5,5)\n        self.act1 = activation()\n        self.layer2 = nn.Linear(5,5)\n        self.act2 = activation()\n        self.layer3 = nn.Linear(5,5)\n        self.act3 = activation()\n        self.layer4 = nn.Linear(5,1)\n        self.act4 = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.act0(self.layer0(x))\n        x = self.act1(self.layer1(x))\n        x = self.act2(self.layer2(x))\n        x = self.act3(self.layer3(x))\n        x = self.act4(self.layer4(x))\n        return x\n\ndef train_loop(model, X, y, n_epochs=300, batch_size=32):\n    loss_fn = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n    batch_start = torch.arange(0, len(X), batch_size)\n\n    bce_hist = []\n    acc_hist = []\n\n    for epoch in range(n_epochs):\n        # train model with optimizer\n        model.train()\n        for start in batch_start:\n            X_batch = X[start:start+batch_size]\n            y_batch = y[start:start+batch_size]\n            y_pred = model(X_batch)\n            loss = loss_fn(y_pred, y_batch)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        # evaluate BCE and accuracy at end of each epoch\n        model.eval()\n        with torch.no_grad():\n            y_pred = model(X)\n            bce = float(loss_fn(y_pred, y))\n            acc = float((y_pred.round() == y).float().mean())\n        bce_hist.append(bce)\n        acc_hist.append(acc)\n        # print metrics every 10 epochs\n        if (epoch+1) % 10 == 0:\n            print(\"Before epoch %d: BCE=%.4f, Accuracy=%.2f%%\" % (epoch+1, bce, acc*100))\n    return bce_hist, acc_hist\n```", "```py\nactivation = nn.ReLU\nmodel = Model(activation=activation)\nbce_hist, acc_hist = train_loop(model, X, y)\nplt.plot(bce_hist, label=\"BCE\")\nplt.plot(acc_hist, label=\"Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylim(0, 1)\nplt.title(str(activation))\nplt.legend()\nplt.show()\n```", "```py\nBefore epoch 10: BCE=0.7025, Accuracy=50.00%\nBefore epoch 20: BCE=0.6990, Accuracy=50.00%\nBefore epoch 30: BCE=0.6959, Accuracy=50.00%\n...\nBefore epoch 280: BCE=0.3051, Accuracy=96.30%\nBefore epoch 290: BCE=0.2785, Accuracy=96.90%\nBefore epoch 300: BCE=0.2543, Accuracy=97.00%\n```", "```py\nx = torch.linspace(-4, 4, 200)\nrelu = nn.ReLU()(x)\ntanh = nn.Tanh()(x)\nsigmoid = nn.Sigmoid()(x)\n\nplt.plot(x, sigmoid, label=\"sigmoid\")\nplt.plot(x, tanh, label=\"tanh\")\nplt.plot(x, relu, label=\"ReLU\")\nplt.ylim(-1.5, 2)\nplt.legend()\nplt.show()\n```", "```py\ndef train_loop(model, X, y, n_epochs=300, batch_size=32):\n    loss_fn = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n    batch_start = torch.arange(0, len(X), batch_size)\n\n    bce_hist = []\n    acc_hist = []\n    grad_hist = [[],[],[],[],[]]\n\n    for epoch in range(n_epochs):\n        # train model with optimizer\n        model.train()\n        layer_grad = [[],[],[],[],[]]\n        for start in batch_start:\n            X_batch = X[start:start+batch_size]\n            y_batch = y[start:start+batch_size]\n            y_pred = model(X_batch)\n            loss = loss_fn(y_pred, y_batch)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            # collect mean absolute value of gradients\n            layers = [model.layer0, model.layer1, model.layer2, model.layer3, model.layer4]\n            for n,layer in enumerate(layers):\n                mean_grad = float(layer.weight.grad.abs().mean())\n                layer_grad[n].append(mean_grad)\n        # evaluate BCE and accuracy at end of each epoch\n        model.eval()\n        with torch.no_grad():\n            y_pred = model(X)\n            bce = float(loss_fn(y_pred, y))\n            acc = float((y_pred.round() == y).float().mean())\n        bce_hist.append(bce)\n        acc_hist.append(acc)\n        for n, grads in enumerate(layer_grad):\n            grad_hist[n].append(sum(grads)/len(grads))\n        # print metrics every 10 epochs\n        if epoch % 10 == 9:\n            print(\"Epoch %d: BCE=%.4f, Accuracy=%.2f%%\" % (epoch, bce, acc*100))\n    return bce_hist, acc_hist, layer_grad\n```", "```py\nactivation = nn.ReLU\nmodel = Model(activation=activation)\nbce_hist, acc_hist, grad_hist = train_loop(model, X, y)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\nax[0].plot(bce_hist, label=\"BCE\")\nax[0].plot(acc_hist, label=\"Accuracy\")\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylim(0, 1)\nfor n, grads in enumerate(grad_hist):\n    ax[1].plot(grads, label=\"layer\"+str(n))\nax[1].set_xlabel(\"Epochs\")\nfig.suptitle(str(activation))\nax[0].legend()\nax[1].legend()\nplt.show()\n```", "```py\nactivation = nn.Sigmoid\nmodel = Model(activation=activation)\nbce_hist, acc_hist, grad_hist = train_loop(model, X, y)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\nax[0].plot(bce_hist, label=\"BCE\")\nax[0].plot(acc_hist, label=\"Accuracy\")\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylim(0, 1)\nfor n, grads in enumerate(grad_hist):\n    ax[1].plot(grads, label=\"layer\"+str(n))\nax[1].set_xlabel(\"Epochs\")\nfig.suptitle(str(activation))\nax[0].legend()\nax[1].legend()\nplt.show()\n```", "```py\nactivation = nn.Tanh\nmodel = Model(activation=activation)\nbce_hist, acc_hist, grad_hist = train_loop(model, X, y)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\nax[0].plot(bce_hist, label=\"BCE\")\nax[0].plot(acc_hist, label=\"Accuracy\")\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylim(0, 1)\nfor n, grads in enumerate(grad_hist):\n    ax[1].plot(grads, label=\"layer\"+str(n))\nax[1].set_xlabel(\"Epochs\")\nfig.suptitle(str(activation))\nax[0].legend()\nax[1].legend()\nplt.show()\n```", "```py\nx = torch.linspace(-8, 8, 200)\nrelu = nn.ReLU()(x)\nrelu6 = nn.ReLU6()(x)\nleaky = nn.LeakyReLU()(x)\n\nplt.plot(x, relu, label=\"ReLU\")\nplt.plot(x, relu6, label=\"ReLU6\")\nplt.plot(x, leaky, label=\"LeakyReLU\")\nplt.legend()\nplt.show()\n```", "```py\nactivation = nn.ReLU6\nmodel = Model(activation=activation)\nbce_hist, acc_hist, grad_hist = train_loop(model, X, y)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\nax[0].plot(bce_hist, label=\"BCE\")\nax[0].plot(acc_hist, label=\"Accuracy\")\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylim(0, 1)\nfor n, grads in enumerate(grad_hist):\n    ax[1].plot(grads, label=\"layer\"+str(n))\nax[1].set_xlabel(\"Epochs\")\nfig.suptitle(str(activation))\nax[0].legend()\nax[1].legend()\nplt.show()\n```", "```py\nactivation = nn.LeakyReLU\nmodel = Model(activation=activation)\nbce_hist, acc_hist, grad_hist = train_loop(model, X, y)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\nax[0].plot(bce_hist, label=\"BCE\")\nax[0].plot(acc_hist, label=\"Accuracy\")\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylim(0, 1)\nfor n, grads in enumerate(grad_hist):\n    ax[1].plot(grads, label=\"layer\"+str(n))\nax[1].set_xlabel(\"Epochs\")\nfig.suptitle(str(activation))\nax[0].legend()\nax[1].legend()\nplt.show()\n```", "```py\nfrom sklearn.datasets import make_circles\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Make data: Two circles on x-y plane as a classification problem\nX, y = make_circles(n_samples=1000, factor=0.5, noise=0.1)\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y.reshape(-1, 1), dtype=torch.float32)\n\n# Binary classification model\nclass Model(nn.Module):\n    def __init__(self, activation=nn.ReLU):\n        super().__init__()\n        self.layer0 = nn.Linear(2,5)\n        self.act0 = activation()\n        self.layer1 = nn.Linear(5,5)\n        self.act1 = activation()\n        self.layer2 = nn.Linear(5,5)\n        self.act2 = activation()\n        self.layer3 = nn.Linear(5,5)\n        self.act3 = activation()\n        self.layer4 = nn.Linear(5,1)\n        self.act4 = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.act0(self.layer0(x))\n        x = self.act1(self.layer1(x))\n        x = self.act2(self.layer2(x))\n        x = self.act3(self.layer3(x))\n        x = self.act4(self.layer4(x))\n        return x\n\n# train the model and produce history\ndef train_loop(model, X, y, n_epochs=300, batch_size=32):\n    loss_fn = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n    batch_start = torch.arange(0, len(X), batch_size)\n\n    bce_hist = []\n    acc_hist = []\n    grad_hist = [[],[],[],[],[]]\n\n    for epoch in range(n_epochs):\n        # train model with optimizer\n        model.train()\n        layer_grad = [[],[],[],[],[]]\n        for start in batch_start:\n            X_batch = X[start:start+batch_size]\n            y_batch = y[start:start+batch_size]\n            y_pred = model(X_batch)\n            loss = loss_fn(y_pred, y_batch)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            # collect mean absolute value of gradients\n            layers = [model.layer0, model.layer1, model.layer2, model.layer3, model.layer4]\n            for n,layer in enumerate(layers):\n                mean_grad = float(layer.weight.grad.abs().mean())\n                layer_grad[n].append(mean_grad)\n        # evaluate BCE and accuracy at end of each epoch\n        model.eval()\n        with torch.no_grad():\n            y_pred = model(X)\n            bce = float(loss_fn(y_pred, y))\n            acc = float((y_pred.round() == y).float().mean())\n        bce_hist.append(bce)\n        acc_hist.append(acc)\n        for n, grads in enumerate(layer_grad):\n            grad_hist[n].append(sum(grads)/len(grads))\n        # print metrics every 10 epochs\n        if epoch % 10 == 9:\n            print(\"Epoch %d: BCE=%.4f, Accuracy=%.2f%%\" % (epoch, bce, acc*100))\n    return bce_hist, acc_hist, layer_grad\n\n# pick different activation functions and compare the result visually\nfor activation in [nn.Sigmoid, nn.Tanh, nn.ReLU, nn.ReLU6, nn.LeakyReLU]:\n    model = Model(activation=activation)\n    bce_hist, acc_hist, grad_hist = train_loop(model, X, y)\n\n    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n    ax[0].plot(bce_hist, label=\"BCE\")\n    ax[0].plot(acc_hist, label=\"Accuracy\")\n    ax[0].set_xlabel(\"Epochs\")\n    ax[0].set_ylim(0, 1)\n    for n, grads in enumerate(grad_hist):\n        ax[1].plot(grads, label=\"layer\"+str(n))\n    ax[1].set_xlabel(\"Epochs\")\n    fig.suptitle(str(activation))\n    ax[0].legend()\n    ax[1].legend()\n    plt.show()\n```"]