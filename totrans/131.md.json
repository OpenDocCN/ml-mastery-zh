["```py\nimport torch\nimport torch.nn as nn\n\nmae = nn.L1Loss()\nmse = nn.MSELoss()\n\npredict = torch.tensor([0., 3.])\ntarget = torch.tensor([1., 0.])\n\nprint(\"MAE: %.3f\" % mae(predict, target))\nprint(\"MSE: %.3f\" % mse(predict, target))\n```", "```py\nMAE: 2.000\nMSE: 5.000\n```", "```py\nimport torch\nimport torch.nn as nn\n\nce = nn.CrossEntropyLoss()\n\nlogits = torch.tensor([[-1.90, -0.29, -2.30], [-0.29, -1.90, -2.30]])\ntarget = torch.tensor([[0., 1., 0.], [1., 0., 0.]])\nprint(\"Cross entropy: %.3f\" % ce(logits, target))\n```", "```py\nCross entropy: 0.288\n```", "```py\nprobs = torch.tensor([[0.15, 0.75, 0.1], [0.75, 0.15, 0.1]])\n```", "```py\nimport torch\nimport torch.nn as nn\n\nce = nn.CrossEntropyLoss()\n\nlogits = torch.tensor([[-1.90, -0.29, -2.30], [-0.29, -1.90, -2.30]])\nindices = torch.tensor([1, 0])\nprint(\"Cross entropy: %.3f\" % ce(logits, indices))\n```", "```py\nimport torch\n\ntarget = torch.tensor([[0., 1., 0.], [1., 0., 0.]])\nindices = torch.argmax(target, dim=1)\nprint(indices)\n```", "```py\ntensor([1, 0])\n```", "```py\nimport torch\nimport torch.nn as nn\n\nce = nn.NLLLoss()\n\n# softmax to apply on dimension 1, i.e. per row\nlogsoftmax = nn.LogSoftmax(dim=1)\n\nlogits = torch.tensor([[-1.90, -0.29, -2.30], [-0.29, -1.90, -2.30]])\npred = logsoftmax(logits)\nindices = torch.tensor([1, 0])\nprint(\"Cross entropy: %.3f\" % ce(pred, indices))\n```", "```py\nimport torch\nimport torch.nn as nn\n\nbce = nn.BCELoss()\n\npred = torch.tensor([0.75, 0.25])\ntarget = torch.tensor([1., 0.])\nprint(\"Binary cross entropy: %.3f\" % bce(pred, target))\n```", "```py\nBinary cross entropy: 0.288\n```", "```py\nimport copy\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import StandardScaler\n\n# Read data\ndata = fetch_california_housing()\nX, y = data.data, data.target\n\n# train-test split for model evaluation\nX_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n\n# Standardizing data\nscaler = StandardScaler()\nscaler.fit(X_train_raw)\nX_train = scaler.transform(X_train_raw)\nX_test = scaler.transform(X_test_raw)\n\n# Convert to 2D PyTorch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n\n# Define the model\nmodel = nn.Sequential(\n    nn.Linear(8, 24),\n    nn.ReLU(),\n    nn.Linear(24, 12),\n    nn.ReLU(),\n    nn.Linear(12, 6),\n    nn.ReLU(),\n    nn.Linear(6, 1)\n)\n\n# loss function and optimizer\ndef loss_fn(output, target):\n    # MAPE loss\n    return torch.mean(torch.abs((target - output) / target))\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\nn_epochs = 100   # number of epochs to run\nbatch_size = 10  # size of each batch\nbatch_start = torch.arange(0, len(X_train), batch_size)\n\n# Hold the best model\nbest_mape = np.inf   # init to infinity\nbest_weights = None\n\nfor epoch in range(n_epochs):\n    model.train()\n    for start in batch_start:\n        # take a batch\n        X_batch = X_train[start:start+batch_size]\n        y_batch = y_train[start:start+batch_size]\n        # forward pass\n        y_pred = model(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        # update weights\n        optimizer.step()\n    # evaluate accuracy at end of each epoch\n    model.eval()\n    y_pred = model(X_test)\n    mape = float(loss_fn(y_pred, y_test))\n    if mape < best_mape:\n        best_mape = mape\n        best_weights = copy.deepcopy(model.state_dict())\n\n# restore model and return best accuracy\nmodel.load_state_dict(best_weights)\nprint(\"MAPE: %.2f\" % best_mape)\n\nmodel.eval()\nwith torch.no_grad():\n    # Test out inference with 5 samples\n    for i in range(5):\n        X_sample = X_test_raw[i: i+1]\n        X_sample = scaler.transform(X_sample)\n        X_sample = torch.tensor(X_sample, dtype=torch.float32)\n        y_pred = model(X_sample)\n        print(f\"{X_test_raw[i]} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")\n```"]