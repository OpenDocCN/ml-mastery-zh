["```py\n# Read dataset from OpenML\nfrom sklearn.datasets import fetch_openml\ndataset = fetch_openml(\"diabetes\", version=1, as_frame=True, return_X_y=False)[\"frame\"]\n```", "```py\nprint(type(dataset))\nprint(dataset.head())\n```", "```py\n<class 'pandas.core.frame.DataFrame'>\n   preg   plas  pres  skin   insu  mass   pedi   age            class\n0   6.0  148.0  72.0  35.0    0.0  33.6  0.627  50.0  tested_positive\n1   1.0   85.0  66.0  29.0    0.0  26.6  0.351  31.0  tested_negative\n2   8.0  183.0  64.0   0.0    0.0  23.3  0.672  32.0  tested_positive\n3   1.0   89.0  66.0  23.0   94.0  28.1  0.167  21.0  tested_negative\n4   0.0  137.0  40.0  35.0  168.0  43.1  2.288  33.0  tested_positive\n```", "```py\nimport sqlite3\n\nconn = sqlite3.connect(\":memory:\")\ncur = conn.cursor()\n```", "```py\nconn = sqlite3.connect(\"example.db\")\n```", "```py\n...\ncreate_sql = \"\"\"\n    CREATE TABLE diabetes(\n        preg NUM,\n        plas NUM, \n        pres NUM,\n        skin NUM,\n        insu NUM,\n        mass NUM,\n        pedi NUM,\n        age NUM,\n        class TEXT\n    )\n\"\"\"\ncur.execute(create_sql)\n```", "```py\n# Prepare a parameterized SQL for insert\ninsert_sql = \"INSERT INTO diabetes VALUES (?,?,?,?,?,?,?,?,?)\"\n# execute the SQL multiple times with each element in dataset.to_numpy().tolist()\ncur.executemany(insert_sql, dataset.to_numpy().tolist())\n```", "```py\nimport pandas as pd\n\ndef cursor2dataframe(cur):\n    \"\"\"Read the column header from the cursor and then the rows of\n    data from it. Afterwards, create a DataFrame\"\"\"\n    header = [x[0] for x in cur.description]\n    # gets data from the last executed SQL query\n    data = cur.fetchall()\n    # convert the data into a pandas DataFrame\n    return pd.DataFrame(data, columns=header)\n\n# get 5 random rows from the diabetes table\nselect_sql = \"SELECT * FROM diabetes ORDER BY random() LIMIT 5\"\ncur.execute(select_sql)\nsample = cursor2dataframe(cur)\nprint(sample)\n```", "```py\n   preg  plas  pres  skin  insu  mass   pedi  age            class\n0     2    90    68    42     0  38.2  0.503   27  tested_positive\n1     9   124    70    33   402  35.4  0.282   34  tested_negative\n2     7   160    54    32   175  30.5  0.588   39  tested_positive\n3     7   105     0     0     0   0.0  0.305   24  tested_negative\n4     1   107    68    19     0  26.5  0.165   24  tested_negative\n```", "```py\nimport sqlite3\n\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\n\n# Read dataset from OpenML\ndataset = fetch_openml(\"diabetes\", version=1, as_frame=True, return_X_y=False)[\"frame\"]\nprint(\"Data from OpenML:\")\nprint(type(dataset))\nprint(dataset.head())\n\n# Create database\nconn = sqlite3.connect(\":memory:\")\ncur = conn.cursor()\ncreate_sql = \"\"\"\n    CREATE TABLE diabetes(\n        preg NUM,\n        plas NUM,\n        pres NUM,\n        skin NUM,\n        insu NUM,\n        mass NUM,\n        pedi NUM,\n        age NUM,\n        class TEXT\n    )\n\"\"\"\ncur.execute(create_sql)\n\n# Insert data into the table using a parameterized SQL\ninsert_sql = \"INSERT INTO diabetes VALUES (?,?,?,?,?,?,?,?,?)\"\nrows = dataset.to_numpy().tolist()\ncur.executemany(insert_sql, rows)\n\ndef cursor2dataframe(cur):\n    \"\"\"Read the column header from the cursor and then the rows of\n    data from it. Afterwards, create a DataFrame\"\"\"\n    header = [x[0] for x in cur.description]\n    # gets data from the last executed SQL query\n    data = cur.fetchall()\n    # convert the data into a pandas DataFrame\n    return pd.DataFrame(data, columns=header)\n\n# get 5 random rows from the diabetes table\nselect_sql = \"SELECT * FROM diabetes ORDER BY random() LIMIT 5\"\ncur.execute(select_sql)\nsample = cursor2dataframe(cur)\nprint(\"Data from SQLite database:\")\nprint(sample)\n\n# close database connection\nconn.commit()\nconn.close()\n```", "```py\ndef datagen(batch_size):\n    conn = sqlite3.connect(\"diabetes.db\", check_same_thread=False)\n    cur = conn.cursor()\n    sql = f\"\"\"\n        SELECT preg, plas, pres, skin, insu, mass, pedi, age, class\n        FROM diabetes\n        ORDER BY random()\n        LIMIT {batch_size}\n    \"\"\"\n    while True:\n        cur.execute(sql)\n        data = cur.fetchall()\n        X = [row[:-1] for row in data]\n        y = [1 if row[-1]==\"tested_positive\" else 0 for row in data]\n        yield np.asarray(X), np.asarray(y)\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# create binary classification model\nmodel = Sequential()\nmodel.add(Dense(16, input_dim=8, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# train model\nhistory = model.fit(datagen(32), epochs=5, steps_per_epoch=2000)\n```", "```py\nEpoch 1/5\n2000/2000 [==============================] - 6s 3ms/step - loss: 2.2360 - accuracy: 0.6730\nEpoch 2/5\n2000/2000 [==============================] - 5s 2ms/step - loss: 0.5292 - accuracy: 0.7380\nEpoch 3/5\n2000/2000 [==============================] - 5s 2ms/step - loss: 0.4936 - accuracy: 0.7564\nEpoch 4/5\n2000/2000 [==============================] - 5s 2ms/step - loss: 0.4751 - accuracy: 0.7662\nEpoch 5/5\n2000/2000 [==============================] - 5s 2ms/step - loss: 0.4487 - accuracy: 0.7834\n```", "```py\nimport sqlite3\n\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# Create database\nconn = sqlite3.connect(\"diabetes.db\")\ncur = conn.cursor()\ncur.execute(\"DROP TABLE IF EXISTS diabetes\")\ncreate_sql = \"\"\"\n    CREATE TABLE diabetes(\n        preg NUM,\n        plas NUM,\n        pres NUM,\n        skin NUM,\n        insu NUM,\n        mass NUM,\n        pedi NUM,\n        age NUM,\n        class TEXT\n    )\n\"\"\"\ncur.execute(create_sql)\n\n# Read data from OpenML, insert data into the table using a parameterized SQL\ndataset = fetch_openml(\"diabetes\", version=1, as_frame=True, return_X_y=False)[\"frame\"]\ninsert_sql = \"INSERT INTO diabetes VALUES (?,?,?,?,?,?,?,?,?)\"\nrows = dataset.to_numpy().tolist()\ncur.executemany(insert_sql, rows)\n\n# Commit to flush change to disk, then close connection\nconn.commit()\nconn.close()\n\n# Create data generator for Keras classifier model\ndef datagen(batch_size):\n    \"\"\"A generator to produce samples from database\n    \"\"\"\n    # Tensorflow may run in different thread, thus needs check_same_thread=False\n    conn = sqlite3.connect(\"diabetes.db\", check_same_thread=False)\n    cur = conn.cursor()\n    sql = f\"\"\"\n        SELECT preg, plas, pres, skin, insu, mass, pedi, age, class\n        FROM diabetes\n        ORDER BY random()\n        LIMIT {batch_size}\n    \"\"\"\n    while True:\n        # Read rows from database\n        cur.execute(sql)\n        data = cur.fetchall()\n        # Extract features\n        X = [row[:-1] for row in data]\n        # Extract targets, encode into binary (0 or 1)\n        y = [1 if row[-1]==\"tested_positive\" else 0 for row in data]\n        yield np.asarray(X), np.asarray(y)\n\n# create binary classification model\nmodel = Sequential()\nmodel.add(Dense(16, input_dim=8, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# train model\nhistory = model.fit(datagen(32), epochs=5, steps_per_epoch=2000)\n```", "```py\nimport sklearn.datasets\n\n# get digits dataset (8x8 images of digits)\ndigits = sklearn.datasets.load_digits()\n```", "```py\nimport dbm\nimport pickle\n\n# create file if not exists, otherwise open for read/write\nwith dbm.open(\"digits.dbm\", \"c\") as db:\n    for idx in range(len(digits.target)):\n        db[str(idx)] = pickle.dumps((digits.images[idx], digits.target[idx]))\n```", "```py\nimport random\nimport numpy as np\n\n# number of images that we want in our sample\nbatchsize = 4\nimages = []\ntargets = []\n\n# open the database and read a sample\nwith dbm.open(\"digits.dbm\", \"r\") as db:\n    # get all keys from the database\n    keys = db.keys()\n    # randomly samples n keys\n    for key in random.sample(keys, batchsize):\n        # go through each key in the random sample\n        image, target = pickle.loads(db[key])\n        images.append(image)\n        targets.append(target)\n    print(np.asarray(images), np.asarray(targets))\n```", "```py\n[[[ 0\\.  0\\.  1\\.  9\\. 14\\. 11\\.  1\\.  0.]\n  [ 0\\.  0\\. 10\\. 15\\.  9\\. 13\\.  5\\.  0.]\n  [ 0\\.  3\\. 16\\.  7\\.  0\\.  0\\.  0\\.  0.]\n  [ 0\\.  5\\. 16\\. 16\\. 16\\. 10\\.  0\\.  0.]\n  [ 0\\.  7\\. 16\\. 11\\. 10\\. 16\\.  5\\.  0.]\n  [ 0\\.  2\\. 16\\.  5\\.  0\\. 12\\.  8\\.  0.]\n  [ 0\\.  0\\. 10\\. 15\\. 13\\. 16\\.  5\\.  0.]\n  [ 0\\.  0\\.  0\\.  9\\. 12\\.  7\\.  0\\.  0.]]\n...\n] [6 8 7 3]\n```", "```py\nimport dbm\nimport pickle\nimport random\n\nimport numpy as np\nimport sklearn.datasets\n\n# get digits dataset (8x8 images of digits)\ndigits = sklearn.datasets.load_digits()\n\n# create file if not exists, otherwise open for read/write\nwith dbm.open(\"digits.dbm\", \"c\") as db:\n    for idx in range(len(digits.target)):\n        db[str(idx)] = pickle.dumps((digits.images[idx], digits.target[idx]))\n\n# number of images that we want in our sample\nbatchsize = 4\nimages = []\ntargets = []\n\n# open the database and read a sample\nwith dbm.open(\"digits.dbm\", \"r\") as db:\n    # get all keys from the database\n    keys = db.keys()\n    # randomly samples n keys\n    for key in random.sample(keys, batchsize):\n        # go through each key in the random sample\n        image, target = pickle.loads(db[key])\n        images.append(image)\n        targets.append(target)\n    print(np.array(images), np.array(targets))\n```", "```py\ndef datagen(batch_size):\n    \"\"\"A generator to produce samples from database\n    \"\"\"\n    with dbm.open(\"digits.dbm\", \"r\") as db:\n        keys = db.keys()\n        while True:\n            images = []\n            targets = []\n            for key in random.sample(keys, batch_size):\n                image, target = pickle.loads(db[key])\n                images.append(image)\n                targets.append(target)\n            yield np.array(images).reshape(-1,64), np.array(targets)\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=64, activation='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=\"adam\",\n              metrics=[\"sparse_categorical_accuracy\"])\n\nhistory = model.fit(datagen(32), epochs=5, steps_per_epoch=1000)\n```", "```py\nEpoch 1/5\n1000/1000 [==============================] - 3s 2ms/step - loss: 0.6714 - sparse_categorical_accuracy: 0.8090\nEpoch 2/5\n1000/1000 [==============================] - 2s 2ms/step - loss: 0.1049 - sparse_categorical_accuracy: 0.9688\nEpoch 3/5\n1000/1000 [==============================] - 2s 2ms/step - loss: 0.0442 - sparse_categorical_accuracy: 0.9875\nEpoch 4/5\n1000/1000 [==============================] - 2s 2ms/step - loss: 0.0484 - sparse_categorical_accuracy: 0.9850\nEpoch 5/5\n1000/1000 [==============================] - 2s 2ms/step - loss: 0.0245 - sparse_categorical_accuracy: 0.9935\n```", "```py\nimport dbm\nimport pickle\nimport random\n\nimport numpy as np\nimport sklearn.datasets\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# get digits dataset (8x8 images of digits)\ndigits = sklearn.datasets.load_digits()\n\n# create file if not exists, otherwise open for read/write\nwith dbm.open(\"digits.dbm\", \"c\") as db:\n    for idx in range(len(digits.target)):\n        db[str(idx)] = pickle.dumps((digits.images[idx], digits.target[idx]))\n\n# retrieving data from database for model\ndef datagen(batch_size):\n    \"\"\"A generator to produce samples from database\n    \"\"\"\n    with dbm.open(\"digits.dbm\", \"r\") as db:\n        keys = db.keys()\n        while True:\n            images = []\n            targets = []\n            for key in random.sample(keys, batch_size):\n                image, target = pickle.loads(db[key])\n                images.append(image)\n                targets.append(target)\n            yield np.array(images).reshape(-1,64), np.array(targets)\n\n# Classification model in Keras\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=64, activation='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=\"adam\",\n              metrics=[\"sparse_categorical_accuracy\"])\n\n# Train with data from dbm store\nhistory = model.fit(datagen(32), epochs=5, steps_per_epoch=1000)\n```", "```py\npip install openpyxl\n```", "```py\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nimport openpyxl\n\n# Read dataset from OpenML\ndataset = fetch_openml(\"diabetes\", version=1, as_frame=True, return_X_y=False)[\"frame\"]\nheader = list(dataset.columns)\ndata = dataset.to_numpy().tolist()\n\n# Create Excel workbook and write data into the default worksheet\nwb = openpyxl.Workbook()\nsheet = wb.active # use the default worksheet\nsheet.title = \"Diabetes\"\nfor n,colname in enumerate(header):\n    sheet.cell(row=1, column=1+n, value=colname)\nfor n,row in enumerate(data):\n    for m,cell in enumerate(row):\n        sheet.cell(row=2+n, column=1+m, value=cell)\n# Save\nwb.save(\"MLM.xlsx\")\n```", "```py\nsheet.cell(row=3, column=4, value=\"my data\")\n```", "```py\nsheet.cell(row=3, column=4).value\n```", "```py\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nimport openpyxl\n\n# Read dataset from OpenML\ndataset = fetch_openml(\"diabetes\", version=1, as_frame=True, return_X_y=False)[\"frame\"]\nheader = list(dataset.columns)\ndata = dataset.to_numpy().tolist()\n\n# Create Excel workbook and write data into the default worksheet\nwb = openpyxl.Workbook()\nsheet = wb.create_sheet(\"Diabetes\")  # or wb.active for default sheet\nsheet.append(header)\nfor row in data:\n    sheet.append(row)\n# Save\nwb.save(\"MLM.xlsx\")\n```", "```py\nimport random\n\nimport numpy as np\nimport openpyxl\nfrom sklearn.datasets import fetch_openml\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# Read data from OpenML\ndataset = fetch_openml(\"diabetes\", version=1, as_frame=True, return_X_y=False)[\"frame\"]\nheader = list(dataset.columns)\nrows = dataset.to_numpy().tolist()\n\n# Create Excel workbook and write data into the default worksheet\nwb = openpyxl.Workbook()\nsheet = wb.active\nsheet.title = \"Diabetes\"\nsheet.append(header)\nfor row in rows:\n    sheet.append(row)\n# Save\nwb.save(\"MLM.xlsx\")\n\n# Create data generator for Keras classifier model\ndef datagen(batch_size):\n    \"\"\"A generator to produce samples from database\n    \"\"\"\n    wb = openpyxl.load_workbook(\"MLM.xlsx\", read_only=True)\n    sheet = wb.active\n    maxrow = sheet.max_row\n    while True:\n        # Read rows from Excel file\n        X = []\n        y = []\n        for _ in range(batch_size):\n            # data starts at row 2\n            row_num = random.randint(2, maxrow)\n            rowdata = [cell.value for cell in sheet[row_num]]\n            X.append(rowdata[:-1])\n            y.append(1 if rowdata[-1]==\"tested_positive\" else 0)\n        yield np.asarray(X), np.asarray(y)\n\n# create binary classification model\nmodel = Sequential()\nmodel.add(Dense(16, input_dim=8, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# train model\nhistory = model.fit(datagen(32), epochs=5, steps_per_epoch=20)\n```", "```py\nimport random\n\nimport numpy as np\nimport openpyxl\nfrom sklearn.datasets import fetch_openml\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# Read data from OpenML\ndataset = fetch_openml(\"diabetes\", version=1, as_frame=True, return_X_y=False)[\"frame\"]\nheader = list(dataset.columns)\nrows = dataset.to_numpy().tolist()\n\n# Create Excel workbook and write data into the default worksheet\nwb = openpyxl.Workbook()\nsheet = wb.active\nsheet.title = \"Diabetes\"\nsheet.append(header)\nfor row in rows:\n    sheet.append(row)\n# Save\nwb.save(\"MLM.xlsx\")\n\n# Read entire worksheet from the Excel file\nwb = openpyxl.load_workbook(\"MLM.xlsx\", read_only=True)\nsheet = wb.active\nX = []\ny = []\nfor i, row in enumerate(sheet.rows):\n    if i==0:\n        continue # skip the header row\n    rowdata = [cell.value for cell in row]\n    X.append(rowdata[:-1])\n    y.append(1 if rowdata[-1]==\"tested_positive\" else 0)\nX, y = np.asarray(X), np.asarray(y)\n\n# create binary classification model\nmodel = Sequential()\nmodel.add(Dense(16, input_dim=8, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# train model\nhistory = model.fit(X, y, epochs=5)\n```", "```py\npip install google-api-python-client google-auth-httplib2 google-auth-oauthlib\n```", "```py\n{\n  \"type\": \"service_account\",\n  \"project_id\": \"mlm-python\",\n  \"private_key_id\": \"3863a6254774259a1249\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n\n                  MIIEvgIBADANBgkqh...\n                  -----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"ml-access@mlm-python.iam.gserviceaccount.com\",\n  \"client_id\": \"11542775381574\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/ml-access%40mlm-python.iam.gserviceaccount.com\"\n}\n```", "```py\nfrom oauth2client.service_account import ServiceAccountCredentials\nfrom googleapiclient.discovery import build\nfrom httplib2 import Http\n\ncred_file = \"mlm-python.json\"\nscopes = ['https://www.googleapis.com/auth/spreadsheets']\ncred = ServiceAccountCredentials.from_json_keyfile_name(cred_file, scopes)\nservice = build(\"sheets\", \"v4\", http=cred.authorize(Http()))\nsheet = service.spreadsheets()\n```", "```py\n...\n\nsheet_id = '12Pc2_pX3HOSltcRLHtqiq3RSOL9RcG72CZxRqsMeRul'\nbody = {\n    \"requests\": [{\n        \"deleteRange\": {\n            \"range\": {\n                \"sheetId\": 0\n            },\n            \"shiftDimension\": \"ROWS\"\n        }\n    }]\n}\naction = sheet.batchUpdate(spreadsheetId=sheet_id, body=body)\naction.execute()\n```", "```py\n...\nrows = [list(dataset.columns)]\nrows += dataset.to_numpy().tolist()\nmaxcol = max(len(row) for row in rows)\nmaxcol = chr(ord(\"A\") - 1 + maxcol)\naction = sheet.values().append(\n    spreadsheetId = sheet_id,\n    body = {\"values\": rows},\n    valueInputOption = \"RAW\",\n    range = \"Sheet1!A1:%s\" % maxcol\n)\naction.execute()\n```", "```py\n...\n# Check the sheets\nsheet_properties = sheet.get(spreadsheetId=sheet_id).execute()[\"sheets\"]\nprint(sheet_properties)\n# Read it back\nmaxrow = sheet_properties[0][\"properties\"][\"gridProperties\"][\"rowCount\"]\nmaxcol = sheet_properties[0][\"properties\"][\"gridProperties\"][\"columnCount\"]\nmaxcol = chr(ord(\"A\") - 1 + maxcol)\nrow = random.randint(1, maxrow)\nreadrange = f\"A{row}:{maxcol}{row}\"\ndata = sheet.values().get(spreadsheetId=sheet_id, range=readrange).execute()\n```", "```py\n[{'properties': {'sheetId': 0, 'title': 'Sheet1', 'index': 0,\n'sheetType': 'GRID', 'gridProperties': {'rowCount': 769, 'columnCount': 9}}}]\n```", "```py\n{'range': 'Sheet1!A536:I536',\n 'majorDimension': 'ROWS',\n 'values': [['1',\n   '77',\n   '56',\n   '30',\n   '56',\n   '33.3',\n   '1.251',\n   '24',\n   'tested_negative']]}\n```", "```py\nimport random\n\nfrom googleapiclient.discovery import build\nfrom httplib2 import Http\nfrom oauth2client.service_account import ServiceAccountCredentials\nfrom sklearn.datasets import fetch_openml\n\n# Connect to Google Sheet\ncred_file = \"mlm-python.json\"\nscopes = ['https://www.googleapis.com/auth/spreadsheets']\ncred = ServiceAccountCredentials.from_json_keyfile_name(cred_file, scopes)\nservice = build(\"sheets\", \"v4\", http=cred.authorize(Http()))\nsheet = service.spreadsheets()\n\n# Google Sheet ID, as granted access to the service account\nsheet_id = '12Pc2_pX3HOSltcRLHtqiq3RSOL9RcG72CZxRqsMeRul'\n\n# Delete everything on spreadsheet 0\nbody = {\n    \"requests\": [{\n        \"deleteRange\": {\n            \"range\": {\n                \"sheetId\": 0\n            },\n            \"shiftDimension\": \"ROWS\"\n        }\n    }]\n}\naction = sheet.batchUpdate(spreadsheetId=sheet_id, body=body)\naction.execute()\n\n# Read dataset from OpenML\ndataset = fetch_openml(\"diabetes\", version=1, as_frame=True, return_X_y=False)[\"frame\"]\nrows = [list(dataset.columns)]       # column headers\nrows += dataset.to_numpy().tolist()  # rows of data\n\n# Write to spreadsheet 0\nmaxcol = max(len(row) for row in rows)\nmaxcol = chr(ord(\"A\") - 1 + maxcol)\naction = sheet.values().append(\n    spreadsheetId = sheet_id,\n    body = {\"values\": rows},\n    valueInputOption = \"RAW\",\n    range = \"Sheet1!A1:%s\" % maxcol\n)\naction.execute()\n\n# Check the sheets\nsheet_properties = sheet.get(spreadsheetId=sheet_id).execute()[\"sheets\"]\nprint(sheet_properties)\n\n# Read a random row of data\nmaxrow = sheet_properties[0][\"properties\"][\"gridProperties\"][\"rowCount\"]\nmaxcol = sheet_properties[0][\"properties\"][\"gridProperties\"][\"columnCount\"]\nmaxcol = chr(ord(\"A\") - 1 + maxcol)\nrow = random.randint(1, maxrow)\nreadrange = f\"A{row}:{maxcol}{row}\"\ndata = sheet.values().get(spreadsheetId=sheet_id, range=readrange).execute()\nprint(data)\n```", "```py\nimport gspread\n\ncred_file = \"mlm-python.json\"\ngc = gspread.service_account(filename=cred_file)\nsheet = gc.open_by_key(sheet_id)\nspreadsheet = sheet.get_worksheet(0)\nprint(spreadsheet.row_count, spreadsheet.col_count)\n```", "```py\n...\n# Clear all data\nspreadsheet.clear()\n# Write to spreadsheet\nspreadsheet.append_rows(rows)\n# Read a random row of data\nmaxcol = chr(ord(\"A\") - 1 + spreadsheet.col_count)\nrow = random.randint(2, spreadsheet.row_count)\nreadrange = f\"A{row}:{maxcol}{row}\"\ndata = spreadsheet.get(readrange)\nprint(data)\n```", "```py\nimport random\n\nimport gspread\nfrom sklearn.datasets import fetch_openml\n\n# Google Sheet ID, as granted access to the service account\nsheet_id = '12Pc2_pX3HOSltcRLHtqiq3RSOL9RcG72CZxRqsMeRul'\n\n# Connect to Google Sheet\ncred_file = \"mlm-python.json\"\ngc = gspread.service_account(filename=cred_file)\nsheet = gc.open_by_key(sheet_id)\nspreadsheet = sheet.get_worksheet(0)\n\n# Clear all data\nspreadsheet.clear()\n\n# Read dataset from OpenML\ndataset = fetch_openml(\"diabetes\", version=1, as_frame=True, return_X_y=False)[\"frame\"]\nrows = [list(dataset.columns)]       # column headers\nrows += dataset.to_numpy().tolist()  # rows of data\n\n# Write to spreadsheet\nspreadsheet.append_rows(rows)\n\n# Check the number of rows and columns in the spreadsheet\nprint(spreadsheet.row_count, spreadsheet.col_count)\n\n# Read a random row of data\nmaxcol = chr(ord(\"A\") - 1 + spreadsheet.col_count)\nrow = random.randint(2, spreadsheet.row_count)\nreadrange = f\"A{row}:{maxcol}{row}\"\ndata = spreadsheet.get(readrange)\nprint(data)\n```", "```py\nimport random\n\nimport numpy as np\nimport gspread\nfrom sklearn.datasets import fetch_openml\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# Google Sheet ID, as granted access to the service account\nsheet_id = '12Pc2_pX3HOSltcRLHtqiq3RSOL9RcG72CZxRqsMeRul'\n\n# Connect to Google Sheet\ncred_file = \"mlm-python.json\"\ngc = gspread.service_account(filename=cred_file)\nsheet = gc.open_by_key(sheet_id)\nspreadsheet = sheet.get_worksheet(0)\n\n# Clear all data\nspreadsheet.clear()\n\n# Read dataset from OpenML\ndataset = fetch_openml(\"diabetes\", version=1, as_frame=True, return_X_y=False)[\"frame\"]\nrows = [list(dataset.columns)]       # column headers\nrows += dataset.to_numpy().tolist()  # rows of data\n\n# Write to spreadsheet\nspreadsheet.append_rows(rows)\n\n# Read the entire spreadsheet, except header\nmaxrow = spreadsheet.row_count\nmaxcol = chr(ord(\"A\") - 1 + spreadsheet.col_count)\ndata = spreadsheet.get(f\"A2:{maxcol}{maxrow}\")\nX = [row[:-1] for row in data]\ny = [1 if row[-1]==\"tested_positive\" else 0 for row in data]\nX, y = np.asarray(X).astype(float), np.asarray(y)\n\n# create binary classification model\nmodel = Sequential()\nmodel.add(Dense(16, input_dim=8, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# train model\nhistory = model.fit(X, y, epochs=5)\n```"]