["```py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\n\n# Load MNIST training data\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor()\n])\ntrain = torchvision.datasets.MNIST('./datafiles/', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\n\n# LeNet5 model\ntorch_model = nn.Sequential(\n    nn.Conv2d(1, 6, kernel_size=(5,5), stride=1, padding=2),\n    nn.Tanh(),\n    nn.AvgPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n    nn.Tanh(),\n    nn.AvgPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(16, 120, kernel_size=5, stride=1, padding=0),\n    nn.Tanh(),\n    nn.Flatten(),\n    nn.Linear(120, 84),\n    nn.Tanh(),\n    nn.Linear(84, 10),\n    nn.Softmax(dim=1)\n)\n\n# Training loop\ndef training_loop(model, optimizer, loss_fn, train_loader, n_epochs=100):\n    model.train()\n    for epoch in range(n_epochs):\n        for data, target in train_loader:\n            output = model(data)\n            loss = loss_fn(output, target)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n    model.eval()\n\n# Run training\noptimizer = optim.Adam(torch_model.parameters())\nloss_fn = nn.CrossEntropyLoss()\ntraining_loop(torch_model, optimizer, loss_fn, train_loader, n_epochs=20)\n\n# Save model\ntorch.save(torch_model, \"lenet5.pt\")\n```", "```py\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dense, AveragePooling2D, Flatten\nfrom tensorflow.keras.datasets import mnist\n\n# LeNet5 model\nkeras_model = Sequential([\n    Conv2D(6, (5,5), input_shape=(28,28,1), padding=\"same\", activation=\"tanh\"),\n    AveragePooling2D((2,2), strides=2),\n    Conv2D(16, (5,5), activation=\"tanh\"),\n    AveragePooling2D((2,2), strides=2),\n    Conv2D(120, (5,5), activation=\"tanh\"),\n    Flatten(),\n    Dense(84, activation=\"tanh\"),\n    Dense(10, activation=\"softmax\")\n])\n\n# Reshape data to shape of (n_sample, height, width, n_channel)\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = np.expand_dims(X_train, axis=3).astype('float32')\n\n# Train\nkeras_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nkeras_model.fit(X_train, y_train, epochs=20, batch_size=32)\n\n# Save\nkeras_model.save(\"lenet5.h5\")\n```", "```py\nimport torch\nimport tensorflow as tf\ntorch_model = torch.load(\"lenet5.pt\")\nkeras_model = tf.keras.models.load_model(\"lenet5.h5\")\n```", "```py\nPython 3.9.13 (main, May 19 2022, 13:48:47)\n[Clang 13.1.6 (clang-1316.0.21.2)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\n>>> import tensorflow as tf\n>>> torch_model = torch.load(\"lenet5.pt\")\n>>> keras_model = tf.keras.models.load_model(\"lenet5.h5\")\n```", "```py\n>>> type(torch_model)\n<class 'torch.nn.modules.container.Sequential'>\n>>> type(keras_model)\n<class 'keras.engine.sequential.Sequential'>\n```", "```py\n>>> dir(torch_model)\n['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', \n'__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', \n...\n'_slow_forward', '_state_dict_hooks', '_version', 'add_module', 'append', 'apply', \n'bfloat16', 'buffers', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', \n'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', \n'get_submodule', 'half', 'load_state_dict', 'modules', 'named_buffers', \n'named_children', 'named_modules', 'named_parameters', 'parameters', \n'register_backward_hook', 'register_buffer', 'register_forward_hook', \n'register_forward_pre_hook', 'register_full_backward_hook', 'register_module', \n'register_parameter', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict',\n'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n>>> dir(keras_model)\n['_SCALAR_UPRANKING_ON', '_TF_MODULE_IGNORED_PROPERTIES', '__call__', '__class__', \n'__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', \n...\n'activity_regularizer', 'add', 'add_loss', 'add_metric', 'add_update', 'add_variable', \n'add_weight', 'build', 'built', 'call', 'compile', 'compiled_loss', 'compiled_metrics', \n'compute_dtype', 'compute_loss', 'compute_mask', 'compute_metrics', \n'compute_output_shape', 'compute_output_signature', 'count_params', \n'distribute_strategy', 'dtype', 'dtype_policy', 'dynamic', 'evaluate', \n'evaluate_generator', 'finalize_state', 'fit', 'fit_generator', 'from_config', \n'get_config', 'get_input_at', 'get_input_mask_at', 'get_input_shape_at', 'get_layer', \n'get_output_at', 'get_output_mask_at', 'get_output_shape_at', 'get_weights', 'history', \n'inbound_nodes', 'input', 'input_mask', 'input_names', 'input_shape', 'input_spec', \n'inputs', 'layers', 'load_weights', 'loss', 'losses', 'make_predict_function', \n'make_test_function', 'make_train_function', 'metrics', 'metrics_names', 'name', \n'name_scope', 'non_trainable_variables', 'non_trainable_weights', 'optimizer', \n'outbound_nodes', 'output', 'output_mask', 'output_names', 'output_shape', 'outputs', \n'pop', 'predict', 'predict_function', 'predict_generator', 'predict_on_batch', \n'predict_step', 'reset_metrics', 'reset_states', 'run_eagerly', 'save', 'save_spec', \n'save_weights', 'set_weights', 'state_updates', 'stateful', 'stop_training', \n'submodules', 'summary', 'supports_masking', 'test_function', 'test_on_batch', \n'test_step', 'to_json', 'to_yaml', 'train_function', 'train_on_batch', 'train_step', \n'train_tf_function', 'trainable', 'trainable_variables', 'trainable_weights', 'updates',\n'variable_dtype', 'variables', 'weights', 'with_name_scope']\n```", "```py\n>>> import inspect\n>>> inspect(torch_model)\n>>> inspect.getmembers(torch_model)\n[('T_destination', ~T_destination), ('__annotations__', {'_modules': typing.Dict[str, \ntorch.nn.modules.module.Module]}), ('__call__', <bound method Module._call_impl of \nSequential(\n...\n```", "```py\n>>> [n for n in dir(torch_model) if 'state' in n]\n['__setstate__', '_load_from_state_dict', '_load_state_dict_pre_hooks', \n'_register_load_state_dict_pre_hook', '_register_state_dict_hook', \n'_save_to_state_dict', '_state_dict_hooks', 'get_extra_state', 'load_state_dict', \n'set_extra_state', 'state_dict']\n>>> [n for n in dir(keras_model) if 'weight' in n]\n['_assert_weights_created', '_captured_weight_regularizer', \n'_check_sample_weight_warning', '_dedup_weights', '_handle_weight_regularization', \n'_initial_weights', '_non_trainable_weights', '_trainable_weights', \n'_undeduplicated_weights', 'add_weight', 'get_weights', 'load_weights', \n'non_trainable_weights', 'save_weights', 'set_weights', 'trainable_weights', 'weights']\n```", "```py\n>>> torch_model.state_dict\n<bound method Module.state_dict of Sequential(\n  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n  (1): Tanh()\n  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n  (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (4): Tanh()\n  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n  (6): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n  (7): Tanh()\n  (8): Flatten(start_dim=1, end_dim=-1)\n  (9): Linear(in_features=120, out_features=84, bias=True)\n  (10): Tanh()\n  (11): Linear(in_features=84, out_features=10, bias=True)\n  (12): Softmax(dim=1)\n)>\n>>> torch_model.state_dict()\nOrderedDict([('0.weight', tensor([[[[ 0.1559,  0.1681,  0.2726,  0.3187,  0.4909],\n          [ 0.1179,  0.1340, -0.0815, -0.3253,  0.0904],\n          [ 0.2326, -0.2079, -0.8614, -0.8643, -0.0632],\n          [ 0.3874, -0.3490, -0.7957, -0.5873, -0.0638],\n          [ 0.2800,  0.0947,  0.0308,  0.4065,  0.6916]]],\n\n        [[[ 0.5116,  0.1798, -0.1062, -0.4099, -0.3307],\n          [ 0.1090,  0.0689, -0.1010, -0.9136, -0.5271],\n          [ 0.2910,  0.2096, -0.2442, -1.5576, -0.0305],\n...\n```", "```py\n>>> keras_model.get_weights\n<bound method Model.get_weights of <keras.engine.sequential.Sequential object at 0x159d93eb0>>\n>>> keras_model.get_weights()\n[array([[[[ 0.14078194,  0.04990018, -0.06204645, -0.03128023,\n          -0.22033708,  0.19721672]],\n\n        [[-0.06618818, -0.152075  ,  0.13130261,  0.22893831,\n           0.08880515,  0.01917628]],\n\n        [[-0.28716782, -0.23207009,  0.00505603,  0.2697424 ,\n          -0.1916888 , -0.25858143]],\n\n        [[-0.41863152, -0.20710683,  0.13254236,  0.18774481,\n          -0.14866787, -0.14398652]],\n\n        [[-0.25119543, -0.14405733, -0.048533  , -0.12108403,\n           0.06704573, -0.1196835 ]]],\n\n       [[[-0.2438466 ,  0.02499897, -0.1243961 , -0.20115352,\n          -0.0241346 ,  0.15888865]],\n\n        [[-0.20548582, -0.26495507,  0.21004884,  0.32183227,\n          -0.13990627, -0.02996112]],\n...\n```", "```py\n>>> keras_model.weights\n[<tf.Variable 'conv2d/kernel:0' shape=(5, 5, 1, 6) dtype=float32, numpy=\narray([[[[ 0.14078194,  0.04990018, -0.06204645, -0.03128023,\n          -0.22033708,  0.19721672]],\n\n        [[-0.06618818, -0.152075  ,  0.13130261,  0.22893831,\n           0.08880515,  0.01917628]],\n...\n         8.25365111e-02, -1.72486171e-01,  3.16280037e-01,\n         4.12595004e-01]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32, numpy=\narray([-0.19007775,  0.14427921,  0.0571407 , -0.24149619, -0.03247226,\n        0.18109408, -0.17159976,  0.21736498, -0.10254183,  0.02417901],\n      dtype=float32)>]\n```", "```py\n>>> [(key, val.shape) for key, val in torch_model.state_dict().items()]\n[('0.weight', torch.Size([6, 1, 5, 5])), ('0.bias', torch.Size([6])), ('3.weight', \ntorch.Size([16, 6, 5, 5])), ('3.bias', torch.Size([16])), ('6.weight', torch.Size([120,\n16, 5, 5])), ('6.bias', torch.Size([120])), ('9.weight', torch.Size([84, 120])), \n('9.bias', torch.Size([84])), ('11.weight', torch.Size([10, 84])), ('11.bias', \ntorch.Size([10]))]\n>>> [arr.shape for arr in keras_model.get_weights()]\n[(5, 5, 1, 6), (6,), (5, 5, 6, 16), (16,), (5, 5, 16, 120), (120,), (120, 84), (84,), \n(84, 10), (10,)]\n```", "```py\n>>> keras_model.layers\n[<keras.layers.convolutional.conv2d.Conv2D object at 0x159ddd850>, \n<keras.layers.pooling.average_pooling2d.AveragePooling2D object at 0x159ddd820>, \n<keras.layers.convolutional.conv2d.Conv2D object at 0x15a12b1c0>, \n<keras.layers.pooling.average_pooling2d.AveragePooling2D object at 0x15a1705e0>, \n<keras.layers.convolutional.conv2d.Conv2D object at 0x15a1812b0>, \n<keras.layers.reshaping.flatten.Flatten object at 0x15a194310>, \n<keras.layers.core.dense.Dense object at 0x15a1947c0>, <keras.layers.core.dense.Dense \nobject at 0x15a194910>]\n>>> [layer.name for layer in keras_model.layers]\n['conv2d', 'average_pooling2d', 'conv2d_1', 'average_pooling2d_1', 'conv2d_2', \n'flatten', 'dense', 'dense_1']\n>>>\n```", "```py\n>>> torch_states = torch_model.state_dict()\n>>> torch_states.keys()\nodict_keys(['0.weight', '0.bias', '3.weight', '3.bias', '6.weight', '6.bias', '9.weight', '9.bias', '11.weight', '11.bias'])\n>>> torch_states[\"0.weight\"]\ntensor([[[[ 0.1559,  0.1681,  0.2726,  0.3187,  0.4909],\n          [ 0.1179,  0.1340, -0.0815, -0.3253,  0.0904],\n          [ 0.2326, -0.2079, -0.8614, -0.8643, -0.0632],\n          [ 0.3874, -0.3490, -0.7957, -0.5873, -0.0638],\n          [ 0.2800,  0.0947,  0.0308,  0.4065,  0.6916]]],\n...\n        [[[ 0.0980,  0.0240,  0.3295,  0.4507,  0.4539],\n          [-0.1530, -0.3991, -0.3834, -0.2716,  0.0809],\n          [-0.4639, -0.5537, -1.0207, -0.8049, -0.4977],\n          [ 0.1825, -0.1284, -0.0669, -0.4652, -0.2961],\n          [ 0.3402,  0.4256,  0.4329,  0.1503,  0.4207]]]])\n>>> dir(torch_states[\"0.weight\"])\n['H', 'T', '__abs__', '__add__', '__and__', '__array__', '__array_priority__', \n'__array_wrap__', '__bool__', '__class__', '__complex__', '__contains__', \n...\n'trunc', 'trunc_', 'type', 'type_as', 'unbind', 'unflatten', 'unfold', 'uniform_', \n'unique', 'unique_consecutive', 'unsafe_chunk', 'unsafe_split', \n'unsafe_split_with_sizes', 'unsqueeze', 'unsqueeze_', 'values', 'var', 'vdot', 'view', \n'view_as', 'vsplit', 'where', 'xlogy', 'xlogy_', 'xpu', 'zero_']\n>>> torch_states[\"0.weight\"].numpy()\narray([[[[ 0.15587455,  0.16805592,  0.27259687,  0.31871665,\n           0.49091515],\n         [ 0.11791296,  0.13400094, -0.08148099, -0.32530317,\n           0.09039831],\n...\n         [ 0.18252987, -0.12838107, -0.0669101 , -0.4652463 ,\n          -0.2960882 ],\n         [ 0.34022188,  0.4256311 ,  0.4328527 ,  0.15025541,\n           0.4207182 ]]]], dtype=float32)\n>>> torch_states[\"0.weight\"].shape\ntorch.Size([6, 1, 5, 5])\n>>> torch_states[\"0.weight\"].numpy().shape\n(6, 1, 5, 5)\n```", "```py\n>>> help(torch_states[\"0.weight\"].numpy)\n```", "```py\n>>> keras_weights = keras_model.get_weights()\n>>> keras_weights[0].shape\n(5, 5, 1, 6)\n```", "```py\n>>> keras_weights[6].shape\n(120, 84)\n>>> list(torch_states.values())[6].shape\ntorch.Size([84, 120])\n```", "```py\n>>> for k,t in zip(keras_weights, torch_states.values()):\n...     print(f\"Keras: {k.shape}, Torch: {t.shape}\")\n...\nKeras: (5, 5, 1, 6), Torch: torch.Size([6, 1, 5, 5])\nKeras: (6,), Torch: torch.Size([6])\nKeras: (5, 5, 6, 16), Torch: torch.Size([16, 6, 5, 5])\nKeras: (16,), Torch: torch.Size([16])\nKeras: (5, 5, 16, 120), Torch: torch.Size([120, 16, 5, 5])\nKeras: (120,), Torch: torch.Size([120])\nKeras: (120, 84), Torch: torch.Size([84, 120])\nKeras: (84,), Torch: torch.Size([84])\nKeras: (84, 10), Torch: torch.Size([10, 84])\nKeras: (10,), Torch: torch.Size([10])\n```", "```py\n>>> for k, t in zip(keras_model.weights, torch_states.keys()):\n...     print(f\"Keras: {k.name}, Torch: {t}\")\n...\nKeras: conv2d/kernel:0, Torch: 0.weight\nKeras: conv2d/bias:0, Torch: 0.bias\nKeras: conv2d_1/kernel:0, Torch: 3.weight\nKeras: conv2d_1/bias:0, Torch: 3.bias\nKeras: conv2d_2/kernel:0, Torch: 6.weight\nKeras: conv2d_2/bias:0, Torch: 6.bias\nKeras: dense/kernel:0, Torch: 9.weight\nKeras: dense/bias:0, Torch: 9.bias\nKeras: dense_1/kernel:0, Torch: 11.weight\nKeras: dense_1/bias:0, Torch: 11.bias\n```", "```py\n>>> keras_model.set_weights\n<bound method Layer.set_weights of <keras.engine.sequential.Sequential object at 0x159d93eb0>>\n>>> torch_model.load_state_dict\n<bound method Module.load_state_dict of Sequential(\n  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n  (1): Tanh()\n  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n  (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (4): Tanh()\n  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n  (6): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n  (7): Tanh()\n  (8): Flatten(start_dim=1, end_dim=-1)\n  (9): Linear(in_features=120, out_features=84, bias=True)\n  (10): Tanh()\n  (11): Linear(in_features=84, out_features=10, bias=True)\n  (12): Softmax(dim=1)\n)>\n>>> help(torch_model.load_state_dict)\n\n>>> help(keras_model.set_weights)\n```", "```py\nimport torch\nimport tensorflow as tf\n\n# Load the models\ntorch_model = torch.load(\"lenet5.pt\")\nkeras_model = tf.keras.models.load_model(\"lenet5.h5\")\n\n# Extract weights from Keras model\nkeras_weights = keras_model.get_weights()\n\n# Transform shape from Keras to PyTorch\nfor idx in [0, 2, 4]:\n    # conv layers: (out, in, height, width)\n    keras_weights[idx] = keras_weights[idx].transpose([3, 2, 0, 1])\nfor idx in [6, 8]:\n    # dense layers: (out, in)\n    keras_weights[idx] = keras_weights[idx].transpose()\n\n# Set weights\ntorch_states = torch_model.state_dict()\nfor key, weight in zip(torch_states.keys(), keras_weights):\n    torch_states[key] = torch.tensor(weight)\ntorch_model.load_state_dict(torch_states)\n\n# Save new model\ntorch.save(torch_model, \"lenet5-keras.pt\")\n```", "```py\nimport torch\nimport tensorflow as tf\n\n# Load the models\ntorch_model = torch.load(\"lenet5.pt\")\nkeras_model = tf.keras.models.load_model(\"lenet5.h5\")\n\n# Extract weights from PyTorch model\ntorch_states = torch_model.state_dict()\nweights = list(torch_states.values())\n\n# Transform tensor to numpy array\nweights = [w.numpy() for w in weights]\n\n# Transform shape from PyTorch to Keras\nfor idx in [0, 2, 4]:\n    # conv layers: (height, width, in, out)\n    weights[idx] = weights[idx].transpose([2, 3, 1, 0])\nfor idx in [6, 8]:\n    # dense layers: (in, out)\n    weights[idx] = weights[idx].transpose()\n\n# Set weights\nkeras_model.set_weights(weights)\n\n# Save new model\nkeras_model.save(\"lenet5-torch.h5\")\n```", "```py\nimport numpy as np\nimport torch\nimport tensorflow as tf\n\n# Load the models\ntorch_orig_model = torch.load(\"lenet5.pt\")\nkeras_orig_model = tf.keras.models.load_model(\"lenet5.h5\")\ntorch_converted_model = torch.load(\"lenet5-keras.pt\")\nkeras_converted_model = tf.keras.models.load_model(\"lenet5-torch.h5\")\n\n# Create a random input\nsample = np.random.random((28,28))\n\n# Convert sample to torch input shape\ntorch_sample = torch.Tensor(sample.reshape(1,1,28,28))\n\n# Convert sample to keras input shape\nkeras_sample = sample.reshape(1,28,28,1)\n\n# Check output\nkeras_converted_output = keras_converted_model.predict(keras_sample, verbose=0)\nkeras_orig_output = keras_orig_model.predict(keras_sample, verbose=0)\ntorch_converted_output = torch_converted_model(torch_sample).detach().numpy()\ntorch_orig_output = torch_orig_model(torch_sample).detach().numpy()\n\nnp.set_printoptions(precision=4)\nprint(keras_orig_output)\nprint(torch_converted_output)\nprint()\nprint(torch_orig_output)\nprint(keras_converted_output)\n```", "```py\n[[9.8908e-06 2.4246e-07 3.1996e-04 8.2742e-01 1.6853e-10 1.7212e-01\n  3.6018e-10 1.5521e-06 1.3128e-04 2.2083e-06]]\n[[9.8908e-06 2.4245e-07 3.1996e-04 8.2742e-01 1.6853e-10 1.7212e-01\n  3.6018e-10 1.5521e-06 1.3128e-04 2.2083e-06]]\n\n[[4.1505e-10 1.9959e-17 1.7399e-08 4.0302e-11 9.5790e-14 3.7395e-12\n  1.0634e-10 1.7682e-16 1.0000e+00 8.8126e-10]]\n[[4.1506e-10 1.9959e-17 1.7399e-08 4.0302e-11 9.5791e-14 3.7395e-12\n  1.0634e-10 1.7682e-16 1.0000e+00 8.8127e-10]]\n```"]