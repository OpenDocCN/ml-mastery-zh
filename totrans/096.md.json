["```py\nimport sklearn.datasets\n\ndata, target = sklearn.datasets.load_iris(return_X_y=True, as_frame=True)\ndata[\"target\"] = target\nprint(data)\n```", "```py\n     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  target\n0                  5.1               3.5                1.4               0.2       0\n1                  4.9               3.0                1.4               0.2       0\n2                  4.7               3.2                1.3               0.2       0\n3                  4.6               3.1                1.5               0.2       0\n4                  5.0               3.6                1.4               0.2       0\n..                 ...               ...                ...               ...     ...\n145                6.7               3.0                5.2               2.3       2\n146                6.3               2.5                5.0               1.9       2\n147                6.5               3.0                5.2               2.0       2\n148                6.2               3.4                5.4               2.3       2\n149                5.9               3.0                5.1               1.8       2\n\n[150 rows x 5 columns]\n```", "```py\nimport sklearn.datasets\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata, target = sklearn.datasets.load_iris(return_X_y=True, as_frame=True)\ndata[\"target\"] = target\n\nsns.pairplot(data, kind=\"scatter\", diag_kind=\"kde\", hue=\"target\",\n             palette=\"muted\", plot_kws={'alpha':0.7})\nplt.show()\n```", "```py\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata = sns.load_dataset(\"iris\")\nsns.pairplot(data, kind=\"scatter\", diag_kind=\"kde\", hue=\"species\",\n             palette=\"muted\", plot_kws={'alpha':0.7})\nplt.show()\n```", "```py\nimport seaborn as sns\nprint(sns.get_dataset_names())\n```", "```py\n['anagrams', 'anscombe', 'attention', 'brain_networks', 'car_crashes',\n'diamonds', 'dots', 'exercise', 'flights', 'fmri', 'gammas', 'geyser',\n'iris', 'mpg', 'penguins', 'planets', 'taxis', 'tips', 'titanic']\n```", "```py\nimport sklearn.datasets\n\ndata = sklearn.datasets.fetch_california_housing(return_X_y=False, as_frame=True)\ndata = data[\"frame\"]\nprint(data)\n```", "```py\n       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  Longitude  MedHouseVal\n0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88    -122.23        4.526\n1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86    -122.22        3.585\n2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85    -122.24        3.521\n3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85    -122.25        3.413\n4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85    -122.25        3.422\n...       ...       ...       ...        ...         ...       ...       ...        ...          ...\n20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48    -121.09        0.781\n20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49    -121.21        0.771\n20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43    -121.22        0.923\n20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43    -121.32        0.847\n20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37    -121.24        0.894\n\n[20640 rows x 9 columns]\n```", "```py\nimport sklearn.datasets\n\ndata = sklearn.datasets.fetch_openml(\"diabetes\", version=1, as_frame=True, return_X_y=False)\ndata = data[\"frame\"]\nprint(data)\n```", "```py\n     preg   plas  pres  skin   insu  mass   pedi   age            class\n0     6.0  148.0  72.0  35.0    0.0  33.6  0.627  50.0  tested_positive\n1     1.0   85.0  66.0  29.0    0.0  26.6  0.351  31.0  tested_negative\n2     8.0  183.0  64.0   0.0    0.0  23.3  0.672  32.0  tested_positive\n3     1.0   89.0  66.0  23.0   94.0  28.1  0.167  21.0  tested_negative\n4     0.0  137.0  40.0  35.0  168.0  43.1  2.288  33.0  tested_positive\n..    ...    ...   ...   ...    ...   ...    ...   ...              ...\n763  10.0  101.0  76.0  48.0  180.0  32.9  0.171  63.0  tested_negative\n764   2.0  122.0  70.0  27.0    0.0  36.8  0.340  27.0  tested_negative\n765   5.0  121.0  72.0  23.0  112.0  26.2  0.245  30.0  tested_negative\n766   1.0  126.0  60.0   0.0    0.0  30.1  0.349  47.0  tested_positive\n767   1.0   93.0  70.0  31.0    0.0  30.4  0.315  23.0  tested_negative\n\n[768 rows x 9 columns]\n```", "```py\nimport sklearn.datasets\n\ndata = sklearn.datasets.fetch_openml(data_id=42437, return_X_y=False, as_frame=True)\ndata = data[\"frame\"]\nprint(data)\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import fetch_openml\n\nX, y = fetch_openml(data_id=42437, return_X_y=True, as_frame=False)\nclf = LogisticRegression(random_state=0).fit(X, y)\nprint(clf.score(X,y)) # accuracy\nprint(clf.coef_)      # coefficient in logistic regression\n```", "```py\n0.8114478114478114\n[[-0.7551392   2.24013347 -0.20761281  0.28073571  0.24416706 -0.36699113\n   0.4782924 ]]\n```", "```py\npip install tensorflow-datasets\n```", "```py\nimport tensorflow_datasets as tfds\nprint(tfds.list_builders())\n```", "```py\nimport tensorflow_datasets as tfds\nds = tfds.load(\"mnist\", split=\"train\", shuffle_files=True)\nprint(ds)\n```", "```py\n<_OptionsDataset shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>\n```", "```py\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dense, AveragePooling2D, Dropout, Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Read data with train-test split\nds_train, ds_test = tfds.load(\"mnist\", split=['train', 'test'],\n                              shuffle_files=True, as_supervised=True)\n\n# Set up BatchDataset from the OptionsDataset object\nds_train = ds_train.batch(32)\nds_test = ds_test.batch(32)\n\n# Build LeNet5 model and fit\nmodel = Sequential([\n    Conv2D(6, (5,5), input_shape=(28,28,1), padding=\"same\", activation=\"tanh\"),\n    AveragePooling2D((2,2), strides=2),\n    Conv2D(16, (5,5), activation=\"tanh\"),\n    AveragePooling2D((2,2), strides=2),\n    Conv2D(120, (5,5), activation=\"tanh\"),\n    Flatten(),\n    Dense(84, activation=\"tanh\"),\n    Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"sparse_categorical_accuracy\"])\nearlystopping = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\nmodel.fit(ds_train, validation_data=ds_test, epochs=100, callbacks=[earlystopping])\n```", "```py\nfrom sklearn.datasets import make_circles\nimport matplotlib.pyplot as plt\n\ndata, target = make_circles(n_samples=500, shuffle=True, factor=0.7, noise=0.1)\nplt.figure(figsize=(6,6))\nplt.scatter(data[:,0], data[:,1], c=target, alpha=0.8, cmap=\"Set1\")\nplt.show()\n```", "```py\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\n\ndata, target = make_blobs(n_samples=500, n_features=3, centers=4,\n                          shuffle=True, random_state=42, cluster_std=2.5)\n\nfig = plt.figure(figsize=(8,8))\nax = fig.add_subplot(projection='3d')\nax.scatter(data[:,0], data[:,1], data[:,2], c=target, alpha=0.7, cmap=\"Set1\")\nplt.show()\n```", "```py\nfrom sklearn.datasets import make_s_curve, make_swiss_roll\nimport matplotlib.pyplot as plt\n\ndata, target = make_s_curve(n_samples=5000, random_state=42)\n\nfig = plt.figure(figsize=(15,8))\nax = fig.add_subplot(121, projection='3d')\nax.scatter(data[:,0], data[:,1], data[:,2], c=target, alpha=0.7, cmap=\"viridis\")\n\ndata, target = make_swiss_roll(n_samples=5000, random_state=42)\nax = fig.add_subplot(122, projection='3d')\nax.scatter(data[:,0], data[:,1], data[:,2], c=target, alpha=0.7, cmap=\"viridis\")\n\nplt.show()\n```", "```py\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# Generate 10-dimensional features and 1-dimensional targets\nX, y = make_regression(n_samples=500, n_features=10, n_targets=1, n_informative=4,\n                       noise=0.5, bias=-2.5, random_state=42)\n\n# Run linear regression on the data\nreg = LinearRegression()\nreg.fit(X, y)\n\n# Print the coefficient and intercept found\nwith np.printoptions(precision=5, linewidth=100, suppress=True):\n    print(np.array(reg.coef_))\n    print(reg.intercept_)\n```", "```py\n[-0.00435 -0.02232 19.0113   0.04391 46.04906 -0.02882 -0.05692 28.61786 -0.01839 16.79397]\n-2.5106367126731413\n```", "```py\nfrom sklearn.datasets import make_classification\nfrom sklearn.svm import SVC\nimport numpy as np\n\n# Generate 10-dimensional features and 3-class targets\nX, y = make_classification(n_samples=1000, n_features=10, n_classes=3,\n                           n_informative=4, n_redundant=2, n_repeated=1,\n                           random_state=42)\n\n# Run SVC on the data\nclf = SVC(kernel=\"rbf\")\nclf.fit(X, y)\n\n# Print the accuracy\nprint(clf.score(X, y))\n```"]