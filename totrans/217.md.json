["```py\nimport numpy as np\n\n# Find a small float to avoid division by zero\nepsilon = np.finfo(float).eps\n\n# Sigmoid function and its differentiation\ndef sigmoid(z):\n    return 1/(1+np.exp(-z.clip(-500, 500)))\ndef dsigmoid(z):\n    s = sigmoid(z)\n    return 2 * s * (1-s)\n\n# ReLU function and its differentiation\ndef relu(z):\n    return np.maximum(0, z)\ndef drelu(z):\n    return (z > 0).astype(float)\n```", "```py\n# Loss function L(y, yhat) and its differentiation\ndef cross_entropy(y, yhat):\n    \"\"\"Binary cross entropy function\n        L = - y log yhat - (1-y) log (1-yhat)\n\n    Args:\n        y, yhat (np.array): 1xn matrices which n are the number of data instances\n    Returns:\n        average cross entropy value of shape 1x1, averaging over the n instances\n    \"\"\"\n    return -(y.T @ np.log(yhat.clip(epsilon)) + (1-y.T) @ np.log((1-yhat).clip(epsilon))) / y.shape[1]\n\ndef d_cross_entropy(y, yhat):\n    \"\"\" dL/dyhat \"\"\"\n    return - np.divide(y, yhat.clip(epsilon)) + np.divide(1-y, (1-yhat).clip(epsilon))\n```", "```py\nclass mlp:\n    '''Multilayer perceptron using numpy\n    '''\n    def __init__(self, layersizes, activations, derivatives, lossderiv):\n        \"\"\"remember config, then initialize array to hold NN parameters without init\"\"\"\n        # hold NN config\n        self.layersizes = layersizes\n        self.activations = activations\n        self.derivatives = derivatives\n        self.lossderiv = lossderiv\n        # parameters, each is a 2D numpy array\n        L = len(self.layersizes)\n        self.z = [None] * L\n        self.W = [None] * L\n        self.b = [None] * L\n        self.a = [None] * L\n        self.dz = [None] * L\n        self.dW = [None] * L\n        self.db = [None] * L\n        self.da = [None] * L\n\n    def initialize(self, seed=42):\n        np.random.seed(seed)\n        sigma = 0.1\n        for l, (insize, outsize) in enumerate(zip(self.layersizes, self.layersizes[1:]), 1):\n            self.W[l] = np.random.randn(insize, outsize) * sigma\n            self.b[l] = np.random.randn(1, outsize) * sigma\n\n    def forward(self, x):\n        self.a[0] = x\n        for l, func in enumerate(self.activations, 1):\n            # z = W a + b, with `a` as output from previous layer\n            # `W` is of size rxs and `a` the size sxn with n the number of data instances, `z` the size rxn\n            # `b` is rx1 and broadcast to each column of `z`\n            self.z[l] = (self.a[l-1] @ self.W[l]) + self.b[l]\n            # a = g(z), with `a` as output of this layer, of size rxn\n            self.a[l] = func(self.z[l])\n        return self.a[-1]\n```", "```py\nclass mlp:\n    ...\n\n    def backward(self, y, yhat):\n        # first `da`, at the output\n        self.da[-1] = self.lossderiv(y, yhat)\n        for l, func in reversed(list(enumerate(self.derivatives, 1))):\n            # compute the differentials at this layer\n            self.dz[l] = self.da[l] * func(self.z[l])\n            self.dW[l] = self.a[l-1].T @ self.dz[l]\n            self.db[l] = np.mean(self.dz[l], axis=0, keepdims=True)\n            self.da[l-1] = self.dz[l] @ self.W[l].T\n\n    def update(self, eta):\n        for l in range(1, len(self.W)):\n            self.W[l] -= eta * self.dW[l]\n            self.b[l] -= eta * self.db[l]\n```", "```py\nfrom sklearn.datasets import make_circles\nfrom sklearn.metrics import accuracy_score\n\n# Make data: Two circles on x-y plane as a classification problem\nX, y = make_circles(n_samples=1000, factor=0.5, noise=0.1)\ny = y.reshape(-1,1) # our model expects a 2D array of (n_sample, n_dim)\n```", "```py\n# Build a model\nmodel = mlp(layersizes=[2, 4, 3, 1],\n            activations=[relu, relu, sigmoid],\n            derivatives=[drelu, drelu, dsigmoid],\n            lossderiv=d_cross_entropy)\nmodel.initialize()\nyhat = model.forward(X)\nloss = cross_entropy(y, yhat)\nprint(\"Before training - loss value {} accuracy {}\".format(loss, accuracy_score(y, (yhat > 0.5))))\n```", "```py\nBefore training - loss value [[693.62972747]] accuracy 0.5\n```", "```py\n# train for each epoch\nn_epochs = 150\nlearning_rate = 0.005\nfor n in range(n_epochs):\n    model.forward(X)\n    yhat = model.a[-1]\n    model.backward(y, yhat)\n    model.update(learning_rate)\n    loss = cross_entropy(y, yhat)\n    print(\"Iteration {} - loss value {} accuracy {}\".format(n, loss, accuracy_score(y, (yhat > 0.5))))\n```", "```py\nIteration 0 - loss value [[693.62972747]] accuracy 0.5\nIteration 1 - loss value [[693.62166655]] accuracy 0.5\nIteration 2 - loss value [[693.61534159]] accuracy 0.5\nIteration 3 - loss value [[693.60994018]] accuracy 0.5\n...\nIteration 145 - loss value [[664.60120828]] accuracy 0.818\nIteration 146 - loss value [[697.97739669]] accuracy 0.58\nIteration 147 - loss value [[681.08653776]] accuracy 0.642\nIteration 148 - loss value [[665.06165774]] accuracy 0.71\nIteration 149 - loss value [[683.6170298]] accuracy 0.614\n```", "```py\nfrom sklearn.datasets import make_circles\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nnp.random.seed(0)\n\n# Find a small float to avoid division by zero\nepsilon = np.finfo(float).eps\n\n# Sigmoid function and its differentiation\ndef sigmoid(z):\n    return 1/(1+np.exp(-z.clip(-500, 500)))\ndef dsigmoid(z):\n    s = sigmoid(z)\n    return 2 * s * (1-s)\n\n# ReLU function and its differentiation\ndef relu(z):\n    return np.maximum(0, z)\ndef drelu(z):\n    return (z > 0).astype(float)\n\n# Loss function L(y, yhat) and its differentiation\ndef cross_entropy(y, yhat):\n    \"\"\"Binary cross entropy function\n        L = - y log yhat - (1-y) log (1-yhat)\n\n    Args:\n        y, yhat (np.array): nx1 matrices which n are the number of data instances\n    Returns:\n        average cross entropy value of shape 1x1, averaging over the n instances\n    \"\"\"\n    return -(y.T @ np.log(yhat.clip(epsilon)) + (1-y.T) @ np.log((1-yhat).clip(epsilon))) / y.shape[1]\n\ndef d_cross_entropy(y, yhat):\n    \"\"\" dL/dyhat \"\"\"\n    return - np.divide(y, yhat.clip(epsilon)) + np.divide(1-y, (1-yhat).clip(epsilon))\n\nclass mlp:\n    '''Multilayer perceptron using numpy\n    '''\n    def __init__(self, layersizes, activations, derivatives, lossderiv):\n        \"\"\"remember config, then initialize array to hold NN parameters without init\"\"\"\n        # hold NN config\n        self.layersizes = tuple(layersizes)\n        self.activations = tuple(activations)\n        self.derivatives = tuple(derivatives)\n        self.lossderiv = lossderiv\n        assert len(self.layersizes)-1 == len(self.activations), \\\n            \"number of layers and the number of activation functions does not match\"\n        assert len(self.activations) == len(self.derivatives), \\\n            \"number of activation functions and number of derivatives does not match\"\n        assert all(isinstance(n, int) and n >= 1 for n in layersizes), \\\n            \"Only positive integral number of perceptons is allowed in each layer\"\n        # parameters, each is a 2D numpy array\n        L = len(self.layersizes)\n        self.z = [None] * L\n        self.W = [None] * L\n        self.b = [None] * L\n        self.a = [None] * L\n        self.dz = [None] * L\n        self.dW = [None] * L\n        self.db = [None] * L\n        self.da = [None] * L\n\n    def initialize(self, seed=42):\n        \"\"\"initialize the value of weight matrices and bias vectors with small random numbers.\"\"\"\n        np.random.seed(seed)\n        sigma = 0.1\n        for l, (insize, outsize) in enumerate(zip(self.layersizes, self.layersizes[1:]), 1):\n            self.W[l] = np.random.randn(insize, outsize) * sigma\n            self.b[l] = np.random.randn(1, outsize) * sigma\n\n    def forward(self, x):\n        \"\"\"Feed forward using existing `W` and `b`, and overwrite the result variables `a` and `z`\n\n        Args:\n            x (numpy.ndarray): Input data to feed forward\n        \"\"\"\n        self.a[0] = x\n        for l, func in enumerate(self.activations, 1):\n            # z = W a + b, with `a` as output from previous layer\n            # `W` is of size rxs and `a` the size sxn with n the number of data instances, `z` the size rxn\n            # `b` is rx1 and broadcast to each column of `z`\n            self.z[l] = (self.a[l-1] @ self.W[l]) + self.b[l]\n            # a = g(z), with `a` as output of this layer, of size rxn\n            self.a[l] = func(self.z[l])\n        return self.a[-1]\n\n    def backward(self, y, yhat):\n        \"\"\"back propagation using NN output yhat and the reference output y, generates dW, dz, db,\n        da\n        \"\"\"\n        assert y.shape[1] == self.layersizes[-1], \"Output size doesn't match network output size\"\n        assert y.shape == yhat.shape, \"Output size doesn't match reference\"\n        # first `da`, at the output\n        self.da[-1] = self.lossderiv(y, yhat)\n        for l, func in reversed(list(enumerate(self.derivatives, 1))):\n            # compute the differentials at this layer\n            self.dz[l] = self.da[l] * func(self.z[l])\n            self.dW[l] = self.a[l-1].T @ self.dz[l]\n            self.db[l] = np.mean(self.dz[l], axis=0, keepdims=True)\n            self.da[l-1] = self.dz[l] @ self.W[l].T\n            assert self.z[l].shape == self.dz[l].shape\n            assert self.W[l].shape == self.dW[l].shape\n            assert self.b[l].shape == self.db[l].shape\n            assert self.a[l].shape == self.da[l].shape\n\n    def update(self, eta):\n        \"\"\"Updates W and b\n\n        Args:\n            eta (float): Learning rate\n        \"\"\"\n        for l in range(1, len(self.W)):\n            self.W[l] -= eta * self.dW[l]\n            self.b[l] -= eta * self.db[l]\n\n# Make data: Two circles on x-y plane as a classification problem\nX, y = make_circles(n_samples=1000, factor=0.5, noise=0.1)\ny = y.reshape(-1,1) # our model expects a 2D array of (n_sample, n_dim)\nprint(X.shape)\nprint(y.shape)\n\n# Build a model\nmodel = mlp(layersizes=[2, 4, 3, 1],\n            activations=[relu, relu, sigmoid],\n            derivatives=[drelu, drelu, dsigmoid],\n            lossderiv=d_cross_entropy)\nmodel.initialize()\nyhat = model.forward(X)\nloss = cross_entropy(y, yhat)\nprint(\"Before training - loss value {} accuracy {}\".format(loss, accuracy_score(y, (yhat > 0.5))))\n\n# train for each epoch\nn_epochs = 150\nlearning_rate = 0.005\nfor n in range(n_epochs):\n    model.forward(X)\n    yhat = model.a[-1]\n    model.backward(y, yhat)\n    model.update(learning_rate)\n    loss = cross_entropy(y, yhat)\n    print(\"Iteration {} - loss value {} accuracy {}\".format(n, loss, accuracy_score(y, (yhat > 0.5))))\n```"]