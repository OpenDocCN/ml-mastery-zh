["```py\nimport torch\nimport matplotlib.pyplot as plt\n\n# generate synthetic the data\nX = torch.arange(-30, 30, 1).view(-1, 1).type(torch.FloatTensor)\nY = torch.zeros(X.shape[0])\nY[(X[:, 0] <= -10)] = 1.0\nY[(X[:, 0] > -10) & (X[:, 0] < 10)] = 0.5\nY[(X[:, 0] > 10)] = 0\n```", "```py\n...\nplt.plot(X, Y)\nplt.show()\n```", "```py\n...\n\n# Define the class for single layer NN\nclass one_layer_net(torch.nn.Module):    \n    # Constructor\n    def __init__(self, input_size, hidden_neurons, output_size):\n        super(one_layer_net, self).__init__()\n        # hidden layer \n        self.linear_one = torch.nn.Linear(input_size, hidden_neurons)\n        self.linear_two = torch.nn.Linear(hidden_neurons, output_size) \n        # defining layers as attributes\n        self.layer_in = None\n        self.act = None\n        self.layer_out = None\n    # prediction function\n    def forward(self, x):\n        self.layer_in = self.linear_one(x)\n        self.act = torch.sigmoid(self.layer_in)\n        self.layer_out = self.linear_two(self.act)\n        y_pred = torch.sigmoid(self.linear_two(self.act))\n        return y_pred\n```", "```py\n# create the model \nmodel = one_layer_net(1, 2, 1)  # 2 represents two neurons in one hidden layer\n```", "```py\ndef criterion(y_pred, y):\n    out = -1 * torch.mean(y * torch.log(y_pred) + (1 - y) * torch.log(1 - y_pred))\n    return out\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n```", "```py\n# Define the training loop\nepochs=5000\ncost = []\ntotal=0\nfor epoch in range(epochs):\n    total=0\n    epoch = epoch + 1\n    for x, y in zip(X, Y):\n        yhat = model(x)\n        loss = criterion(yhat, y)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        # get total loss \n        total+=loss.item() \n    cost.append(total)\n    if epoch % 1000 == 0:\n        print(str(epoch)+ \" \" + \"epochs done!\") # visualze results after every 1000 epochs   \n        # plot the result of function approximator\n        plt.plot(X.numpy(), model(X).detach().numpy())\n        plt.plot(X.numpy(), Y.numpy(), 'm')\n        plt.xlabel('x')\n        plt.show()\n```", "```py\n# plot the cost\nplt.plot(cost)\nplt.xlabel('epochs')\nplt.title('cross entropy loss')\nplt.show()\n```", "```py\nimport torch\nimport matplotlib.pyplot as plt\n\n# generate synthetic the data\nX = torch.arange(-30, 30, 1).view(-1, 1).type(torch.FloatTensor)\nY = torch.zeros(X.shape[0])\nY[(X[:, 0] <= -10)] = 1.0\nY[(X[:, 0] > -10) & (X[:, 0] < 10)] = 0.5\nY[(X[:, 0] > 10)] = 0\n\nplt.plot(X, Y)\nplt.show()\n\n# Define the class for single layer NN\nclass one_layer_net(torch.nn.Module):    \n    # Constructor\n    def __init__(self, input_size, hidden_neurons, output_size):\n        super(one_layer_net, self).__init__()\n        # hidden layer \n        self.linear_one = torch.nn.Linear(input_size, hidden_neurons)\n        self.linear_two = torch.nn.Linear(hidden_neurons, output_size) \n        # defining layers as attributes\n        self.layer_in = None\n        self.act = None\n        self.layer_out = None\n    # prediction function\n    def forward(self, x):\n        self.layer_in = self.linear_one(x)\n        self.act = torch.sigmoid(self.layer_in)\n        self.layer_out = self.linear_two(self.act)\n        y_pred = torch.sigmoid(self.linear_two(self.act))\n        return y_pred\n\n# create the model \nmodel = one_layer_net(1, 2, 1)  # 2 represents two neurons in one hidden layer\n\ndef criterion(y_pred, y):\n    out = -1 * torch.mean(y * torch.log(y_pred) + (1 - y) * torch.log(1 - y_pred))\n    return out\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Define the training loop\nepochs=5000\ncost = []\ntotal=0\nfor epoch in range(epochs):\n    total=0\n    epoch = epoch + 1\n    for x, y in zip(X, Y):\n        yhat = model(x)\n        loss = criterion(yhat, y)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        # get total loss \n        total+=loss.item() \n    cost.append(total)\n    if epoch % 1000 == 0:\n        print(str(epoch)+ \" \" + \"epochs done!\") # visualze results after every 1000 epochs   \n        # plot the result of function approximator\n        plt.plot(X.numpy(), model(X).detach().numpy())\n        plt.plot(X.numpy(), Y.numpy(), 'm')\n        plt.xlabel('x')\n        plt.show()\n\n# plot the cost\nplt.plot(cost)\nplt.xlabel('epochs')\nplt.title('cross entropy loss')\nplt.show()\n```"]