- en: A Gentle Introduction to Prompt Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://machinelearningmastery.com/a-gentle-introduction-to-prompt-engineering/](https://machinelearningmastery.com/a-gentle-introduction-to-prompt-engineering/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ChatGPT is a service provided by OpenAI that is a conversational large language
    model. It is widespread, and it is found to be very useful. Behind the scene,
    it is a large language model. Unlike the other LLMs that generate continuing text
    from the leading sentence you provided, ChatGPT enables you to ask questions or
    provide instructions, and the model will respond like a conversation. To make
    ChatGPT respond correctly, you must interact with the model. This technique is
    called prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, you will understand ChatGPT as an LLM and learn about prompt engineering.
    In particular,
  prefs: []
  type: TYPE_NORMAL
- en: What is the input context to LLM in ChatGPT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How ChatGPT interacts with the input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to provide an appropriate prompt to get the result you desired
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Get started and apply ChatGPT** with my book [Maximizing Productivity with
    ChatGPT](https://machinelearningmastery.com/productivity-with-chatgpt/). It provides
    **real-world use cases** and **prompt examples** designed to get you using ChatGPT
    quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.![](../Images/2e078534971f7abc10aac092ee5fb660.png)
  prefs: []
  type: TYPE_NORMAL
- en: A Gentle Introduction to Prompt Engineering
  prefs: []
  type: TYPE_NORMAL
- en: Picture generated by the author using Stable Diffusion. Some rights reserved.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This article is divided into three parts; they are:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding ChatGPT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engineering the Context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advices for Prompt Engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding ChatGPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ChatGPT is a conversational large language model. A language model can generate
    words given the leading text. A conversational large language model is a natural
    variation. If you have read the drama play, such as the following example written
    by Shakespeare, you should notice a conversation is a dialog between multiple
    individuals:'
  prefs: []
  type: TYPE_NORMAL
- en: Abr. Do you bite your thumb at us, sir?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sam. I do bite my thumb, sir.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Abr. Do you bite your thumb at us, sir?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sam. Is the law of our side, if I say—ay?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Gre. No.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sam. No, sir, I do not bite my thumb at you, sir; but I bite my thumb, sir.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you input the first four lines of a conversation into a language model, it
    is reasonable to expect that it will generate the fifth line. As the model has
    learned from a vast amount of text, the format of a play is just a style that
    it understands. Because the model can understand context, its words should flow
    naturally with the preceding text, as if it were a proper response in a chat.
  prefs: []
  type: TYPE_NORMAL
- en: Engineering the Context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When using LLMs to generate text, the context plays a crucial role in determining
    the output. For ChatGPT, the context is derived from previous conversations. To
    ensure that ChatGPT responds in a desired manner, it is essential to carefully
    structure the input to provide the necessary cues.
  prefs: []
  type: TYPE_NORMAL
- en: 'While ChatGPT is a robust language model, it does have its limitations. Although
    it has acquired some basic “common sense” from its training data, it may not be
    capable of demonstrating detailed logical reasoning. For instance, if you ask
    ChatGPT to “Provide information on machine learning,” it may respond with a lengthy
    but not necessarily top-quality answer. However, if you ask, “Tell me the pros
    and cons of using machine learning to solve image classification problems,” you
    are more likely to receive a superior outcome because:'
  prefs: []
  type: TYPE_NORMAL
- en: You gave a specific scope, i.e., the image classification problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You requested a specific format of the response, i.e., pros and cons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Similar cases are when you ask a complicated question, such as a math word
    problem like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: To prepare for Easter, Mrs. Smith went to a store and bought 10 toy bunnies
    for \$5 each. Chocolate egg is \$1 cheaper than bunnies, and Mrs Smith bought
    15\. How much did Mrs. Smith spend?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A newer version of ChatGPT can probably do it right, but questions of this
    type are easily error-prone if the large language model attempts to give you an
    answer immediately. The reason is that when you try to generate a short answer,
    the context needs more clues to keep the logic right. However, it is easy to do
    the math right when the LLM tries to lay out the logic in steps. Therefore, someone
    suggested you ask ChatGPT not “How much did Mrs. Smith spend” but “Explain how
    much did Mrs. Smith spend with steps in detail.” These are the clues to force
    ChatGPT to reason with logic. For example, the following is what you may observe
    as an answer:'
  prefs: []
  type: TYPE_NORMAL
- en: To calculate how much Mrs. Smith spent, we need to determine the cost of the
    toy bunnies and the chocolate eggs separately and then add them together.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mrs. Smith bought 10 toy bunnies at \$5 each, so the total cost of the bunnies
    is 10 * \$5 = \$<<10*5=50>>50.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Each chocolate egg is \$1 cheaper than the toy bunnies, so the cost of each
    chocolate egg is \$5 – \$1 = \$<<5-1=4>>4.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mrs. Smith bought 15 chocolate eggs, so the total cost of the chocolate eggs
    is 15 * \$4 = \$<<15*4=60>>60.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Finally, to find out the total amount Mrs. Smith spent, we add the cost of
    the bunnies and the cost of the chocolate eggs: \$50 + \$60 = \$<<50+60=110>>110.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Therefore, Mrs. Smith spent \$110 in total.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can see how an LLM can give you the correct answer. LLM cannot do mental
    math and has limited logic capability (since it is trained with text, it learns
    as much as a single sentence can express). Should there be more complicated logic,
    you must ensure the model goes in small steps. It is called the **chain of thoughts**.
  prefs: []
  type: TYPE_NORMAL
- en: Advices for Prompt Engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Below are some ideas to craft the context to make ChatGPT or other LLMs produce
    the useful output:'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than the model on the loose, you should set up the scenario and scopes
    in the prompt by providing details of what, where, when, why, who, and how
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assigning a persona in the prompt, for example, “As a computer science professor,
    explain what is machine learning” rather than merely “Explain what machine learning
    is,” can make the response more academic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can control the output style by requesting “explain to a 5-year-old”, “explain
    with an analogy,” “make a convincing statement,” or “in 3 to 5 points.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To encourage the model to respond with a chain of thoughts, end your request
    with “solve this in steps.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can provide additional information to the model by saying, “Reference to
    the following information,” followed by the material you want the model to work
    on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because the previous conversation constructs the context, beginning the prompt
    with “ignore all previous instructions before this one” can make the model start
    from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making the prompt straightforward and easy to understand is essential since
    the context deduced can be more accurate to reflect your intention.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this post, you learned how prompts drive the output from an LLM, particularly
    ChatGPT. Specifically, you learned.
  prefs: []
  type: TYPE_NORMAL
- en: How a prompt set up a context so that the model can generate output within the
    context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are limited; you need to provide correct guidance in the prompt to produce
    accurate output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing specific, detailed prompts can help get correct output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
