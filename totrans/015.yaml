- en: A Gentle Introduction to Prompt Engineering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逐步介绍提示工程
- en: 原文：[https://machinelearningmastery.com/a-gentle-introduction-to-prompt-engineering/](https://machinelearningmastery.com/a-gentle-introduction-to-prompt-engineering/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/a-gentle-introduction-to-prompt-engineering/](https://machinelearningmastery.com/a-gentle-introduction-to-prompt-engineering/)
- en: ChatGPT is a service provided by OpenAI that is a conversational large language
    model. It is widespread, and it is found to be very useful. Behind the scene,
    it is a large language model. Unlike the other LLMs that generate continuing text
    from the leading sentence you provided, ChatGPT enables you to ask questions or
    provide instructions, and the model will respond like a conversation. To make
    ChatGPT respond correctly, you must interact with the model. This technique is
    called prompt engineering.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 是 OpenAI 提供的服务，是一个对话型的大型语言模型。它非常普及，并且被发现非常有用。在幕后，它是一个大型语言模型。不同于其他从你提供的引导句生成连续文本的
    LLM，ChatGPT 允许你提问或提供指令，模型将以对话形式回应。为了使 ChatGPT 正确回应，你必须与模型互动。这种技术称为提示工程。
- en: In this post, you will understand ChatGPT as an LLM and learn about prompt engineering.
    In particular,
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，你将了解 ChatGPT 作为 LLM 的特点，并学习提示工程。特别是，
- en: What is the input context to LLM in ChatGPT
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ChatGPT 中 LLM 的输入上下文是什么
- en: How ChatGPT interacts with the input
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ChatGPT 如何与输入互动
- en: How to provide an appropriate prompt to get the result you desired
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何提供合适的提示以获得所需的结果
- en: '**Get started and apply ChatGPT** with my book [Maximizing Productivity with
    ChatGPT](https://machinelearningmastery.com/productivity-with-chatgpt/). It provides
    **real-world use cases** and **prompt examples** designed to get you using ChatGPT
    quickly.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**开始使用 ChatGPT**，可以参考我的书籍 [《利用 ChatGPT 提高生产力》](https://machinelearningmastery.com/productivity-with-chatgpt/)。这本书提供了**实际应用案例**和**提示示例**，旨在帮助您迅速上手
    ChatGPT。'
- en: Let’s get started.![](../Images/2e078534971f7abc10aac092ee5fb660.png)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。 ![](../Images/2e078534971f7abc10aac092ee5fb660.png)
- en: A Gentle Introduction to Prompt Engineering
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步介绍提示工程
- en: Picture generated by the author using Stable Diffusion. Some rights reserved.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用 Stable Diffusion 生成。保留部分权利。
- en: Overview
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'This article is divided into three parts; they are:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文分为三部分：
- en: Understanding ChatGPT
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 ChatGPT
- en: Engineering the Context
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文工程
- en: Advices for Prompt Engineering
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程建议
- en: Understanding ChatGPT
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 ChatGPT
- en: 'ChatGPT is a conversational large language model. A language model can generate
    words given the leading text. A conversational large language model is a natural
    variation. If you have read the drama play, such as the following example written
    by Shakespeare, you should notice a conversation is a dialog between multiple
    individuals:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 是一个对话型的大型语言模型。语言模型可以根据引导文本生成词汇。对话型的大型语言模型是一种自然变体。如果你读过戏剧，比如莎士比亚所写的例子，你会注意到对话是多个人之间的对话：
- en: Abr. Do you bite your thumb at us, sir?
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Abr. 你对我们挑衅吗，先生？
- en: ''
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sam. I do bite my thumb, sir.
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Sam. 我咬自己的拇指，先生。
- en: ''
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Abr. Do you bite your thumb at us, sir?
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Abr. 你对我们挑衅吗，先生？
- en: ''
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sam. Is the law of our side, if I say—ay?
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Sam. 如果我说——是吗？法律在我们这边吗？
- en: ''
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Gre. No.
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Gre. 不。
- en: ''
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sam. No, sir, I do not bite my thumb at you, sir; but I bite my thumb, sir.
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Sam. 不，先生，我并没有对您挑衅；我只是咬自己的拇指，先生。
- en: If you input the first four lines of a conversation into a language model, it
    is reasonable to expect that it will generate the fifth line. As the model has
    learned from a vast amount of text, the format of a play is just a style that
    it understands. Because the model can understand context, its words should flow
    naturally with the preceding text, as if it were a proper response in a chat.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将对话的前四行输入到语言模型中，可以合理地期待它会生成第五行。由于模型从大量文本中学习，剧本的格式只是它理解的一种风格。由于模型能够理解上下文，它的回答应该与前文自然衔接，就像聊天中的恰当回应一样。
- en: Engineering the Context
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文工程
- en: When using LLMs to generate text, the context plays a crucial role in determining
    the output. For ChatGPT, the context is derived from previous conversations. To
    ensure that ChatGPT responds in a desired manner, it is essential to carefully
    structure the input to provide the necessary cues.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 LLM 生成文本时，上下文在确定输出中起着至关重要的作用。对于 ChatGPT，上下文来自于之前的对话。为了确保 ChatGPT 以期望的方式回应，必须仔细构建输入以提供必要的提示。
- en: 'While ChatGPT is a robust language model, it does have its limitations. Although
    it has acquired some basic “common sense” from its training data, it may not be
    capable of demonstrating detailed logical reasoning. For instance, if you ask
    ChatGPT to “Provide information on machine learning,” it may respond with a lengthy
    but not necessarily top-quality answer. However, if you ask, “Tell me the pros
    and cons of using machine learning to solve image classification problems,” you
    are more likely to receive a superior outcome because:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然ChatGPT是一个强大的语言模型，但它确实有其局限性。尽管它从训练数据中获得了一些基本的“常识”，但可能无法展示详细的逻辑推理。例如，如果你问ChatGPT“提供有关机器学习的信息”，它可能会回答很长，但不一定质量很高。不过，如果你问，“告诉我使用机器学习解决图像分类问题的优缺点”，你更有可能获得优质的结果，因为：
- en: You gave a specific scope, i.e., the image classification problem
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你给出了一个具体的范围，即图像分类问题。
- en: You requested a specific format of the response, i.e., pros and cons
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你请求了特定格式的回答，即优缺点。
- en: 'Similar cases are when you ask a complicated question, such as a math word
    problem like the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的情况是你问一个复杂的问题，例如以下数学应用题：
- en: To prepare for Easter, Mrs. Smith went to a store and bought 10 toy bunnies
    for \$5 each. Chocolate egg is \$1 cheaper than bunnies, and Mrs Smith bought
    15\. How much did Mrs. Smith spend?
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了准备复活节，史密斯太太去了商店，买了10只每只5美元的玩具兔子。巧克力蛋比兔子便宜1美元，史密斯太太买了15个。史密斯太太花了多少钱？
- en: 'A newer version of ChatGPT can probably do it right, but questions of this
    type are easily error-prone if the large language model attempts to give you an
    answer immediately. The reason is that when you try to generate a short answer,
    the context needs more clues to keep the logic right. However, it is easy to do
    the math right when the LLM tries to lay out the logic in steps. Therefore, someone
    suggested you ask ChatGPT not “How much did Mrs. Smith spend” but “Explain how
    much did Mrs. Smith spend with steps in detail.” These are the clues to force
    ChatGPT to reason with logic. For example, the following is what you may observe
    as an answer:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 更新版的ChatGPT可能能做对，但这类问题如果大型语言模型立即尝试给出答案，容易出错。原因是当你尝试生成简短答案时，上下文需要更多线索以保持逻辑正确。然而，当LLM尝试逐步展示逻辑时，很容易算对。因此，有人建议你不要问ChatGPT“史密斯太太花了多少钱”，而是“详细解释史密斯太太花了多少钱的步骤”。这些是强迫ChatGPT进行逻辑推理的线索。例如，以下是你可能观察到的回答：
- en: To calculate how much Mrs. Smith spent, we need to determine the cost of the
    toy bunnies and the chocolate eggs separately and then add them together.
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 要计算史密斯太太花了多少钱，我们需要分别确定玩具兔子和巧克力蛋的费用，然后将它们加在一起。
- en: ''
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mrs. Smith bought 10 toy bunnies at \$5 each, so the total cost of the bunnies
    is 10 * \$5 = \$<<10*5=50>>50.
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 史密斯太太以每只5美元的价格买了10只玩具兔子，所以兔子的总费用是10 * 5美元 = 50美元。
- en: ''
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Each chocolate egg is \$1 cheaper than the toy bunnies, so the cost of each
    chocolate egg is \$5 – \$1 = \$<<5-1=4>>4.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 每个巧克力蛋比玩具兔子便宜1美元，因此每个巧克力蛋的费用是5美元 - 1美元 = 4美元。
- en: ''
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mrs. Smith bought 15 chocolate eggs, so the total cost of the chocolate eggs
    is 15 * \$4 = \$<<15*4=60>>60.
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 史密斯太太买了15个巧克力蛋，所以巧克力蛋的总费用是15 * 4美元 = 60美元。
- en: ''
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Finally, to find out the total amount Mrs. Smith spent, we add the cost of
    the bunnies and the cost of the chocolate eggs: \$50 + \$60 = \$<<50+60=110>>110.'
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 最后，为了找出史密斯太太花费的总金额，我们将兔子的费用和巧克力蛋的费用相加：50美元 + 60美元 = 110美元。
- en: ''
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Therefore, Mrs. Smith spent \$110 in total.
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因此，史密斯太太总共花了110美元。
- en: You can see how an LLM can give you the correct answer. LLM cannot do mental
    math and has limited logic capability (since it is trained with text, it learns
    as much as a single sentence can express). Should there be more complicated logic,
    you must ensure the model goes in small steps. It is called the **chain of thoughts**.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到LLM如何给出正确的答案。LLM不能进行心算，逻辑能力有限（因为它是通过文本训练的，它学习的内容只限于单句话所表达的）。如果有更复杂的逻辑，你必须确保模型逐步进行。这称为**思维链**。
- en: Advices for Prompt Engineering
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示工程的建议
- en: 'Below are some ideas to craft the context to make ChatGPT or other LLMs produce
    the useful output:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些构建上下文的想法，以使ChatGPT或其他LLM产生有用的输出：
- en: Rather than the model on the loose, you should set up the scenario and scopes
    in the prompt by providing details of what, where, when, why, who, and how
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其让模型随意发挥，不如通过提供详细的什么、在哪里、何时、为什么、谁以及如何来设置提示中的场景和范围。
- en: Assigning a persona in the prompt, for example, “As a computer science professor,
    explain what is machine learning” rather than merely “Explain what machine learning
    is,” can make the response more academic.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提示中分配一个角色，例如，“作为一名计算机科学教授，解释什么是机器学习”，而不是仅仅“解释什么是机器学习”，可以使回答更具学术性。
- en: You can control the output style by requesting “explain to a 5-year-old”, “explain
    with an analogy,” “make a convincing statement,” or “in 3 to 5 points.”
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过请求“向5岁孩子解释”、“用类比解释”、“做出令人信服的陈述”或“用3到5点说明”来控制输出风格。
- en: To encourage the model to respond with a chain of thoughts, end your request
    with “solve this in steps.”
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了鼓励模型以思维链的方式回应，请在请求的结尾加上“分步骤解决此问题”。
- en: You can provide additional information to the model by saying, “Reference to
    the following information,” followed by the material you want the model to work
    on
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过说“参考以下信息”，然后提供你希望模型处理的材料，来向模型提供额外的信息。
- en: Because the previous conversation constructs the context, beginning the prompt
    with “ignore all previous instructions before this one” can make the model start
    from scratch
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为之前的对话构建了上下文，所以以“忽略所有之前的指令”开头的提示可以使模型从头开始。
- en: Making the prompt straightforward and easy to understand is essential since
    the context deduced can be more accurate to reflect your intention.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使提示直接明了是至关重要的，因为这样推断出的上下文可以更准确地反映你的意图。
- en: Summary
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this post, you learned how prompts drive the output from an LLM, particularly
    ChatGPT. Specifically, you learned.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，你了解了提示如何驱动大型语言模型，特别是ChatGPT的输出。具体来说，你学到了。
- en: How a prompt set up a context so that the model can generate output within the
    context
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示如何设置上下文，以便模型可以在该上下文内生成输出。
- en: LLMs are limited; you need to provide correct guidance in the prompt to produce
    accurate output.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）存在局限；你需要在提示中提供正确的指导，以生成准确的输出。
- en: Providing specific, detailed prompts can help get correct output
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供具体、详细的提示可以帮助获得正确的输出。
