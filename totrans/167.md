# 稳定扩散室内设计（8天迷你课程）

> 原文：[https://machinelearningmastery.com/interior-design-with-stable-diffusion-7-day-mini-course/](https://machinelearningmastery.com/interior-design-with-stable-diffusion-7-day-mini-course/)

在核心部分，稳定扩散是一个能够生成图片的深度学习模型。结合其他一些模型和用户界面，你可以把它看作是一个工具，帮助你在一个新的维度上创作图片。不仅可以提供图片外观的指令，还可以让生成模型来构思你未明确指定的内容。

在这个七部分的速成课程中，你将通过示例学习如何利用稳定扩散完成一个绘画项目。这个迷你课程专注于使用生成AI模型，而不是它们的内部机制。因此，你不必担心它们如何提供如此惊人的结果的复杂理论。然而，因为没有一步能完成所有，你应该期待学习多个扩展和参数如何协同作用来完成一个图像生成项目。让我们开始吧。

![](../Images/4b6e59f01a7d37ff105cdda96cc51853.png)

稳定扩散室内设计（7天迷你课程）

照片由 [Arno Smit](https://unsplash.com/photos/opened-door-of-house-iI72r3gSwWY) 提供。部分权利保留。

## 这个迷你课程适合谁？

在我们开始之前，让我们确保你来对了地方。以下列表提供了关于这门课程设计对象的一些一般指导方针。如果你的情况不完全符合，不要惊慌；你可能只需要在某些领域稍作补充，就可以跟上了。

+   **你知道什么是生成模型**。你不期望有魔法。你所看到的一切都是一些复杂算法的结果。因此，所有的结果都可以解释，并且一旦你了解了内部原理，这些步骤都是可重用的。

+   **你不是艺术家**。你不是在数字画布上作画。事实上，你是在没有画笔的情况下创作画作。生成模型不允许你控制太多细节，但你可以给出一些高级指令。这意味着你不应该期望能够精确控制输出。而且，你也不需要学习绘画技巧来创作一幅好画。

+   **你有耐心完成一个项目**。就像用画笔创作一幅画一样，你需要耐心，完成一个项目需要时间。不像绘画，你花的时间是在实验生成管道中的不同旋钮。根据项目的性质，你需要检查哪些是最佳参数以获得最佳结果。

这个迷你课程不是关于稳定扩散的教科书。但你会看到许多组件如何工作，以及它们如何帮助图像生成的最终结果。关键在于了解每个组件和参数的作用，这样你就可以决定如何在你的下一个项目中使用它们。

## 迷你课程概述

这个迷你课程分为八个部分。

每节课的设计目标是花费大约30分钟。您可能会更快地完成一些课程，而在其他课程中，您可能会选择更深入，花更多时间。

您可以根据自己的喜好快速或缓慢地完成每个部分。建议的舒适进度可能是在八天内每天完成一课。强烈推荐。

下面是你将在接下来的8堂课中学习的主题：

+   **第01课**：创建您的稳定扩散环境

+   **第02课**：为自己腾出空间

+   **第03课**：试验和错误

+   **第04课**：提示语法

+   **第05课**：更多试验和错误

+   **第06课**：ControlNet

+   **第07课**：LoRA

+   **第08课**：更好的面孔

这将会非常有趣。

您需要做一些工作：一些阅读、研究和实验。您想学习如何完成一个稳定扩散项目，对吧？

**在评论中发布您的结果**；我会为您加油！

坚持下去，不要放弃。

## 第01课：创建您的稳定扩散环境

稳定扩散是一个深度学习模型，模拟扩散过程生成图片。您需要了解扩散物理学，以欣赏一个看似不寻常的计算机算法如何生成艺术品。然而，作为用户，您可以假设它是一个可以将您的输入（如文字描述）转换为图片的黑盒子。

稳定扩散是一个基础模型，社区提供了许多重新训练或微调的衍生模型。但归根结底，它是一个需要大量计算资源的深度学习模型。要运行该模型，建议您使用带有良好GPU的计算机。如果您的计算机没有GPU，可以使用AWS等云提供商的计算资源。

您可能希望有一个UI来使您的工作流程更加顺畅。它可以帮助您更快地迭代，并避免许多可能在编写代码时犯的错误。稳定扩散有几个UI可供选择。ComfyUI非常灵活和强大。然而，由Automatic1111在GitHub上创建的Web UI是最易于使用的。这是您在这些课程中将使用的UI。

首先，您需要一个现代的Python，如Python 3.10或更高版本。Linux系统最好，因为流程更加顺畅，但Windows或Mac也可以使用。首先，您从GitHub下载Web UI。在Linux中，您运行以下git命令：

```py
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui
```

然后，您需要下载稳定扩散模型。Civitai是一个知名社区，提供用户生成的模型。例如，您可以从以下位置获取“Realistic Vision v6”模型：

+   [https://civitai.com/models/4201/realistic-vision-v60-b1](https://civitai.com/models/4201/realistic-vision-v60-b1)

选择“safetensors”格式并点击下载按钮。然后将下载的模型文件移动到`stable-diffusion-webui/models/Stable-diffusion`。

您可以在Hugging Face找到模型的另一个地方。您可以使用关键词“stable diffusion”进行搜索。例如，这些是您可以找到Deliberate模型以及原始Stable Diffusion v1.5模型的位置：

+   [https://huggingface.co/XpucT/Deliberate](https://huggingface.co/XpucT/Deliberate)

+   [https://huggingface.co/models?pipeline_tag=text-to-image&sort=downloads](https://huggingface.co/models?pipeline_tag=text-to-image&sort=downloads)

去这些页面的“文件”选项卡，获取模型文件。类似地，在下载后将其移动到`stable-diffusion-webui/models/Stable-diffusion`。

下载完模型后，您可以进入Web UI目录并启动它：

```py
cd stable-diffusion-web-ui
./webui.sh
```

这将自动创建一个Python虚拟环境，安装所需的包（如PyTorch），并启动Web UI。如果您在自己的计算机上启动它，应该会看到浏览器启动到以下URL：

+   http://localhost:7860

但如果您是在远程运行此操作，例如在云上的远程计算机上，您应该让Web UI“监听”公共IP：

```py
./webui.sh --listen
```

并且这是您在浏览器上应该看到的内容：

![](../Images/9366c3a18318a50e0e03fadfdf6a9d92.png)

### 您的任务

尝试按照上述步骤启动您的Web UI。确保没有遇到错误，并在左上角的下拉菜单中选择了您的检查点。

## 第二课：为自己腾出空间

想象一下，您是室内设计师，您的项目是设计一间卧室。您想向其他人展示您的想法。您可以在纸上绘制它。但您也可以让计算机为您绘制它。

要使用Stable Diffusion绘制图片，您可以提供文本提示，并期待返回一幅图片。这就是Web UI中的“text2img”功能。您可以在正面提示框中输入以下内容：

> 卧室、现代风格、一面墙上有一个窗户、真实照片

确保您选择了“检查点”，即您正在使用的Stable Diffusion模型。将宽度和高度设置为768和512。然后点击“生成”并等待图片生成。这是您由Stable Diffusion创建的第一件艺术品。

正如您在屏幕上所见，您可以在生成图片之前进行更多调整。您使用了正面提示，但也有负面提示。正面提示应描述输出内容，而负面提示告诉图片应该避免什么。由于您要求“真实的照片”，因此可以通过在负面提示中提到其他风格来增强结果，例如

> 素描、黑白、绘画

您可以添加更多内容，比如“户外”，因为显然您的输出应该是一个室内场景。您还可以尝试调整一些关键词，如果希望以不同风格的房间，例如“砖墙”或“小屋内部”。

![](../Images/19ebef625553078f395595c2cad6595c.png)

使用文本提示生成一个房间的图像

任何提示词（即使是非英语的）都可以使用标准算法转换为嵌入向量。然而，不同模型可能会以不同方式理解提示词。模型可能会忽略或错误解释模型不认识的关键词。这就是为什么你需要尝试不同的模型。

### 你的任务

在网页界面中，你可以看到许多其他选项。尝试上述方法，并进行多次生成。你应该每次看到不同的图片。然后，找到随机种子的框，输入一个固定的正整数，再次生成。你是否看到图片始终保持不变？固定随机种子但更改采样器会怎样？

要重新创建相同的图片，你需要固定提示词、模型以及选项，包括种子、采样器和步骤。

## 课程03：试错法

不幸的是，扩散过程的本质涉及大量随机性，有许多可调节的参数，因此很难判断你的设置是否正确。最简单的检查方法是保持设置（包括提示词、模型、调度器等）不变，但调整随机种子。如果多个随机种子产生了持续错误的结果，你就知道需要更改其他设置。

在网页界面中，你可以调整批量大小和批量计数，同时将随机种子保持在$-1$，一次按下生成按钮即可生成多张图片。

与许多深度学习模型一样，数据是批量处理的，批量大小告诉你Stable Diffusion模型需要同时处理多少张图片，每张图片使用不同的随机种子。这仅在你的GPU有足够内存时有效。否则，你可以增加批量计数，即你想生成的批次数。每个批次将运行一次生成（因此相对较慢）。

![](../Images/b7fa2d64edbb378d19dea133971ef638.png)

将“批量大小”设置为4将一次生成四张图片。设置“批量计数”也会产生类似的效果。

使用多个批量大小或批量计数，你将能在网页界面中一次看到所有输出，显示为“联络表”，展示所有图片。你可以点击每张图片以检查生成它的参数（包括使用的随机种子）。

![](../Images/fa176cbd83d826f18998300fa9d71efd.png)

多张图片是在相同批次中使用相同提示词和设置生成的，但随机种子不同。

### 你的任务

尝试使用多个批量大小或批量计数生成图片。调整你的提示词或其他设置，再次尝试。你能找到屏幕上显示的图片下载按钮吗？你知道如何查找之前生成的所有图片的历史记录吗？网页界面中有一个标签页可以查看。你还可以找到存储之前生成图片的磁盘位置吗？

## 课程04：提示词语法

您提供了一个提示给 Stable Diffusion，并且有一个预处理器将您的文本提示转换为数值嵌入向量。提示将被分解为关键字，并且您可以使每个关键字对图片生成过程有不同的影响。

考虑一个如下提示：

> 卧室，现代风格，一面墙上有一个窗户，灰色床单，木制床头柜，吊灯，墙上挂着图片，（一只猫在地板上睡觉:1.1），真实照片

在此提示中，您可以看到“（一只猫在地板上睡觉:1.1）”的片段被括号括起来，并且具有“（文本:权重）”的样式。这使得引用的文本对最终的嵌入向量有不同的权重，其中默认权重为1.0。您可以在 Web UI 的正负提示框中使用此提示语法。

你不应该尝试非常极端的权重，因为模型不是为此设计的。通常情况下，您的权重应该在0.5到1.5之间。如果您需要非常极端的权重，这可能意味着您的模型没有经过训练来理解该关键字。

### 您的任务

查看 [https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki) 并查看支持的提示语法。这由 Web UI 中的提示处理器处理，不属于 Stable Diffusion 模型的一部分。有一种语法可以使您在生成步骤的前半部分使用一个提示，在剩余步骤中使用另一个提示。如何编写这样的提示？

## 第05课：更多的试验与错误

使用相同的参数集，您可以使用批量大小同时生成多张具有不同随机种子的图片。通常情况下可以正常工作，并且允许您找到一个适合好图片的良好种子。

有时候，您需要尝试不同的参数，因为您不确定哪个参数可以产生正确的效果。这可以尝试在不同模型上使用相同的提示，或者替换提示中的关键字。

默认情况下，Web UI 中有一个“脚本”部分，在该部分中有一个“X/Y/Z 绘图”。这个名称意味着创建“轴”，其中轴中的每个点都是一个选项，因此您可以通过一个按钮点击尝试所有组合。

让我们尝试以下：将正提示设置为：

> 卧室，现代风格，一面墙上有一个窗户，真实照片

以及负提示：

> 素描，黑白，绘画

然后在脚本部分选择“X/Y/Z 绘图”，选择“提示 S/R”作为“X 类型”，然后输入以下内容作为“X 值”：

> 现代，皇家，中世纪，日本风

![](../Images/a0bde43e5eb199362bc4db17fd98a0b7.png)

使用“X/Y/Z 绘图”脚本和“提示 S/R”类型生成多种具有不同提示的图像。

X/Y/Z图示部分的所有值都是以逗号分隔的。第一个值，“现代”，是要搜索的内容（S），其余的是要替换的内容（R）。这就是Prompt S/R帮助你构建组合的方式。当你点击“生成”时，你将看到如上图所示的图片。例如，现代风格的卧室有简约的装饰风格，而日本卧室没有床，而是榻榻米。

### 你的任务

脚本中还有一个“提示矩阵”，但“Prompt S/R”更容易使用。尝试使用其他“类型”和“值”的“X/Y/Z图示”。你最多可以同时使用三个轴。你发现哪个“类型”允许你尝试多个模型？你认为不同的模型能给你不同的房间吗？尝试使用提示生成图像中的人。你应该能看到不同模型如何不同地生成人的面孔。X/Y/Z图示是一个强大的功能，让你可以探索生成你喜欢的图像的选项。

## 第06课：ControlNet

在幕后，Stable Diffusion模型从一个随机数字矩阵开始，逐渐将其转换为一个像素矩阵，在这个矩阵中你可以识别出图像符合你提示的描述。这个过程涉及多个迭代（调度步骤），去除的随机性量取决于你在Web UI中设置的参数。

由于有多个迭代，你可以故意调整每次迭代的输出，然后再将其反馈给Stable Diffusion模型。这就是ControlNet的理念。要使用ControlNet，请在Web UI中检查“扩展”选项卡并安装ControlNet插件。然后，你还应该从Hugging Face下载并安装ControlNet模型，按照ControlNet扩展的wiki页面中的说明进行操作：

+   [https://github.com/Mikubill/sd-webui-controlnet/wiki/Model-download](https://github.com/Mikubill/sd-webui-controlnet/wiki/Model-download)

安装ControlNet后，你可能需要重新启动Web UI（在扩展选项卡中有一个按钮）并刷新浏览器以加载它。

让我们尝试解决这个提示：如果你多次生成房间，你会发现每次视角都不同。很难写出描述你想要的视角的提示，但在图像中展示出来却很容易。让我们下载并使用这张“空房间”图片：

+   [https://machinelearningmastery.com/wp-content/uploads/2024/09/RoomAnime.png](https://machinelearningmastery.com/wp-content/uploads/2024/09/RoomAnime.png)

[![](../Images/473493ee6a1c22b2d178023b5c43e438.png)](https://machinelearningmastery.com/wp-content/uploads/2024/09/RoomAnime.png)

动漫风格的空房间图像。由作者使用Stable Diffusion生成。

您设置提示和其他设置与之前相同。但这次，扩展并启用ControlNet部分。在“单图像”选项卡上，上传这张图片。然后选择“MLSD”作为“控制类型”。最重要的是，将“开始控制步骤”设置为0，并将“结束控制步骤”设置为0.3。这意味着您仅在步骤的前30%中使用此ControlNet。例如，如果您选择了采样步骤为40，ControlNet将在步骤0到12中干扰图像。

设置批处理大小并生成。您会发现生成的图像都是以相同角度观看的。这是因为“MLSD”是一个边缘检测ControlNet，它将您上传的图片转换为线条艺术并应用于图片上。您可以尝试将Control Type更改为“Canny”，这应该会产生类似的效果，因为它是另一种边缘检测算法。

![](../Images/446680136c0943618bc4bef28489ccce.png)

使用MLSD ControlNet生成同一视角房间的图片。

### 您的任务

按照上述说明生成图片后，您看到了查看边缘检测结果的方法吗？您看到了一些控制边缘检测算法灵敏度的参数吗？还有另一种名为“涂鸦”的控制类型。这不需要您上传图片，但允许您用鼠标画一张图片。尝试这个并观察效果。

![](../Images/d91dcb16c09a1e3e5176b0887c58a030.png)

使用“涂鸦”ControlNet。

## 第07课：LoRA

ControlNet不是干扰生成图片的扩散过程的唯一方法。LoRA是一个插件，也可以将效果应用到输出上。首先，让我们下载并使用一个稳定扩散XL模型（SDXL），就像这里的一个：

+   [https://civitai.com/models/312530](https://civitai.com/models/312530)（CyberRealistic XL model）

然后，您可以从Civitai下载“更好的图片、更多细节的LoRA”：

+   [https://civitai.com/models/126343/better-picture-more-details-lora](https://civitai.com/models/126343/better-picture-more-details-lora)

并将其保存到路径（`stable-diffusion-webui/models/Lora`）。请注意，LoRA需要匹配的稳定扩散架构。这个需要SDXL。其他一些LoRA可能需要SD1或SD2。您不能混合使用它们。

![](../Images/5405f04b66bbce212b9c0ef75897dcd0.png)

您可以从Civitai.com下载LoRA模型。请注意，此LoRA需要使用基于SDXL的模型。

下载LoRA后，请尝试以下提示：

> 卧室，现代风格，墙上有一个窗户，灰色床单，木制床头柜，吊灯，墙上有图片，（一个女孩坐在地板上抱着一只猫：1.2），逼真的照片，<lora:SDXLHighDetail_v5:1.1>

带有尖括号的部分是在 Web UI 中使用 LoRA 的方法。您需要指定 LoRA 的文件名和要使用的权重。某些 LoRA 可能允许您使用负权重，但必须检查。如果省略“:1.1”部分，则默认权重为 1.0。

这个特定的 LoRA 添加了诸如纹理之类的细节到您生成的图片中。例如，您应该看到更逼真的布料和头发。

![](../Images/c98b873a99c6ad1c01d1419c77b5b3ab.png)

在生成管道中使用 LoRA

### 您的任务

探索 Hugging Face 和 Civitai 的 LoRA 模型。您看到了如何检查 LoRA 是否适用于 SD1、SD2 或 SDXL 吗？您看到了如何在 Civitai 中使用它们的示例吗？您可以在 SD 1.5 上启用 ControlNet 使用 SDXL 的 LoRA 吗？

## 第 08 课：更好的面孔

在前一课中，您看到了“一个女孩坐在地板上抱着一只猫”的提示被添加。这使得图片更加复杂，您应该更容易看到不同模型之间的区别。但是，您也应该看到人类面孔有时看起来不自然。特别是如果您不是生成肖像，而是人类仅占图片的一小部分时。

有一种方法可以修复它。首先，您需要转到“扩展”选项卡，并安装名为“ADetailer”的插件。安装完成后，您可能需要重新加载 Stable Diffusion Web UI。然后，您应该像以前一样设置提示和其他选项。

之后，您可以展开“ADetailer”部分，并选择“face_yolov8n.pt”作为面部检测器。您可以跳过提示或设置一个像“困倦的面孔”这样的提示。ADetailer 的作用是在扩散过程完成后检测生成图片上的面部，然后使用您的提示重新生成面部。这有助于使面部更加逼真，或更精确地调整面部表情。

![](../Images/b6d820c1657570b2f4dc5ced393ac1c1.png)

使用 ADetailer 创建逼真的面孔

使用 ADetailer 可以修正畸形的面部，从而使图片变得更好，但不能使面部看起来像另一个人。这是另一个插件的工作：ReActor。同样，您需要转到“扩展”选项卡来安装它。然后，展开“ReActor”部分并上传一张面部的图片。例如，这张图片：

+   [https://unsplash.com/photos/girl-sitting-and-leaning-on-hedge-YWrXZjb3Ulg](https://unsplash.com/photos/girl-sitting-and-leaning-on-hedge-YWrXZjb3Ulg)

![](../Images/620416bb4af145aed76dbf4d43438852.png)

照片由[Kune Chan](https://unsplash.com/photos/girl-sitting-and-leaning-on-hedge-YWrXZjb3Ulg)提供。部分版权保留。

为了获得最佳结果，您选择的图片应该是正面且清晰，这样面部的更多特征就能被看到。只需上传图片并生成，您应该看到生成的所有图片都与您上传的照片中的人物相似。

![](../Images/4af9be6445ed52f0dc035e17fd67147a.png)

使用 ReActor 扩展生成与另一幅肖像有关的图片

### 您的任务

在ADetailer中，你不仅可以打磨脸部，还可以打磨手部。你能找到用于这一目的的探测器吗？在ReActor中，你可以提供多人照片，并选择不同的面孔来使用。你如何控制这个过程？

这是最后一课。

## 结束啦！（*看看你已经走了多远*）

你做到了。干得漂亮！

现在你是一个拥有名为稳定扩散的助手的室内设计师。你可以轻松让计算机为你生成不同的设计，并为你生成设计草图。这就是你如何利用生成式AI的力量来节省时间，并让你更多地专注于思想，而不是细节。

现在花点时间回顾一下你已经走过的路程。

+   你学会了如何快速设置和运行稳定扩散。

+   你学会了如何使用提示和各种语法控制图像生成。

+   你学会了一些可以帮助你生成更好图片的Web UI扩展。

+   你学会了如何有效地进行生成过程的实验。

不要小看这一点；你在短时间内取得了长足的进步。这只是你生成式AI之旅的开始。继续练习和发展你的技能。

## 概要

**你在迷你课程中表现如何？**

你喜欢这个速成课程吗？

**你有任何问题吗？有什么难以解决的问题吗？**

让我知道。请在下面留言。
