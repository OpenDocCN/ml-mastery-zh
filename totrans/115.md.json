["```py\n# importing the libraries\nimport torch\nimport matplotlib.pyplot as plt\n\n# create a PyTorch tensor\nx = torch.linspace(-10, 10, 100)\n\n# apply the logistic activation function to the tensor\ny = torch.sigmoid(x)\n\n# plot the results with a custom color\nplt.plot(x.numpy(), y.numpy(), color='purple')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.title('Logistic Activation Function')\nplt.show()\n```", "```py\n# apply the tanh activation function to the tensor\ny = torch.tanh(x)\n\n# plot the results with a custom color\nplt.plot(x.numpy(), y.numpy(), color='blue')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.title('Tanh Activation Function')\nplt.show()\n```", "```py\n# apply the ReLU activation function to the tensor\ny = torch.relu(x)\n\n# plot the results with a custom color\nplt.plot(x.numpy(), y.numpy(), color='green')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.title('ReLU Activation Function')\nplt.show()\n```", "```py\n# importing the libraries\nimport torch\nimport matplotlib.pyplot as plt\n\n# create a PyTorch tensor\nx = torch.linspace(-10, 10, 100)\n\n# apply the logistic activation function to the tensor and plot\ny = torch.sigmoid(x)\nplt.plot(x.numpy(), y.numpy(), color='purple')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.title('Logistic Activation Function')\nplt.show()\n\n# apply the tanh activation function to the tensor and plot\ny = torch.tanh(x)\nplt.plot(x.numpy(), y.numpy(), color='blue')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.title('Tanh Activation Function')\nplt.show()\n\n# apply the ReLU activation function to the tensor and plot\ny = torch.relu(x)\nplt.plot(x.numpy(), y.numpy(), color='green')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.title('ReLU Activation Function')\nplt.show()\n```", "```py\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n# Load the MNIST dataset\ntransform = transforms.ToTensor()\ntrain_dataset = datasets.MNIST(root='data/', train=True, transform=transform, download=True)\ntest_dataset = datasets.MNIST(root='data/', train=False, transform=transform, download=True)\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes, activation_function):\n        super(NeuralNetwork, self).__init__()\n        self.layer1 = nn.Linear(input_size, hidden_size)\n        self.layer2 = nn.Linear(hidden_size, hidden_size)\n        self.layer3 = nn.Linear(hidden_size, num_classes)\n        self.activation_function = activation_function\n\n    def forward(self, x):\n        x = self.activation_function(self.layer1(x))\n        x = self.activation_function(self.layer2(x))\n        x = self.layer3(x)\n        return x\n```", "```py\ndef train(network, data_loader, criterion, optimizer, device):\n    network.train()\n    running_loss = 0.0\n\n    for data, target in data_loader:\n        data, target = data.to(device), target.to(device)\n        data = data.view(data.shape[0], -1)\n\n        optimizer.zero_grad()\n        output = network(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * data.size(0)\n\n    return running_loss / len(data_loader.dataset)\n\ndef test(network, data_loader, criterion, device):\n    network.eval()\n    correct = 0\n    total = 0\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for data, target in data_loader:\n            data, target = data.to(device), target.to(device)\n            data = data.view(data.shape[0], -1)\n\n            output = network(data)\n            loss = criterion(output, target)\n            test_loss += loss.item() * data.size(0)\n            _, predicted = torch.max(output.data, 1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n\n    return test_loss / len(data_loader.dataset), 100 * correct / total\n```", "```py\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ninput_size = 784\nhidden_size = 128\nnum_classes = 10\nnum_epochs = 10\nlearning_rate = 0.001\n\nactivation_functions = {\n    'ReLU': nn.ReLU(),\n    'Sigmoid': nn.Sigmoid(),\n    'Tanh': nn.Tanh(),\n    'LeakyReLU': nn.LeakyReLU()\n}\n\nresults = {}\n\n# Train and test the model with different activation functions\nfor name, activation_function in activation_functions.items():\n    print(f\"Training with {name} activation function...\")\n\n    model = NeuralNetwork(input_size, hidden_size, num_classes, activation_function).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    train_loss_history = []\n    test_loss_history = []\n    test_accuracy_history = []\n\n    for epoch in range(num_epochs):\n        train_loss = train(model, train_loader, criterion, optimizer, device)\n        test_loss, test_accuracy = test(model, test_loader, criterion, device)\n\n        train_loss_history.append(train_loss)\n        test_loss_history.append(test_loss)\n        test_accuracy_history.append(test_accuracy)\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n\n    results[name] = {\n        'train_loss_history': train_loss_history,\n        'test_loss_history': test_loss_history,\n        'test_accuracy_history': test_accuracy_history\n    }\n```", "```py\nTraining with ReLU activation function...\nEpoch [1/10], Test Loss: 0.1589, Test Accuracy: 95.02%\nEpoch [2/10], Test Loss: 0.1138, Test Accuracy: 96.52%\nEpoch [3/10], Test Loss: 0.0886, Test Accuracy: 97.15%\nEpoch [4/10], Test Loss: 0.0818, Test Accuracy: 97.50%\nEpoch [5/10], Test Loss: 0.0783, Test Accuracy: 97.47%\nEpoch [6/10], Test Loss: 0.0754, Test Accuracy: 97.80%\nEpoch [7/10], Test Loss: 0.0832, Test Accuracy: 97.56%\nEpoch [8/10], Test Loss: 0.0783, Test Accuracy: 97.78%\nEpoch [9/10], Test Loss: 0.0789, Test Accuracy: 97.75%\nEpoch [10/10], Test Loss: 0.0735, Test Accuracy: 97.99%\nTraining with Sigmoid activation function...\nEpoch [1/10], Test Loss: 0.2420, Test Accuracy: 92.81%\nEpoch [2/10], Test Loss: 0.1718, Test Accuracy: 94.99%\nEpoch [3/10], Test Loss: 0.1339, Test Accuracy: 96.06%\nEpoch [4/10], Test Loss: 0.1141, Test Accuracy: 96.42%\nEpoch [5/10], Test Loss: 0.1004, Test Accuracy: 97.00%\nEpoch [6/10], Test Loss: 0.0909, Test Accuracy: 97.10%\nEpoch [7/10], Test Loss: 0.0846, Test Accuracy: 97.28%\nEpoch [8/10], Test Loss: 0.0797, Test Accuracy: 97.42%\nEpoch [9/10], Test Loss: 0.0785, Test Accuracy: 97.58%\nEpoch [10/10], Test Loss: 0.0795, Test Accuracy: 97.58%\nTraining with Tanh activation function...\nEpoch [1/10], Test Loss: 0.1660, Test Accuracy: 95.17%\nEpoch [2/10], Test Loss: 0.1152, Test Accuracy: 96.47%\nEpoch [3/10], Test Loss: 0.1057, Test Accuracy: 96.86%\nEpoch [4/10], Test Loss: 0.0865, Test Accuracy: 97.21%\nEpoch [5/10], Test Loss: 0.0760, Test Accuracy: 97.61%\nEpoch [6/10], Test Loss: 0.0856, Test Accuracy: 97.23%\nEpoch [7/10], Test Loss: 0.0735, Test Accuracy: 97.66%\nEpoch [8/10], Test Loss: 0.0790, Test Accuracy: 97.67%\nEpoch [9/10], Test Loss: 0.0805, Test Accuracy: 97.47%\nEpoch [10/10], Test Loss: 0.0834, Test Accuracy: 97.82%\nTraining with LeakyReLU activation function...\nEpoch [1/10], Test Loss: 0.1587, Test Accuracy: 95.14%\nEpoch [2/10], Test Loss: 0.1084, Test Accuracy: 96.37%\nEpoch [3/10], Test Loss: 0.0861, Test Accuracy: 97.22%\nEpoch [4/10], Test Loss: 0.0883, Test Accuracy: 97.06%\nEpoch [5/10], Test Loss: 0.0870, Test Accuracy: 97.37%\nEpoch [6/10], Test Loss: 0.0929, Test Accuracy: 97.26%\nEpoch [7/10], Test Loss: 0.0824, Test Accuracy: 97.54%\nEpoch [8/10], Test Loss: 0.0785, Test Accuracy: 97.77%\nEpoch [9/10], Test Loss: 0.0908, Test Accuracy: 97.92%\nEpoch [10/10], Test Loss: 0.1012, Test Accuracy: 97.76%\n```", "```py\nimport matplotlib.pyplot as plt\n\n# Plot the training loss\nplt.figure()\nfor name, data in results.items():\n    plt.plot(data['train_loss_history'], label=name)\nplt.xlabel('Epoch')\nplt.ylabel('Training Loss')\nplt.legend()\nplt.show()\n\n# Plot the testing loss\nplt.figure()\nfor name, data in results.items():\n    plt.plot(data['test_loss_history'], label=name)\nplt.xlabel('Epoch')\nplt.ylabel('Testing Loss')\nplt.legend()\nplt.show()\n\n# Plot the testing accuracy\nplt.figure()\nfor name, data in results.items():\n    plt.plot(data['test_accuracy_history'], label=name)\nplt.xlabel('Epoch')\nplt.ylabel('Testing Accuracy')\nplt.legend()\nplt.show()\n```"]