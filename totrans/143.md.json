["```py\nimport torch\n\nx = torch.tensor([1, 2, 3])\nprint(x)\nprint(x.shape)\nprint(x.dtype)\n```", "```py\ntensor([1, 2, 3])\ntorch.Size([3])\ntorch.int64\n```", "```py\nimport torch\n\nx = torch.tensor([1., 2., 3.], requires_grad=True)\nprint(x)\nprint(x.shape)\nprint(x.dtype)\n```", "```py\ntensor([1., 2., 3.], requires_grad=True)\ntorch.Size([3])\ntorch.float32\n```", "```py\nimport torch\n\nx = torch.tensor(3.6, requires_grad=True)\ny = x * x\ny.backward()\nprint(x.grad)\n```", "```py\ntensor(7.2000)\n```", "```py\nimport numpy as np\n\npolynomial = np.poly1d([1, 2, 3])\nprint(polynomial)\n```", "```py\n   2\n1 x + 2 x + 3\n```", "```py\nprint(polynomial(1.5))\n```", "```py\nN = 20   # number of samples\n\n# Generate random samples roughly between -10 to +10\nX = np.random.randn(N,1) * 5\nY = polynomial(X)\n```", "```py\nimport torch\n\n# Assume samples X and Y are prepared elsewhere\n\nXX = np.hstack([X*X, X, np.ones_like(X)])\n\nw = torch.randn(3, 1, requires_grad=True)  # the 3 coefficients\nx = torch.tensor(XX, dtype=torch.float32)  # input sample\ny = torch.tensor(Y, dtype=torch.float32)   # output sample\noptimizer = torch.optim.NAdam([w], lr=0.01)\nprint(w)\n\nfor _ in range(1000):\n    optimizer.zero_grad()\n    y_pred = x @ w\n    mse = torch.mean(torch.square(y - y_pred))\n    mse.backward()\n    optimizer.step()\n\nprint(w)\n```", "```py\ntensor([[1.3827],\n        [0.8629],\n        [0.2357]], requires_grad=True)\n```", "```py\ntensor([[1.0004],\n        [1.9924],\n        [2.9159]], requires_grad=True)\n```", "```py\nimport numpy as np\nimport torch\n\npolynomial = np.poly1d([1, 2, 3])\nN = 20   # number of samples\n\n# Generate random samples roughly between -10 to +10\nX = np.random.randn(N,1) * 5\nY = polynomial(X)\n\n# Prepare input as an array of shape (N,3)\nXX = np.hstack([X*X, X, np.ones_like(X)])\n\n# Prepare tensors\nw = torch.randn(3, 1, requires_grad=True)  # the 3 coefficients\nx = torch.tensor(XX, dtype=torch.float32)  # input sample\ny = torch.tensor(Y, dtype=torch.float32)   # output sample\noptimizer = torch.optim.NAdam([w], lr=0.01)\nprint(w)\n\n# Run optimizer\nfor _ in range(1000):\n    optimizer.zero_grad()\n    y_pred = x @ w\n    mse = torch.mean(torch.square(y - y_pred))\n    mse.backward()\n    optimizer.step()\n\nprint(w)\n```", "```py\n[ A ]  +  [ B ]  =  9\n  +         -\n[ C ]  -  [ D ]  =  1\n  =         =\n  8         2\n```", "```py\nimport random\nimport torch\n\nA = torch.tensor(random.random(), requires_grad=True)\nB = torch.tensor(random.random(), requires_grad=True)\nC = torch.tensor(random.random(), requires_grad=True)\nD = torch.tensor(random.random(), requires_grad=True)\n\n# Gradient descent loop\nEPOCHS = 2000\noptimizer = torch.optim.NAdam([A, B, C, D], lr=0.01)\nfor _ in range(EPOCHS):\n    y1 = A + B - 9\n    y2 = C - D - 1\n    y3 = A + C - 8\n    y4 = B - D - 2\n    sqerr = y1*y1 + y2*y2 + y3*y3 + y4*y4\n    optimizer.zero_grad()\n    sqerr.backward()\n    optimizer.step()\n\nprint(A)\nprint(B)\nprint(C)\nprint(D)\n```", "```py\ntensor(4.7191, requires_grad=True)\ntensor(4.2808, requires_grad=True)\ntensor(3.2808, requires_grad=True)\ntensor(2.2808, requires_grad=True)\n```"]