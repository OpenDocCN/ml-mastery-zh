["```py\nimport torch\nfrom torch.utils.data import Dataset\n\nclass toy_data(Dataset):\n    \"The data for multi-class classification\"\n    def __init__(self):\n        # single input\n        self.x = torch.arange(-3, 3, 0.1).view(-1, 1)\n        # multi-class output\n        self.y = torch.zeros(self.x.shape[0])\n        self.y[(self.x > -2.0)[:, 0] * (self.x < 0.0)[:, 0]] = 1 self.y[(self.x >= 0.0)[:, 0] * (self.x < 2.0)[:, 0]] = 2 self.y[(self.x >= 2.0)[:, 0]] = 3\n        self.y = self.y.type(torch.LongTensor)\n        self.len = self.x.shape[0]\n\n    def __getitem__(self, idx):\n        \"accessing one element in the dataset by index\"\n        return self.x[idx], self.y[idx] \n\n    def __len__(self):\n        \"size of the entire dataset\"\n        return self.len\n```", "```py\n# Create the dataset object and check a few samples\ndata = toy_data()\nprint(\"first ten data samples: \", data.x[0:10])\nprint(\"first ten data labels: \", data.y[0:10])\n```", "```py\nfirst ten data samples:  tensor([[-3.0000],\n        [-2.9000],\n        [-2.8000],\n        [-2.7000],\n        [-2.6000],\n        [-2.5000],\n        [-2.4000],\n        [-2.3000],\n        [-2.2000],\n        [-2.1000]])\nfirst ten data labels:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n```", "```py\nclass Softmax(torch.nn.Module):\n    \"custom softmax module\"\n    def __init__(self, n_inputs, n_outputs):\n        super().__init__()\n        self.linear = torch.nn.Linear(n_inputs, n_outputs)\n\n    def forward(self, x):\n        pred = self.linear(x)\n        return pred\n```", "```py\n# call Softmax Classifier\nmodel_softmax = Softmax(1, 4)\nmodel_softmax.state_dict()\n```", "```py\nOrderedDict([('linear.weight',\n              tensor([[-0.0075],\n                      [ 0.5364],\n                      [-0.8230],\n                      [-0.7359]])),\n             ('linear.bias', tensor([-0.3852,  0.2682, -0.0198,  0.7929]))])\n```", "```py\n...\nfrom torch.utils.dataimport DataLoader\n\n# define loss, optimizier, and dataloader\noptimizer = torch.optim.SGD(model_softmax.parameters(), lr = 0.01)\ncriterion = torch.nn.CrossEntropyLoss()\ntrain_loader = DataLoader(dataset = data, batch_size = 2)\n```", "```py\n# Train the model\nLoss = []\nepochs = 100\nfor epoch in range(epochs):\n    for x, y in train_loader:\n        optimizer.zero_grad()\n        y_pred = model_softmax(x)\n        loss = criterion(y_pred, y)\n        Loss.append(loss)\n        loss.backward()\n        optimizer.step()\nprint(\"Done!\")\n```", "```py\n# Make predictions on test data\npred_model =  model_softmax(data.x)\n_, y_pred = pred_model.max(1)\nprint(\"model predictions on test data:\", y_pred)\n```", "```py\nmodel predictions on test data: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n```", "```py\n# check model accuracy\ncorrect = (data.y == y_pred).sum().item()\nacc = correct / len(data)\nprint(\"model accuracy: \", acc)\n```", "```py\nmodel accuracy:  0.9833333333333333\n```", "```py\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass toy_data(Dataset):\n    \"The data for multi-class classification\"\n    def __init__(self):\n        # single input\n        self.x = torch.arange(-3, 3, 0.1).view(-1, 1)\n        # multi-class output\n        self.y = torch.zeros(self.x.shape[0])\n        self.y[(self.x > -2.0)[:, 0] * (self.x < 0.0)[:, 0]] = 1\n        self.y[(self.x >= 0.0)[:, 0] * (self.x < 2.0)[:, 0]] = 2\n        self.y[(self.x >= 2.0)[:, 0]] = 3\n        self.y = self.y.type(torch.LongTensor)\n        self.len = self.x.shape[0]\n\n    def __getitem__(self, idx):\n        \"accessing one element in the dataset by index\"\n        return self.x[idx], self.y[idx] \n\n    def __len__(self):\n        \"size of the entire dataset\"\n        return self.len\n\n# Create the dataset object and check a few samples\ndata = toy_data()\nprint(\"first ten data samples: \", data.x[0:10])\nprint(\"first ten data labels: \", data.y[0:10])\n\nclass Softmax(torch.nn.Module):\n    \"custom softmax module\"\n    def __init__(self, n_inputs, n_outputs):\n        super().__init__()\n        self.linear = torch.nn.Linear(n_inputs, n_outputs)\n\n    def forward(self, x):\n        pred = self.linear(x)\n        return pred\n\n# call Softmax Classifier\nmodel_softmax = Softmax(1, 4)\nmodel_softmax.state_dict()\n\n# define loss, optimizier, and dataloader\noptimizer = torch.optim.SGD(model_softmax.parameters(), lr=0.01)\ncriterion = torch.nn.CrossEntropyLoss()\ntrain_loader = DataLoader(dataset=data, batch_size=2)\n\n# Train the model\nLoss = []\nepochs = 100\nfor epoch in range(epochs):\n    for x, y in train_loader:\n        optimizer.zero_grad()\n        y_pred = model_softmax(x)\n        loss = criterion(y_pred, y)\n        Loss.append(loss)\n        loss.backward()\n        optimizer.step()\nprint(\"Done!\")\n\n# Make predictions on test data\npred_model =  model_softmax(data.x)\n_, y_pred = pred_model.max(1)\nprint(\"model predictions on test data:\", y_pred)\n\n# check model accuracy\ncorrect = (data.y == y_pred).sum().item()\nacc = correct / len(data)\nprint(\"model accuracy: \", acc)\n```"]