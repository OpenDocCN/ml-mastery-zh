- en: Develop Your First Neural Network with PyTorch, Step by Step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://machinelearningmastery.com/develop-your-first-neural-network-with-pytorch-step-by-step/](https://machinelearningmastery.com/develop-your-first-neural-network-with-pytorch-step-by-step/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'PyTorch is a powerful Python library for building deep learning models. It
    provides everything you need to define and train a neural network and use it for
    inference. You don’t need to write much code to complete all this. In this pose,
    you will discover how to create your first deep learning neural network model
    in Python using PyTorch. After completing this post, you will know:'
  prefs: []
  type: TYPE_NORMAL
- en: How to load a CSV dataset and prepare it for use with PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to define a Multilayer Perceptron model in PyToch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to train and evaluate a PyToch model on a validation dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kick-start your project** with my book [Deep Learning with PyTorch](https://machinelearningmastery.com/deep-learning-with-pytorch/).
    It provides **self-study tutorials** with **working code**.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.![](../Images/c98e1f3e5f9db5608610e28f74622e06.png)
  prefs: []
  type: TYPE_NORMAL
- en: Develop your Ffrst neural network with PyTorch, step by step
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [drown_ in_city](https://unsplash.com/photos/V2DylCx9kkc). Some rights
    reserved.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is not a lot of code required. You will go over it slowly so that you
    will know how to create your own models in the future. The steps you will learn
    in this post are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Load Data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define PyToch Model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define Loss Function and Optimizers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run a Training Loop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the Model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make Predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step is to define the functions and classes you intend to use in this
    post. You will use the NumPy library to load your dataset and the  PyTorch library
    for deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The imports required are listed below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can now load your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, you will use the Pima Indians onset of diabetes dataset. This
    has been a standard machine learning dataset since the early days of the field.
    It describes patient medical record data for Pima Indians and whether they had
    an onset of diabetes within five years.
  prefs: []
  type: TYPE_NORMAL
- en: It is a binary classification problem (onset of diabetes as 1 or not as 0).
    All the input variables that describe each patient are transformed and numerical.
    This makes it easy to use directly with neural networks that expect numerical
    input and output values and is an ideal choice for our first neural network in
    PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: You can also download it [here](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv).
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the dataset and place it in your local working directory, the same
    location as your Python file. Save it with the filename `pima-indians-diabetes.csv`.
    Take a look inside the file; you should see rows of data like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now load the file as a matrix of numbers using the NumPy function `loadtxt()`.
    There are eight input variables and one output variable (the last column). You
    will be learning a model to map rows of input variables ($X$) to an output variable
    ($y$), which is often summarized as $y = f(X)$. The variables are summarized as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input Variables ($X$):'
  prefs: []
  type: TYPE_NORMAL
- en: Number of times pregnant
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plasma glucose concentration at 2 hours in an oral glucose tolerance test
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Diastolic blood pressure (mm Hg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Triceps skin fold thickness (mm)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2-hour serum insulin (μIU/ml)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Body mass index (weight in kg/(height in m)2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Diabetes pedigree function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Age (years)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Output Variables ($y$):'
  prefs: []
  type: TYPE_NORMAL
- en: Class label (0 or 1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the CSV file is loaded into memory, you can split the columns of data into
    input and output variables.
  prefs: []
  type: TYPE_NORMAL
- en: The data will be stored in a 2D array where the first dimension is rows and
    the second dimension is columns, e.g., (rows, columns). You can split the array
    into two arrays by selecting subsets of columns using the standard NumPy slice
    operator “`:`“. You can select the first eight columns from index 0 to index 7
    via the slice `0:8`. You can then select the output column (the 9th variable)
    via index 8.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: But these data should be converted to PyTorch tensors first. One reason is that
    PyTorch usually operates in a 32-bit floating point while NumPy, by default, uses
    a 64-bit floating point. Mix-and-match is not allowed in most operations. Converting
    to PyTorch tensors can avoid the implicit conversion that may cause problems.
    You can also take this chance to correct the shape to fit what PyTorch would expect,
    e.g., prefer $n\times 1$ matrix over $n$-vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'To convert, create a tensor out of NumPy arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You are now ready to define your neural network model.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Get Started With Deep Learning with PyTorch?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Take my free email crash course now (with sample code).
  prefs: []
  type: TYPE_NORMAL
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  prefs: []
  type: TYPE_NORMAL
- en: Define the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Indeed, there are two ways to define a model in PyTorch. The goal is to make
    it like a function that takes an input and returns an output.
  prefs: []
  type: TYPE_NORMAL
- en: A model can be defined as a sequence of layers. You create a `Sequential` model
    with the layers listed out. The first thing you need to do to get this right is
    to ensure the first layer has the correct number of input features. In this example,
    you can specify the input dimension  `8` for the eight input variables as one
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: The other parameters for a layer or how many layers you need for a model is
    not an easy question. You may use heuristics to help you design the model, or
    you can refer to other people’s designs in dealing with a similar problem. Often,
    the best neural network structure is found through a process of trial-and-error
    experimentation. Generally, you need a network large enough to capture the structure
    of the problem but small enough to make it fast. In this example, let’s use a
    fully-connected network structure with three layers.
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layers or dense layers are defined using the `Linear` class
    in PyTorch. It simply means an operation similar to matrix multiplication. You
    can specify the number of inputs as the first argument and the number of outputs
    as the second argument. The number of outputs is sometimes called the number of
    neurons or number of nodes in the layer.
  prefs: []
  type: TYPE_NORMAL
- en: You also need an activation function **after** the layer. If not provided, you
    just take the output of the matrix multiplication to the next step, or sometimes
    you call it using linear activation, hence the name of the layer.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, you will use the rectified linear unit activation function,
    referred to as ReLU, on the first two layers and the sigmoid function in the output
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: A sigmoid on the output layer ensures the output is between 0 and 1, which is
    easy to map to either a probability of class 1 or snap to a hard classification
    of either class by a cut-off threshold of 0.5\. In the past, you might have used
    sigmoid and tanh activation functions for all layers, but it turns out that sigmoid
    activation can lead to the problem of vanishing gradient in deep neural networks,
    and ReLU activation is found to provide better performance in terms of both speed
    and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can piece it all together by adding each layer such that:'
  prefs: []
  type: TYPE_NORMAL
- en: The model expects rows of data with 8 variables (the first argument at the first
    layer set to `8`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first hidden layer has 12 neurons, followed by a ReLU activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second hidden layer has 8 neurons, followed by another ReLU activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output layer has one neuron, followed by a sigmoid activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You can check the model by printing it out as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You are free to change the design and see if you get a better or worse result
    than the subsequent part of this post.
  prefs: []
  type: TYPE_NORMAL
- en: 'But note that, in PyTorch, there is a more verbose way of creating a model.
    The model above can be created as a Python `class` inherited from the `nn.Module`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the model printed will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In this approach, a class needs to have all the layers defined in the constructor
    because you need to prepare all its components when it is created, but the input
    is not yet provided. Note that you also need to call the parent class’s constructor
    (the line `super().__init__()`) to bootstrap your model. You also need to define
    a `forward()` function in the class to tell, if an input tensor `x` is provided,
    how you produce the output tensor in return.
  prefs: []
  type: TYPE_NORMAL
- en: You can see from the output above that the model remembers how you call each
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Preparation for Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A defined model is ready for training, but you need to specify what the goal
    of the training is. In this example, the data has the input features $X$ and the
    output label $y$. You want the neural network model to produce an output that
    is as close to $y$ as possible. Training a network means finding the best set
    of weights to map inputs to outputs in your dataset. The loss function is the
    metric to measure the prediction’s distance to $y$. In this example, you should
    use binary cross entropy because it is a binary classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: Once you decide on the loss function, you also need an optimizer. The optimizer
    is the algorithm you use to adjust the model weights progressively to produce
    a better output. There are many optimizers to choose from, and in this example,
    Adam is used. This popular version of gradient descent can automatically tune
    itself and gives good results in a wide range of problems.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The optimizer usually has some configuration parameters. Most notably, the learning
    rate `lr`. But all optimizers need to know what to optimize. Therefore. you pass
    on `model.parameters()`, which is a generator of all parameters from the model
    you created.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have defined your model, the loss metric, and the optimizer. It is ready
    for training by executing the model on some data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training a neural network model usually takes in epochs and batches. They are
    idioms for how data is passed to a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Epoch**: Passes the entire training dataset to the model once'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch**: One or more samples passed to the model, from which the gradient
    descent algorithm will be executed for one iteration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simply speaking, the entire dataset is split into batches, and you pass the
    batches one by one into a model using a training loop. Once you have exhausted
    all the batches, you have finished one epoch. Then you can start over again with
    the same dataset and start the second epoch, continuing to refine the model. This
    process repeats until you are satisfied with the model’s output.
  prefs: []
  type: TYPE_NORMAL
- en: The size of a batch is limited by the system’s memory. Also, the number of computations
    required is linearly proportional to the size of a batch. The total number of
    batches over many epochs is how many times you run the gradient descent to refine
    the model. It is a trade-off that you want more iterations for the gradient descent
    so you can produce a better model, but at the same time, you do not want the training
    to take too long to complete. The number of epochs and the size of a batch can
    be chosen experimentally by trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of training a model is to ensure it learns a good enough mapping of
    input data to output classification. It will not be perfect, and errors are inevitable.
    Usually, you will see the amount of error reducing when in the later epochs, but
    it will eventually level out. This is called model convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way to build a training loop is to use two nested for-loops, one
    for epochs and one for batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'When this runs, it will print the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Evaluate the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have trained our neural network on the entire dataset, and you can evaluate
    the performance of the network on the same dataset. This will only give you an
    idea of how well you have modeled the dataset (e.g., train accuracy) but no idea
    of how well the algorithm might perform on new data. This was done for simplicity,
    but ideally, you could separate your data into train and test datasets for training
    and evaluation of your model.
  prefs: []
  type: TYPE_NORMAL
- en: You can evaluate your model on your training dataset in the same way you invoked
    the model in training. This will generate predictions for each input, but then
    you still need to compute a score for the evaluation. This score can be the same
    as your loss function or something different. Because you are doing binary classification,
    you can use accuracy as your evaluation score by converting the output (a floating
    point in the range of 0 to 1) to an integer (0 or 1) and compare to the label
    we know.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `round()` function rounds off the floating point to the nearest integer.
    The `==` operator compares and returns a Boolean tensor, which can be converted
    to floating point numbers 1.0 and 0.0\. The `mean()` function will provide you
    the count of the number of 1’s (i.e., prediction matches the label) divided by
    the total number of samples. The `no_grad()` context is optional but suggested,
    so you relieve `y_pred` from remembering how it comes up with the number since
    you are not going to do differentiation on it.
  prefs: []
  type: TYPE_NORMAL
- en: Putting everything together, the following is the complete code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: You can copy all the code into your Python file and save it as “`pytorch_network.py`”
    in the same directory as your data file “`pima-indians-diabetes.csv`”. You can
    then run the Python file as a script from your command line.
  prefs: []
  type: TYPE_NORMAL
- en: Running this example, you should see that the training loop progresses on each
    epoch with the loss with the final accuracy printed last. Ideally, you would like
    the loss to go to zero and the accuracy to go to 1.0 (e.g., 100%). This is not
    possible for any but the most trivial machine learning problems. Instead, you
    will always have some error in your model. The goal is to choose a model configuration
    and training configuration that achieves the lowest loss and highest accuracy
    possible for a given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural networks are stochastic algorithms, meaning that the same algorithm
    on the same data can train a different model with different skill each time the
    code is run. This is a feature, not a bug. The variance in the performance of
    the model means that to get a reasonable approximation of how well your model
    is performing, you may need to fit it many times and calculate the average of
    the accuracy scores. For example, below are the accuracy scores from re-running
    the example five times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You can see that all accuracy scores are around 77%, roughly.
  prefs: []
  type: TYPE_NORMAL
- en: Make Predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can adapt the above example and use it to generate predictions on the training
    dataset, pretending it is a new dataset you have not seen before. Making predictions
    is as easy as calling the model as if it is a function. You are using a sigmoid
    activation function on the output layer so that the predictions will be a probability
    in the range between 0 and 1\. You can easily convert them into a crisp binary
    prediction for this classification task by rounding them. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternately, you can convert the probability into 0 or 1 to predict crisp classes
    directly; for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The complete example below makes predictions for each example in the dataset,
    then prints the input data, predicted class, and expected class for the first
    five examples in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This code uses a different way of building the model but should functionally
    be the same as before. After the model is trained, predictions are made for all
    examples in the dataset, and the input rows and predicted class value for the
    first five examples are printed and compared to the expected class value. You
    can see that most rows are correctly predicted. In fact, you can expect about
    77% of the rows to be correctly predicted based on your estimated performance
    of the model in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To learn more about deep learning and PyTorch, take a look at some of these:'
  prefs: []
  type: TYPE_NORMAL
- en: Books
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ian Goodfellow, Yoshua Bengio, and Aaron Courville. [Deep Learning](https://www.amazon.com/dp/0262035618).
    MIT Press, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ([Online version](http://www.deeplearningbook.org)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: APIs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PyTorch documentation](https://pytorch.org/docs/stable/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this post, you discovered how to create your first neural network model
    using PyTorch. Specifically, you learned the key steps in using PyTorch to create
    a neural network or deep learning model step by step, including:'
  prefs: []
  type: TYPE_NORMAL
- en: How to load data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to define a neural network in PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to train a model on data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to evaluate a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to make predictions with the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
