["```py\n# encoder representations of four different words\nword_1 = array([1, 0, 0])\nword_2 = array([0, 1, 0])\nword_3 = array([1, 1, 0])\nword_4 = array([0, 0, 1])\n```", "```py\n...\n# generating the weight matrices\nrandom.seed(42) # to allow us to reproduce the same attention values\nW_Q = random.randint(3, size=(3, 3))\nW_K = random.randint(3, size=(3, 3))\nW_V = random.randint(3, size=(3, 3))\n```", "```py\n...\n# generating the queries, keys and values\nquery_1 = word_1 @ W_Q\nkey_1 = word_1 @ W_K\nvalue_1 = word_1 @ W_V\n\nquery_2 = word_2 @ W_Q\nkey_2 = word_2 @ W_K\nvalue_2 = word_2 @ W_V\n\nquery_3 = word_3 @ W_Q\nkey_3 = word_3 @ W_K\nvalue_3 = word_3 @ W_V\n\nquery_4 = word_4 @ W_Q\nkey_4 = word_4 @ W_K\nvalue_4 = word_4 @ W_V\n```", "```py\n...\n# scoring the first query vector against all key vectors\nscores = array([dot(query_1, key_1), dot(query_1, key_2), dot(query_1, key_3), dot(query_1, key_4)])\n```", "```py\n...\n# computing the weights by a softmax operation\nweights = softmax(scores / key_1.shape[0] ** 0.5)\n```", "```py\n...\n# computing the attention by a weighted sum of the value vectors\nattention = (weights[0] * value_1) + (weights[1] * value_2) + (weights[2] * value_3) + (weights[3] * value_4)\n\nprint(attention)\n```", "```py\n[0.98522025 1.74174051 0.75652026]\n```", "```py\nfrom numpy import array\nfrom numpy import random\nfrom numpy import dot\nfrom scipy.special import softmax\n\n# encoder representations of four different words\nword_1 = array([1, 0, 0])\nword_2 = array([0, 1, 0])\nword_3 = array([1, 1, 0])\nword_4 = array([0, 0, 1])\n\n# stacking the word embeddings into a single array\nwords = array([word_1, word_2, word_3, word_4])\n\n# generating the weight matrices\nrandom.seed(42)\nW_Q = random.randint(3, size=(3, 3))\nW_K = random.randint(3, size=(3, 3))\nW_V = random.randint(3, size=(3, 3))\n\n# generating the queries, keys and values\nQ = words @ W_Q\nK = words @ W_K\nV = words @ W_V\n\n# scoring the query vectors against all key vectors\nscores = Q @ K.transpose()\n\n# computing the weights by a softmax operation\nweights = softmax(scores / K.shape[1] ** 0.5, axis=1)\n\n# computing the attention by a weighted sum of the value vectors\nattention = weights @ V\n\nprint(attention)\n```", "```py\n[[0.98522025 1.74174051 0.75652026]\n [0.90965265 1.40965265 0.5       ]\n [0.99851226 1.75849334 0.75998108]\n [0.99560386 1.90407309 0.90846923]]\n```"]