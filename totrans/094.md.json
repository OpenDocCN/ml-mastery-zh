["```py\nimport numpy as np\nx = np.linspace(-1, 1, 100)\ny = np.linspace(-2, 2, 100)\n```", "```py\nimport matplotlib.pyplot as plt \nimport numpy as np\n\nx = np.linspace(-1, 1, 100)\ny = np.linspace(-2, 2, 100)\n\n# convert vector into 2D arrays\nxx, yy = np.meshgrid(x,y)\n# computation on matching\nz = np.sqrt(1 - xx**2 - (yy/2)**2)\n\nfig = plt.figure(figsize=(8,8))\nax = plt.axes(projection='3d')\nax.set_xlim([-2,2])\nax.set_ylim([-2,2])\nax.set_zlim([0,2])\nax.plot_surface(xx, yy, z, cmap=\"cividis\")\nax.view_init(45, 35)\nplt.show()\n```", "```py\nfrom sklearn.datasets import load_digits\nimages = load_digits()[\"images\"]\nprint(images.shape)\n```", "```py\n(1797, 8, 8)\n```", "```py\n...\n\n# image has axes 0, 1, and 2, adding axis 3\nimages = np.expand_dims(images, 3)\nprint(images.shape)\n```", "```py\n(1797, 8, 8, 1)\n```", "```py\nimport numpy as np\n\nX = np.array([\n    [ 1.299,  0.332,  0.594, -0.047,  0.834],\n    [ 0.842,  0.441, -0.705, -1.086, -0.252],\n    [ 0.785,  0.478, -0.665, -0.532, -0.673],\n    [ 0.062,  1.228, -0.333,  0.867,  0.371]\n])\n```", "```py\n...\ny = (X > 0).all(axis=0)\nprint(y)\n```", "```py\narray([ True,  True, False, False, False])\n```", "```py\n...\ny = X[:, (X > 0).all(axis=0)\nprint(y)\n```", "```py\narray([[1.299, 0.332],\n       [0.842, 0.441],\n       [0.785, 0.478],\n       [0.062, 1.228]])\n```", "```py\n...\ny = X[:, [0,1,1,0]]\nprint(y)\n```", "```py\narray([[1.299, 0.332, 0.332, 1.299],\n       [0.842, 0.441, 0.441, 0.842],\n       [0.785, 0.478, 0.478, 0.785],\n       [0.062, 1.228, 1.228, 0.062]])\n```", "```py\nimport numpy as np\nfrom scipy.stats import multivariate_normal\nimport matplotlib.pyplot as plt\n\nmean = [0, 0]             # zero mean\ncov = [[1, 0.8],[0.8, 1]] # covariance matrix\nX1 = np.random.default_rng().multivariate_normal(mean, cov, 5000)\nX2 = multivariate_normal.rvs(mean, cov, 5000)\n\nfig = plt.figure(figsize=(12,6))\nax = plt.subplot(121)\nax.scatter(X1[:,0], X1[:,1], s=1)\nax.set_xlim([-4,4])\nax.set_ylim([-4,4])\nax.set_title(\"NumPy\")\n\nax = plt.subplot(122)\nax.scatter(X2[:,0], X2[:,1], s=1)\nax.set_xlim([-4,4])\nax.set_ylim([-4,4])\nax.set_title(\"SciPy\")\n\nplt.show()\n```", "```py\nfrom scipy.stats import norm\nn = norm.cdf([1,2,3,-1,-2,-3])\nprint(n)\nprint(n[:3] - n[-3:])\n```", "```py\n[0.84134475 0.97724987 0.9986501  0.15865525 0.02275013 0.0013499 ]\n[0.68268949 0.95449974 0.9973002 ]\n```", "```py\n...\nprint(norm.ppf(0.99))\n```", "```py\n2.3263478740408408\n```", "```py\npip install numba\n```", "```py\nimport numba\n\n@numba.jit(nopython=True)\ndef numpy_only_function(...)\n    ...\n```", "```py\nimport datetime\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numba\n\ndef tSNE(X, ndims=2, perplexity=30, seed=0, max_iter=500, stop_lying_iter=100, mom_switch_iter=400):\n    \"\"\"The t-SNE algorithm\n\n\tArgs:\n\t\tX: the high-dimensional coordinates\n\t\tndims: number of dimensions in output domain\n    Returns:\n        Points of X in low dimension\n    \"\"\"\n    momentum = 0.5\n    final_momentum = 0.8\n    eta = 200.0\n    N, _D = X.shape\n    np.random.seed(seed)\n\n    # normalize input\n    X -= X.mean(axis=0) # zero mean\n    X /= np.abs(X).max() # min-max scaled\n\n    # compute input similarity for exact t-SNE\n    P = computeGaussianPerplexity(X, perplexity)\n    # symmetrize and normalize input similarities\n    P = P + P.T\n    P /= P.sum()\n    # lie about the P-values\n    P *= 12.0\n    # initialize solution\n    Y = np.random.randn(N, ndims) * 0.0001\n    # perform main training loop\n    gains = np.ones_like(Y)\n    uY = np.zeros_like(Y)\n    for i in range(max_iter):\n        # compute gradient, update gains\n        dY = computeExactGradient(P, Y)\n        gains = np.where(np.sign(dY) != np.sign(uY), gains+0.2, gains*0.8).clip(0.1)\n        # gradient update with momentum and gains\n        uY = momentum * uY - eta * gains * dY\n        Y = Y + uY\n        # make the solution zero-mean\n        Y -= Y.mean(axis=0)\n        # Stop lying about the P-values after a while, and switch momentum\n        if i == stop_lying_iter:\n            P /= 12.0\n        if i == mom_switch_iter:\n            momentum = final_momentum\n        # print progress\n        if (i % 50) == 0:\n            C = evaluateError(P, Y)\n            now = datetime.datetime.now()\n            print(f\"{now} - Iteration {i}: Error = {C}\")\n    return Y\n\n@numba.jit(nopython=True)\ndef computeExactGradient(P, Y):\n    \"\"\"Gradient of t-SNE cost function\n\n\tArgs:\n        P: similarity matrix\n        Y: low-dimensional coordinates\n    Returns:\n        dY, a numpy array of shape (N,D)\n\t\"\"\"\n    N, _D = Y.shape\n    # compute squared Euclidean distance matrix of Y, the Q matrix, and the normalization sum\n    DD = computeSquaredEuclideanDistance(Y)\n    Q = 1/(1+DD)\n    sum_Q = Q.sum()\n    # compute gradient\n    mult = (P - (Q/sum_Q)) * Q\n    dY = np.zeros_like(Y)\n    for n in range(N):\n        for m in range(N):\n            if n==m: continue\n            dY[n] += (Y[n] - Y[m]) * mult[n,m]\n    return dY\n\n@numba.jit(nopython=True)\ndef evaluateError(P, Y):\n    \"\"\"Evaluate t-SNE cost function\n\n    Args:\n        P: similarity matrix\n        Y: low-dimensional coordinates\n    Returns:\n        Total t-SNE error C\n    \"\"\"\n    DD = computeSquaredEuclideanDistance(Y)\n    # Compute Q-matrix and normalization sum\n    Q = 1/(1+DD)\n    np.fill_diagonal(Q, np.finfo(np.float32).eps)\n    Q /= Q.sum()\n    # Sum t-SNE error: sum P log(P/Q)\n    error = P * np.log( (P + np.finfo(np.float32).eps) / (Q + np.finfo(np.float32).eps) )\n    return error.sum()\n\n@numba.jit(nopython=True)\ndef computeGaussianPerplexity(X, perplexity):\n    \"\"\"Compute Gaussian Perplexity\n\n    Args:\n        X: numpy array of shape (N,D)\n        perplexity: double\n    Returns:\n        Similarity matrix P\n    \"\"\"\n    # Compute the squared Euclidean distance matrix\n    N, _D = X.shape\n    DD = computeSquaredEuclideanDistance(X)\n    # Compute the Gaussian kernel row by row\n    P = np.zeros_like(DD)\n    for n in range(N):\n        found = False\n        beta = 1.0\n        min_beta = -np.inf\n        max_beta = np.inf\n        tol = 1e-5\n\n        # iterate until we get a good perplexity\n        n_iter = 0\n        while not found and n_iter < 200:\n            # compute Gaussian kernel row\n            P[n] = np.exp(-beta * DD[n])\n            P[n,n] = np.finfo(np.float32).eps\n            # compute entropy of current row\n            # Gaussians to be row-normalized to make it a probability\n            # then H = sum_i -P[i] log(P[i])\n            #        = sum_i -P[i] (-beta * DD[n] - log(sum_P))\n            #        = sum_i P[i] * beta * DD[n] + log(sum_P)\n            sum_P = P[n].sum()\n            H = beta * (DD[n] @ P[n]) / sum_P + np.log(sum_P)\n            # Evaluate if entropy within tolerance level\n            Hdiff = H - np.log2(perplexity)\n            if -tol < Hdiff < tol:\n                found = True\n                break\n            if Hdiff > 0:\n                min_beta = beta\n                if max_beta in (np.inf, -np.inf):\n                    beta *= 2\n                else:\n                    beta = (beta + max_beta) / 2\n            else:\n                max_beta = beta\n                if min_beta in (np.inf, -np.inf):\n                    beta /= 2\n                else:\n                    beta = (beta + min_beta) / 2\n            n_iter += 1\n        # normalize this row\n        P[n] /= P[n].sum()\n    assert not np.isnan(P).any()\n    return P\n\n@numba.jit(nopython=True)\ndef computeSquaredEuclideanDistance(X):\n    \"\"\"Compute squared distance\n    Args:\n        X: numpy array of shape (N,D)\n    Returns:\n        numpy array of shape (N,N) of squared distances\n    \"\"\"\n    N, _D = X.shape\n    DD = np.zeros((N,N))\n    for i in range(N-1):\n        for j in range(i+1, N):\n            diff = X[i] - X[j]\n            DD[j][i] = DD[i][j] = diff @ diff\n    return DD\n\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n# pick 1000 samples from the dataset\nrows = np.random.choice(X_test.shape[0], 1000, replace=False)\nX_data = X_train[rows].reshape(1000, -1).astype(\"float\")\nX_label = y_train[rows]\n# run t-SNE to transform into 2D and visualize in scatter plot\nY = tSNE(X_data, 2, 30, 0, 500, 100, 400)\nplt.figure(figsize=(8,8))\nplt.scatter(Y[:,0], Y[:,1], c=X_label)\nplt.show()\n```"]