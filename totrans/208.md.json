["```py\nfrom pandas import read_csv\nimport numpy as np\nfrom keras import Model\nfrom keras.layers import Layer\nimport keras.backend as K\nfrom keras.layers import Input, Dense, SimpleRNN\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.metrics import mean_squared_error\n```", "```py\ndef get_fib_seq(n, scale_data=True):\n    # Get the Fibonacci sequence\n    seq = np.zeros(n)\n    fib_n1 = 0.0\n    fib_n = 1.0 \n    for i in range(n):\n            seq[i] = fib_n1 + fib_n\n            fib_n1 = fib_n\n            fib_n = seq[i] \n    scaler = []\n    if scale_data:\n        scaler = MinMaxScaler(feature_range=(0, 1))\n        seq = np.reshape(seq, (n, 1))\n        seq = scaler.fit_transform(seq).flatten()        \n    return seq, scaler\n\nfib_seq = get_fib_seq(10, False)[0]\nprint(fib_seq)\n```", "```py\n[ 1\\.  2\\.  3\\.  5\\.  8\\. 13\\. 21\\. 34\\. 55\\. 89.]\n```", "```py\ndef get_fib_XY(total_fib_numbers, time_steps, train_percent, scale_data=True):\n    dat, scaler = get_fib_seq(total_fib_numbers, scale_data)    \n    Y_ind = np.arange(time_steps, len(dat), 1)\n    Y = dat[Y_ind]\n    rows_x = len(Y)\n    X = dat[0:rows_x]\n    for i in range(time_steps-1):\n        temp = dat[i+1:rows_x+i+1]\n        X = np.column_stack((X, temp))\n    # random permutation with fixed seed   \n    rand = np.random.RandomState(seed=13)\n    idx = rand.permutation(rows_x)\n    split = int(train_percent*rows_x)\n    train_ind = idx[0:split]\n    test_ind = idx[split:]\n    trainX = X[train_ind]\n    trainY = Y[train_ind]\n    testX = X[test_ind]\n    testY = Y[test_ind]\n    trainX = np.reshape(trainX, (len(trainX), time_steps, 1))    \n    testX = np.reshape(testX, (len(testX), time_steps, 1))\n    return trainX, trainY, testX, testY, scaler\n\ntrainX, trainY, testX, testY, scaler = get_fib_XY(12, 3, 0.7, False)\nprint('trainX = ', trainX)\nprint('trainY = ', trainY)\n```", "```py\ntrainX =  [[[ 8.]\n  [13.]\n  [21.]]\n\n [[ 5.]\n  [ 8.]\n  [13.]]\n\n [[ 2.]\n  [ 3.]\n  [ 5.]]\n\n [[13.]\n  [21.]\n  [34.]]\n\n [[21.]\n  [34.]\n  [55.]]\n\n [[34.]\n  [55.]\n  [89.]]]\ntrainY =  [ 34\\.  21\\.   8\\.  55\\.  89\\. 144.]\n```", "```py\n# Set up parameters\ntime_steps = 20\nhidden_units = 2\nepochs = 30\n\n# Create a traditional RNN network\ndef create_RNN(hidden_units, dense_units, input_shape, activation):\n    model = Sequential()\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape, activation=activation[0]))\n    model.add(Dense(units=dense_units, activation=activation[1]))\n    model.compile(loss='mse', optimizer='adam')\n    return model\n\nmodel_RNN = create_RNN(hidden_units=hidden_units, dense_units=1, input_shape=(time_steps,1), \n                   activation=['tanh', 'tanh'])\nmodel_RNN.summary()\n```", "```py\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nsimple_rnn_3 (SimpleRNN)     (None, 2)                 8         \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 3         \n=================================================================\nTotal params: 11\nTrainable params: 11\nNon-trainable params: 0\n```", "```py\n# Generate the dataset\ntrainX, trainY, testX, testY, scaler  = get_fib_XY(1200, time_steps, 0.7)\n\nmodel_RNN.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)\n\n# Evalute model\ntrain_mse = model_RNN.evaluate(trainX, trainY)\ntest_mse = model_RNN.evaluate(testX, testY)\n\n# Print error\nprint(\"Train set MSE = \", train_mse)\nprint(\"Test set MSE = \", test_mse)\n```", "```py\nTrain set MSE =  5.631405292660929e-05\nTest set MSE =  2.623497312015388e-05\n```", "```py\n# Add attention layer to the deep learning network\nclass attention(Layer):\n    def __init__(self,**kwargs):\n        super(attention,self).__init__(**kwargs)\n\n    def build(self,input_shape):\n        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n                               initializer='random_normal', trainable=True)\n        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n                               initializer='zeros', trainable=True)        \n        super(attention, self).build(input_shape)\n\n    def call(self,x):\n        # Alignment scores. Pass them through tanh function\n        e = K.tanh(K.dot(x,self.W)+self.b)\n        # Remove dimension of size 1\n        e = K.squeeze(e, axis=-1)   \n        # Compute the weights\n        alpha = K.softmax(e)\n        # Reshape to tensorFlow format\n        alpha = K.expand_dims(alpha, axis=-1)\n        # Compute the context vector\n        context = x * alpha\n        context = K.sum(context, axis=1)\n        return context\n```", "```py\ndef create_RNN_with_attention(hidden_units, dense_units, input_shape, activation):\n    x=Input(shape=input_shape)\n    RNN_layer = SimpleRNN(hidden_units, return_sequences=True, activation=activation)(x)\n    attention_layer = attention()(RNN_layer)\n    outputs=Dense(dense_units, trainable=True, activation=activation)(attention_layer)\n    model=Model(x,outputs)\n    model.compile(loss='mse', optimizer='adam')    \n    return model    \n\nmodel_attention = create_RNN_with_attention(hidden_units=hidden_units, dense_units=1, \n                                  input_shape=(time_steps,1), activation='tanh')\nmodel_attention.summary()\n```", "```py\nModel: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 20, 1)]           0         \n_________________________________________________________________\nsimple_rnn_2 (SimpleRNN)     (None, 20, 2)             8         \n_________________________________________________________________\nattention_1 (attention)      (None, 2)                 22        \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 3         \n=================================================================\nTotal params: 33\nTrainable params: 33\nNon-trainable params: 0\n_________________________________________________________________\n```", "```py\nmodel_attention.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)\n\n# Evalute model\ntrain_mse_attn = model_attention.evaluate(trainX, trainY)\ntest_mse_attn = model_attention.evaluate(testX, testY)\n\n# Print error\nprint(\"Train set MSE with attention = \", train_mse_attn)\nprint(\"Test set MSE with attention = \", test_mse_attn)\n```", "```py\nTrain set MSE with attention =  5.3511179430643097e-05\nTest set MSE with attention =  9.053358553501312e-06\n```", "```py\nfrom pandas import read_csv\nimport numpy as np\nfrom keras import Model\nfrom keras.layers import Layer\nimport keras.backend as K\nfrom keras.layers import Input, Dense, SimpleRNN\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.metrics import mean_squared_error\n\n# Prepare data\ndef get_fib_seq(n, scale_data=True):\n    # Get the Fibonacci sequence\n    seq = np.zeros(n)\n    fib_n1 = 0.0\n    fib_n = 1.0 \n    for i in range(n):\n            seq[i] = fib_n1 + fib_n\n            fib_n1 = fib_n\n            fib_n = seq[i] \n    scaler = []\n    if scale_data:\n        scaler = MinMaxScaler(feature_range=(0, 1))\n        seq = np.reshape(seq, (n, 1))\n        seq = scaler.fit_transform(seq).flatten()        \n    return seq, scaler\n\ndef get_fib_XY(total_fib_numbers, time_steps, train_percent, scale_data=True):\n    dat, scaler = get_fib_seq(total_fib_numbers, scale_data)    \n    Y_ind = np.arange(time_steps, len(dat), 1)\n    Y = dat[Y_ind]\n    rows_x = len(Y)\n    X = dat[0:rows_x]\n    for i in range(time_steps-1):\n        temp = dat[i+1:rows_x+i+1]\n        X = np.column_stack((X, temp))\n    # random permutation with fixed seed   \n    rand = np.random.RandomState(seed=13)\n    idx = rand.permutation(rows_x)\n    split = int(train_percent*rows_x)\n    train_ind = idx[0:split]\n    test_ind = idx[split:]\n    trainX = X[train_ind]\n    trainY = Y[train_ind]\n    testX = X[test_ind]\n    testY = Y[test_ind]\n    trainX = np.reshape(trainX, (len(trainX), time_steps, 1))    \n    testX = np.reshape(testX, (len(testX), time_steps, 1))\n    return trainX, trainY, testX, testY, scaler\n\n# Set up parameters\ntime_steps = 20\nhidden_units = 2\nepochs = 30\n\n# Create a traditional RNN network\ndef create_RNN(hidden_units, dense_units, input_shape, activation):\n    model = Sequential()\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape, activation=activation[0]))\n    model.add(Dense(units=dense_units, activation=activation[1]))\n    model.compile(loss='mse', optimizer='adam')\n    return model\n\nmodel_RNN = create_RNN(hidden_units=hidden_units, dense_units=1, input_shape=(time_steps,1), \n                   activation=['tanh', 'tanh'])\n\n# Generate the dataset for the network\ntrainX, trainY, testX, testY, scaler  = get_fib_XY(1200, time_steps, 0.7)\n# Train the network\nmodel_RNN.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)\n\n# Evalute model\ntrain_mse = model_RNN.evaluate(trainX, trainY)\ntest_mse = model_RNN.evaluate(testX, testY)\n\n# Print error\nprint(\"Train set MSE = \", train_mse)\nprint(\"Test set MSE = \", test_mse)\n\n# Add attention layer to the deep learning network\nclass attention(Layer):\n    def __init__(self,**kwargs):\n        super(attention,self).__init__(**kwargs)\n\n    def build(self,input_shape):\n        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n                               initializer='random_normal', trainable=True)\n        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n                               initializer='zeros', trainable=True)        \n        super(attention, self).build(input_shape)\n\n    def call(self,x):\n        # Alignment scores. Pass them through tanh function\n        e = K.tanh(K.dot(x,self.W)+self.b)\n        # Remove dimension of size 1\n        e = K.squeeze(e, axis=-1)   \n        # Compute the weights\n        alpha = K.softmax(e)\n        # Reshape to tensorFlow format\n        alpha = K.expand_dims(alpha, axis=-1)\n        # Compute the context vector\n        context = x * alpha\n        context = K.sum(context, axis=1)\n        return context\n\ndef create_RNN_with_attention(hidden_units, dense_units, input_shape, activation):\n    x=Input(shape=input_shape)\n    RNN_layer = SimpleRNN(hidden_units, return_sequences=True, activation=activation)(x)\n    attention_layer = attention()(RNN_layer)\n    outputs=Dense(dense_units, trainable=True, activation=activation)(attention_layer)\n    model=Model(x,outputs)\n    model.compile(loss='mse', optimizer='adam')    \n    return model    \n\n# Create the model with attention, train and evaluate\nmodel_attention = create_RNN_with_attention(hidden_units=hidden_units, dense_units=1, \n                                  input_shape=(time_steps,1), activation='tanh')\nmodel_attention.summary()    \n\nmodel_attention.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)\n\n# Evalute model\ntrain_mse_attn = model_attention.evaluate(trainX, trainY)\ntest_mse_attn = model_attention.evaluate(testX, testY)\n\n# Print error\nprint(\"Train set MSE with attention = \", train_mse_attn)\nprint(\"Test set MSE with attention = \", test_mse_attn)\n```"]