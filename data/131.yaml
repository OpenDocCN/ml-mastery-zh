- en: Loss Functions in PyTorch Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 模型中的损失函数
- en: 原文：[https://machinelearningmastery.com/loss-functions-in-pytorch-models/](https://machinelearningmastery.com/loss-functions-in-pytorch-models/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/loss-functions-in-pytorch-models/](https://machinelearningmastery.com/loss-functions-in-pytorch-models/)
- en: The loss metric is very important for neural networks. As all machine learning
    models are one optimization problem or another, the loss is the objective function
    to minimize. In neural networks, the optimization is done with gradient descent
    and backpropagation. But what are loss functions, and how are they affecting your
    neural networks?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 损失指标对神经网络非常重要。由于所有机器学习模型都是一种优化问题，损失函数就是要最小化的目标函数。在神经网络中，优化是通过梯度下降和反向传播来完成的。但是什么是损失函数，它们又如何影响您的神经网络？
- en: 'In this chapter, you will learn what loss functions are and delve into some
    commonly used loss functions and how you can apply them to your neural networks.
    After reading this chapter, you will learn:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，您将了解什么是损失函数，并深入了解一些常用的损失函数以及如何将它们应用于您的神经网络中。阅读完本章后，您将学到：
- en: What are loss functions, and their role in training neural network models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是损失函数，以及它们在训练神经网络模型中的作用
- en: Common loss functions for regression and classification problems
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归和分类问题的常见损失函数
- en: How to use loss functions in your PyTorch model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在您的PyTorch模型中使用损失函数
- en: '**Kick-start your project** with my book [Deep Learning with PyTorch](https://machinelearningmastery.com/deep-learning-with-pytorch/).
    It provides **self-study tutorials** with **working code**.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**用我的书[《深度学习与PyTorch》](https://machinelearningmastery.com/deep-learning-with-pytorch/)启动你的项目**。它提供了**自学教程**和**可运行的代码**。'
- en: Let’s get started!![](../Images/e49e5629fd95d6ec4afd835b8e3415cb.png)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧!![](../Images/e49e5629fd95d6ec4afd835b8e3415cb.png)
- en: Loss Functions in PyTorch Models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 模型中的损失函数。
- en: Photo by [Hans Vivek](https://unsplash.com/photos/_aXtuc7tB00). Some rights
    reserved.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Hans Vivek](https://unsplash.com/photos/_aXtuc7tB00)拍摄。部分权利保留。
- en: Overview
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简介
- en: 'This post is divided into four sections; they are:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文分为四个部分；它们是：
- en: What Are Loss Functions?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是损失函数？
- en: Loss Functions for Regression
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归的损失函数
- en: Loss Functions for Classification
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类问题的损失函数
- en: Custom Loss Function in PyTorch
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 中的自定义损失函数
- en: What Are Loss Functions?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是损失函数？
- en: In neural networks, loss functions help optimize the performance of the model.
    They are usually used to measure some penalty that the model incurs on its predictions,
    such as the deviation of the prediction away from the ground truth label. Loss
    functions are usually differentiable across their domain (but it is allowed that
    the gradient is undefined only for very specific points, such as $x=0$, which
    is basically ignored in practice). In the training loop, they are differentiated
    with respect to parameters, and these gradients are used for your backpropagation
    and gradient descent steps to optimize your model on the training set.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，损失函数有助于优化模型的性能。它们通常用于衡量模型在预测中所产生的一些惩罚，例如预测与真实标签之间的偏差。损失函数通常在其定义域上是可微的（但允许在非常特定的点上梯度未定义，例如$x=0$，实际上在实践中被忽略）。在训练循环中，损失函数相对于参数进行微分，并且这些梯度用于您的反向传播和梯度下降步骤，以优化训练集上的模型。
- en: Loss functions are also slightly different from metrics. While loss functions
    can tell you the performance of our model, they might not be of direct interest
    or easily explainable by humans. This is where metrics come in. Metrics such as
    accuracy are much more useful for humans to understand the performance of a neural
    network even though they might not be good choices for loss functions since they
    might not be differentiable.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数也略有不同于度量标准。虽然损失函数可以告诉您我们模型的性能，但可能并不直接吸引或易于人类解释。这就是度量标准的作用。例如准确度对于人类来理解神经网络的性能要更有用，尽管它们可能不是损失函数的好选择，因为它们可能不可微分。
- en: In the following, let’s explore some common loss functions, for regression problems
    and for classification problems.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探讨一些常见的损失函数，用于回归问题和分类问题。
- en: Want to Get Started With Deep Learning with PyTorch?
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 想要开始使用PyTorch进行深度学习吗？
- en: Take my free email crash course now (with sample code).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在就来参加我的免费电子邮件崩溃课程（附有示例代码）。
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 点击注册，还可以获得课程的免费PDF电子书版本。
- en: Loss Functions for Regression
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归的损失函数
- en: In regression problems, the model is to predict a value in a continuous range.
    Too good to be true that your model can predict the exact value all the time,
    but it is good enough if the value is close enough. Therefore, you need a loss
    function to measure how close it is. The farther away from the exact value, the
    more the loss is your prediction.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归问题中，模型的目标是预测一个连续范围内的值。你的模型能一直准确预测到具体值几乎是不现实的，但如果预测值足够接近就已经很好了。因此，你需要一个损失函数来衡量其接近程度。距离准确值越远，预测的损失就越大。
- en: One simple function is just to measure the difference between the prediction
    and the target value. You do not care the value is greater than or less than the
    target value in finding the difference. Hence, in mathematics, we find $\dfrac{1}{m}\sum_{i=1}^m
    \vert \hat{y}_i – y_i\vert$ with $m$ the number of training examples whereas $y_i$
    and $\hat{y}_i$ are the target and predicted values, respectively, averaged over
    all training examples. This is the mean absolute error (MAE).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的函数就是测量预测值与目标值之间的差异。在计算差异时，你不在乎值是否大于或小于目标值。因此，在数学上，我们找到$\dfrac{1}{m}\sum_{i=1}^m
    \vert \hat{y}_i – y_i\vert$，其中$m$是训练样本的数量，而$y_i$和$\hat{y}_i$分别是目标值和预测值，对所有训练样本取平均。这就是平均绝对误差（MAE）。
- en: The MAE is never negative and would be zero only if the prediction matched the
    ground truth perfectly. It is an intuitive loss function and might also be used
    as one of your metrics, specifically for regression problems, since you want to
    minimize the error in your predictions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 平均绝对误差（MAE）永远不会为负，仅当预测值与实际值完全匹配时才为零。它是一个直观的损失函数，也可以作为你的一个指标，特别是在回归问题中，因为你希望最小化预测中的误差。
- en: However, absolute value is not differentiable at 0\. It is not really a problem
    because you rarely hitting that value. But sometimes people would prefer to use
    mean square error (MSE) instead. MSE equals to $\dfrac{1}{m}\sum_{i=1}^m (\hat{y}_i
    – y_i)^2$, which is similar to MAE but use square function in place of absolute
    value.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，绝对值在0处不可导。这并不是一个大问题，因为你很少会碰到这个值。但有时候，人们更倾向于使用均方误差（MSE）来代替。MSE等于$\dfrac{1}{m}\sum_{i=1}^m
    (\hat{y}_i – y_i)^2$，它类似于MAE，但使用平方函数代替绝对值。
- en: It also measures the deviation of the predicted value from the target value.
    However, the MSE squares this difference (always non-negative since squares of
    real numbers are always non-negative), which gives it slightly different properties.
    One property is that the mean squared error favors a large number of small errors
    over a small number of large errors, which leads to models with fewer outliers
    or at least outliers that are less severe than models trained with a MAE. This
    is because a large error would have a significantly larger impact on the error
    and, consequently, the gradient of the error when compared to a small error.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 它也衡量了预测值与目标值的偏差。然而，MSE对这种差异进行平方处理（始终为非负，因为实数的平方始终为非负），这赋予了它略微不同的特性。一个特性是，均方误差更倾向于大量小误差而不是少量大误差，这导致了具有较少离群值或至少比用MAE训练的模型离群值较少的模型。这是因为大误差对误差和误差梯度的影响会显著大于小误差。
- en: 'Let’s look at what the mean absolute error and mean square error loss function
    looks like graphically:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看平均绝对误差和均方误差损失函数在图形上的表现：
- en: '![](../Images/30c8a20fd3581188d63b4a8d657639e4.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30c8a20fd3581188d63b4a8d657639e4.png)'
- en: Mean absolute error loss function (blue) and gradient (orange)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 平均绝对误差损失函数（蓝色）和梯度（橙色）
- en: '![](../Images/f6ae01c6f242d10f710fec00562228fb.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6ae01c6f242d10f710fec00562228fb.png)'
- en: Mean square error loss function (blue) and gradient (orange)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差损失函数（蓝色）和梯度（橙色）
- en: Similar to activation functions, you might also be interested in what the gradient
    of the loss function looks like since you are using the gradient later to do backpropagation
    to train your model’s parameters. You should see that in MSE, larger errors would
    lead to a larger magnitude for the gradient and a larger loss. Hence, for example,
    two training examples that deviate from their ground truths by 1 unit would lead
    to a loss of 2, while a single training example that deviates from its ground
    truth by 2 units would lead to a loss of 4, hence having a larger impact. This
    is not the case in MAE.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于激活函数，您可能也对损失函数的梯度感兴趣，因为稍后您将使用梯度来进行反向传播来训练模型的参数。您应该看到，在 MSE 中，较大的错误会导致梯度的幅度更大和更大的损失。因此，例如，两个训练示例与其地面真实值相差
    1 单位将导致损失为 2，而单个训练示例与其地面真实值相差 2 单位将导致损失为 4，因此产生更大的影响。在 MAE 中并非如此。
- en: 'In PyTorch, you can create MAE and MSE as loss functions using `nn.L1Loss()`
    and `nn.MSELoss()` respectively. It is named as L1 because the computation of
    MAE is also called the L1-norm in mathematics. Below is an example of computing
    the MAE and MSE between two vectors:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，您可以使用 `nn.L1Loss()` 和 `nn.MSELoss()` 创建 MAE 和 MSE 作为损失函数。它之所以称为
    L1，是因为在数学上计算 MAE 也被称为 L1-范数。以下是计算两个向量之间的 MAE 和 MSE 的示例：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You should get
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该得到
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: MAE is 2.0 because $\frac{1}{2}[\vert 0-1\vert + \vert 3-0\vert]=\frac{1}{2}(1+3)=2$
    whereas MSE is 5.0 because $\frac{1}{2}[(0-1)^2 + (3-0)^2]=\frac{1}{2}(1+9)=5$.
    Notice that in MSE, the second example with a predicted value of 3 and actual
    value of 0 contributes 90% of the error under the mean squared error vs. 75% under
    the mean absolute error.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: MAE 是 2.0，因为 $\frac{1}{2}[\vert 0-1\vert + \vert 3-0\vert]=\frac{1}{2}(1+3)=2$
    而 MSE 是 5.0，因为 $\frac{1}{2}[(0-1)^2 + (3-0)^2]=\frac{1}{2}(1+9)=5$。注意，在 MSE 中，第二个预测值为
    3，实际值为 0 的示例在均方误差下贡献了 90% 的错误，而在平均绝对误差下为 75%。
- en: Sometimes, you may see people use root mean squared error (RMSE) as a metric.
    This will take the square root of MSE. From the perspective of a loss function,
    MSE and RMSE are equivalent. But from the perspective of the value, the RMSE is
    in the same unit as the predicted values. If your prediction is money in dollars,
    both MAE and RMSE give you how much your prediction is away from the true value
    in dollars in average. But MSE is in unit of squared dollars, which its physical
    meaning is not intuitive.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，您可能会看到人们使用均方根误差（RMSE）作为度量标准。这将取 MSE 的平方根。从损失函数的角度来看，MSE 和 RMSE 是等价的。但从值的角度来看，RMSE
    与预测值在同一单位中。如果您的预测是美元金额，MAE 和 RMSE 都会告诉您预测值与真实值的平均偏差是多少美元。但 MSE 的单位是平方美元，其物理含义不直观。
- en: Loss Functions for Classification
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类的损失函数
- en: For classification problems, there is a small, discrete set of numbers that
    the output could take. Furthermore, the number used to label-encode the classes
    is arbitrary and with no semantic meaning (e.g., using the labels 0 for cat, 1
    for dog, and 2 for horse does not represent that a dog is half cat and half horse).
    Therefore, it should not have an impact on the performance of the model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，输出可能取的值是一个小的离散集合。此外，用于标签编码类别的数字是任意的，没有语义含义（例如，使用标签 0 表示猫，1 表示狗，2 表示马，并不表示狗是猫和马的一半）。因此，这不应对模型的性能产生影响。
- en: In a classification problem, the model’s output is usually a vector of probability
    for each category. Often, this vector is usually expected to be “logits,” i.e.,
    real numbers to be transformed to probability using the softmax function, or the
    output of a softmax activation function.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，模型的输出通常是每个类别的概率向量。通常，这个向量被期望是“logits”，即实数，通过 softmax 函数转换为概率，或者是 softmax
    激活函数的输出。
- en: 'The cross-entropy between two probability distributions is a measure of the
    difference between the two probability distributions. Precisely, it is $−\sum_i
    P(X=x_i)\log Q(X=x_i)$ for probability $P$ and $Q$. In machine learning, we usually
    have the probability $P$ provided by the training data and $Q$ predicted by the
    model, which $P$ is 1 for the correct class and 0 for every other class. The predicted
    probability $Q$, however, is usually a floating point valued between 0 and 1\.
    Hence when used for classification problems in machine learning, this formula
    can be simplified into:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 两个概率分布之间的交叉熵是衡量两个概率分布之间差异的度量。准确地说，它是 $−\sum_i P(X=x_i)\log Q(X=x_i)$，其中 $P$
    和 $Q$ 是概率。在机器学习中，我们通常有训练数据提供的概率 $P$ 和模型预测的概率 $Q$，其中 $P$ 对于正确的类别为 1，对于其他所有类别为 0。然而，预测的概率
    $Q$ 通常是介于 0 和 1 之间的浮点值。因此，当用于机器学习中的分类问题时，这个公式可以简化为：
- en: $$\text{categorical cross-entropy} = − \log p_{\text{target}}$$
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: $$\text{categorical cross-entropy} = − \log p_{\text{target}}$$
- en: where $p_{\text{target}}$ is the model-predicted probability of the groud truth
    class for that particular sample.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p_{\text{target}}$ 是模型预测的该样本的真实类别的概率。
- en: Cross-entropy metrics have a negative sign because $\log(x)$ tends to negative
    infinity as $x$ tends to zero. We want a higher loss when the probability approaches
    0 and a lower loss when the probability approaches 1\. Graphically,
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵度量有一个负号，因为 $\log(x)$ 当 $x$ 趋近于零时趋向于负无穷。我们希望在概率接近 0 时损失更高，而在概率接近 1 时损失更低。图形上，
- en: '![](../Images/5b11c08925dab718c9ec2ce103f1b1ff.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b11c08925dab718c9ec2ce103f1b1ff.png)'
- en: Categorical cross-entropy loss function (blue) and gradient (orange)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 分类交叉熵损失函数（蓝色）和梯度（橙色）
- en: Notice that the loss is exactly 0 if the probability of the ground truth class
    is 1 as desired. Also, as the probability of the ground truth class tends to 0,
    the loss tends to positive infinity as well, hence substantially penalizing bad
    predictions. You might recognize this loss function for logistic regression, which
    is similar except the logistic regression loss is specific to the case of binary
    classes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果真实类别的概率为 1，则损失恰好为 0，这符合预期。此外，当真实类别的概率趋近于 0 时，损失也趋向于正无穷，从而显著惩罚不良预测。你可能会认识到这个损失函数用于逻辑回归，逻辑回归损失类似，不过逻辑回归损失特定于二分类情况。
- en: Looking at the gradient, you can see that the gradient is generally negative,
    which is also expected since, to decrease this loss, you would want the probability
    on the ground truth class to be as high as possible. Recall that gradient descent
    goes in the opposite direction of the gradient.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从梯度来看，你可以看到梯度通常是负的，这也是预期的，因为为了减少这个损失，你会希望真实类别的概率尽可能高。回忆一下，梯度下降是朝着梯度的反方向进行的。
- en: 'In PyTorch, the cross-entropy function is provided by `nn.CrossEntropyLoss()`.
    It takes the predicted logits and the target as parameter and compute the categorical
    cross-entropy. Remind that inside the `CrossEntropyLoss()` function, softmax will
    be applied to the logits hence you should not use softmax activation function
    at the output layer. Example of using the cross entropy loss function from PyTorch
    is as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，交叉熵函数由 `nn.CrossEntropyLoss()` 提供。它接收预测的 logits 和目标作为参数，并计算分类交叉熵。请注意，在
    `CrossEntropyLoss()` 函数内部，softmax 将被应用于 logits，因此你不应该在输出层使用 softmax 激活函数。使用 PyTorch
    交叉熵损失函数的示例如下：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'It prints:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 它打印：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Note the first argument to the cross entropy loss function is logit, not probabilities.
    Hence each row does not sum to 1\. The second argument, however, is a tensor containing
    rows of probabilities. If you convert the `logits` tensor above into probability
    using softmax function, it would be:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 注意交叉熵损失函数的第一个参数是 logit，而不是概率。因此，每一行的和不等于 1。第二个参数是一个包含概率行的张量。如果你将上面的 `logits`
    张量通过 softmax 函数转换为概率，它将是：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: which each row sums to 1.0\. This tensor also reveals why the cross entropy
    above calculated to be 0.288, which is $-\log(0.75)$.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行的和为 1.0。这个张量也揭示了为什么上面计算的交叉熵为 0.288，即 $-\log(0.75)$。
- en: 'The other way of calculating the cross entropy in PyTorch is not to use one-hot
    encoding in the target but to use the integer indices label:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中计算交叉熵的另一种方法是使用整数索引标签，而不是在目标中使用独热编码：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This gives you the same cross entropy of 0.288\. Note that,
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了相同的 0.288 的交叉熵。注意，
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'gives you:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 给你：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is how PyTorch interprets your target tensor. It is also called “sparse
    cross entropy” function in other libraries, to make a distinction that it does
    not expect a one-hot vector.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 PyTorch 解释目标张量的方式。在其他库中，它也称为“稀疏交叉熵”函数，以区分它不期望一个独热向量。
- en: 'Note in PyTorch, you can use `nn.LogSoftmax()` as an activation function. It
    is to apply softmax on the output of a layer and than take the logarithm on each
    element. If this is your output layer, you should use `nn.NLLLoss()` (negative
    log likelihood) as the loss function. Mathematically these duo is same as cross
    entropy loss. You can confirm this by checking the code below produced the same
    output:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在 PyTorch 中，您可以使用 `nn.LogSoftmax()` 作为激活函数。它是在层的输出上应用 softmax，然后对每个元素取对数。如果这是您的输出层，您应该使用
    `nn.NLLLoss()`（负对数似然）作为损失函数。数学上，这两者与交叉熵损失相同。您可以通过检查下面的代码确认这一点：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In case of a classification problem with only two classes, it becomes binary
    classification. It is special because the model is now a logistic regression model
    in which there can be only one output instead of a vector of two values. You can
    still implement binary classification as multiclass classification and use the
    same cross entropy function. But if you output $x$ as the probability (between
    0 and 1) for the “positive class”, it is known that the probability for the “negative
    class” must be $1-x$.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于仅有两类的分类问题，它变成了二元分类。这很特殊，因为模型现在是一个逻辑回归模型，其中只能有一个输出，而不是一个包含两个值的向量。您仍然可以将二元分类实现为多类分类，并使用相同的交叉熵函数。但如果您将
    $x$ 输出作为“正类”的概率（介于 0 和 1 之间），则已知“负类”的概率必须是 $1-x$。
- en: 'In PyTorch, you have `nn.BCELoss()` for binary cross entropy. It is specialized
    for binary case. For example:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，您有 `nn.BCELoss()` 用于二元交叉熵。它专门用于二分类情况。例如：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This gives you:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'It is because:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为：
- en: $$\frac{1}{2}[-\log(0.75) + (-\log(1-0.25))] = -\log(0.75) = 0.288$$
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: $$\frac{1}{2}[-\log(0.75) + (-\log(1-0.25))] = -\log(0.75) = 0.288$$
- en: Note that in PyTorch, the target label 1 is taken as the “positive class” and
    label 0 is the “negative class”. There should not be other values in the target
    tensor.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在 PyTorch 中，目标标签 1 被视为“正类”，标签 0 被视为“负类”。目标张量中不应包含其他值。
- en: Custom Loss Function in PyTorch
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 PyTorch 中自定义损失函数
- en: Notice in above, the loss metric is calculated using an object from `torch.nn`
    module. The loss metric computed is a PyTorch tensor, so you can differentiate
    it and start the backpropagation. Therefore, nothing forbid you from creating
    your own loss function as long as you can compute a tensor based on the model’s
    output.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意上面，损失度量是使用 `torch.nn` 模块中的对象计算的。计算的损失度量是一个 PyTorch 张量，因此您可以对其进行求导并开始反向传播。因此，只要您能基于模型的输出计算张量，就没有什么可以阻止您创建自己的损失函数。
- en: 'PyTorch does not give you all the possible loss metrics. For example, mean
    absolute percentage error is not included. It is like MAE, defined as:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 不会提供所有可能的损失度量。例如，平均绝对百分比误差未包含在内。它类似于 MAE，定义如下：
- en: $$\text{MAPE} = \frac{1}{m} \sum_{i=1}^m \lvert\frac{\hat{y}_i – y_i}{y_i}\rvert$$
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: $$\text{MAPE} = \frac{1}{m} \sum_{i=1}^m \lvert\frac{\hat{y}_i – y_i}{y_i}\rvert$$
- en: Sometimes you may prefer to use MAPE. Recall the example on regression on the
    California housing dataset, the prediction is on the house price. It may make
    more sense to consider prediction accurate based on percentage difference rather
    than dollar difference. You can define your MAPE function, just remember to use
    PyTorch functions to compute, and return a PyTorch tensor.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有时您可能更喜欢使用 MAPE。回想一下对加利福尼亚住房数据集的回归示例，预测是房价。考虑到百分比差异而不是美元差异可能更合理。您可以定义自己的 MAPE
    函数，只需记住使用 PyTorch 函数计算，并返回一个 PyTorch 张量。
- en: 'See the full example below:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 查看下面的完整示例：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Compare to the example in the other post, you can see that `loss_fn` now is
    defined as a custom function. Otherwise everything is just the same.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他帖子中的示例相比，您可以看到 `loss_fn` 现在被定义为一个自定义函数。否则，一切都是完全相同的。
- en: Further Readings
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Below are the documentations from PyTorch that give you more details on how
    the various loss functions are implemented:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 PyTorch 提供的文档，详细介绍了各种损失函数的实现：
- en: '[nn.L1Loss](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html)
    from PyTorch'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[nn.L1Loss](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html)
    来自 PyTorch'
- en: '[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)
    from PyTorch'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)
    来自 PyTorch'
- en: '[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)
    from PyTorch'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)
    来自 PyTorch'
- en: '[nn.BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)
    from PyTorch'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[nn.BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)
    来自 PyTorch'
- en: '[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)
    from PyTorch'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)
    来自 PyTorch'
- en: Summary
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this post, you have seen loss functions and the role that they play in a
    neural network. You have also seen some popular loss functions used in regression
    and classification models, as well as how to implement your own loss function
    for your PyTorch model. Specifically, you learned:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，你了解了损失函数及其在神经网络中的作用。你还了解了一些在回归和分类模型中使用的流行损失函数，以及如何为你的 PyTorch 模型实现自己的损失函数。具体来说，你学到了：
- en: What are loss functions, and why they are important in training
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是损失函数，它们在训练中的重要性
- en: Common loss functions for regression and classification problems
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于回归和分类问题的常见损失函数
- en: How to use loss functions in your PyTorch model
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在 PyTorch 模型中使用损失函数
