- en: Building Multilayer Perceptron Models in PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在PyTorch中构建多层感知器模型
- en: 原文：[https://machinelearningmastery.com/building-multilayer-perceptron-models-in-pytorch/](https://machinelearningmastery.com/building-multilayer-perceptron-models-in-pytorch/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/building-multilayer-perceptron-models-in-pytorch/](https://machinelearningmastery.com/building-multilayer-perceptron-models-in-pytorch/)
- en: The PyTorch library is for deep learning. Deep learning, indeed, is just another
    name for a large-scale neural network or multilayer perceptron network. In its
    simplest form, multilayer perceptrons are a sequence of layers connected in tandem.
    In this post, you will discover the simple components you can use to create neural
    networks and simple deep learning models in PyTorch.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch库用于深度学习。深度学习确实只是大规模神经网络或多层感知器网络的另一种名称。在其最简单的形式中，多层感知器是串联在一起的一系列层。在这篇文章中，你将发现可以用来创建神经网络和简单深度学习模型的简单组件。
- en: '**Kick-start your project** with my book [Deep Learning with PyTorch](https://machinelearningmastery.com/deep-learning-with-pytorch/).
    It provides **self-study tutorials** with **working code**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 用我的书籍[《PyTorch深度学习》](https://machinelearningmastery.com/deep-learning-with-pytorch/)**启动你的项目**。它提供了**自学教程**和**可运行的代码**。
- en: Let’s get started.![](../Images/13e5c97fd1dae9155df5f747a1c211bf.png)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！[](../Images/13e5c97fd1dae9155df5f747a1c211bf.png)
- en: Building multilayer perceptron models in PyTorch
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中构建多层感知器模型
- en: Photo by [Sharon Cho](https://unsplash.com/photos/fc7Kplqt9mk). Some rights
    reserved.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Sharon Cho](https://unsplash.com/photos/fc7Kplqt9mk)。部分权利保留。
- en: Overview
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'This post is in six parts; they are:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文分为六部分，它们是：
- en: Neural Network Models in PyTorch
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch中的神经网络模型
- en: Model Inputs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型输入
- en: Layers, Activations, and Layer Properties
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层、激活和层属性
- en: Loss Functions and Model Optimizers
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数和模型优化器
- en: Model Training and Inference
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练与推理
- en: Examination of a Model
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型检查
- en: Neural Network Models in PyTorch
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch中的神经网络模型
- en: 'PyTorch can do a lot of things, but the most common use case is to build a
    deep learning model. The simplest model can be defined using `Sequential` class,
    which is just a linear stack of layers connected in tandem. You can create a `Sequential`
    model and define all the layers in one shot; for example:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch可以做很多事情，但最常见的用例是构建深度学习模型。最简单的模型可以使用`Sequential`类来定义，它只是一个线性堆叠的层串联在一起。你可以创建一个`Sequential`模型，并一次性定义所有层，例如：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You should have all your layers defined inside the parentheses in the processing
    order from input to output. For example:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该在处理顺序中将所有层定义在括号内，从输入到输出。例如：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The other way of using `Sequential` is to pass in an ordered dictionary in
    which you can assign names to each layer:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Sequential`的另一种方式是传入一个有序字典，你可以为每一层分配名称：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And if you would like to build the layers one by one instead of doing everything
    in one shot, you can do the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想逐层构建，而不是一次性完成所有工作，你可以按照以下方式进行：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You will find this helpful in a more complex case where you need to build a
    model based on some conditions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在需要根据某些条件构建模型的复杂情况下，你会发现这些内容非常有帮助。
- en: Model Inputs
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型输入
- en: The first layer in your model hints at the shape of the input. In the example
    above, you have `nn.Linear(764, 100)` as the first layer. Depending on the different
    layer type you use, the arguments may bear different meanings. But in this case,
    it is a `Linear` layer (also known as a dense layer or fully connected layer),
    and the two arguments tell the input and output dimensions of **this layer**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 模型中的第一层提示了输入的形状。在上面的示例中，你有`nn.Linear(764, 100)`作为第一层。根据你使用的不同层类型，参数可能有不同的含义。但在这个例子中，它是一个`Linear`层（也称为密集层或全连接层），这两个参数告诉**该层**的输入和输出维度。
- en: Note that the size of a batch is implicit. In this example, you should pass
    in a PyTorch tensor of shape `(n, 764)` into this layer and expect a tensor of
    shape `(n, 100)` in return, where `n` is the size of a batch.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，批次的大小是隐式的。在这个示例中，你应该将形状为`(n, 764)`的PyTorch张量传入该层，并期望返回形状为`(n, 100)`的张量，其中`n`是批次的大小。
- en: Want to Get Started With Deep Learning with PyTorch?
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 想开始使用PyTorch进行深度学习吗？
- en: Take my free email crash course now (with sample code).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在就参加我的免费电子邮件速成课程（含示例代码）。
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 点击注册并获得课程的免费PDF电子书版本。
- en: Layers, Activations, and Layer Properties
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层、激活和层属性
- en: 'There are many kinds of neural network layers defined in PyTorch. In fact,
    it is easy to define your own layer if you want to. Below are some common layers
    that you may see often:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中定义了许多种类的神经网络层。实际上，如果你愿意，定义自己的层也很简单。以下是一些你可能经常看到的常见层：
- en: '`nn.Linear(input, output)`: The fully-connected layer'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nn.Linear(input, output)`：全连接层'
- en: '`nn.Conv2d(in_channel, out_channel, kernel_size)`: The 2D convolution layer,
    popular in image processing networks'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nn.Conv2d(in_channel, out_channel, kernel_size)`：二维卷积层，在图像处理网络中很受欢迎。'
- en: '`nn.Dropout(probability)`: Dropout layer, usually added to a network to introduce
    regularization'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nn.Dropout(probability)`：Dropout 层，通常添加到网络中以引入正则化。'
- en: '`nn.Flatten()`: Reshape a high-dimensional input tensor into 1-dimensional
    (per each sample in a batch)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nn.Flatten()`：将高维输入张量重塑为 1 维（每个批次中的每个样本）。'
- en: 'Besides layers, there are also activation functions. These are functions applied
    to each element of a tensor. Usually, you take the output of a layer and apply
    the activation before feeding it as input to a subsequent layer. Some common activation
    functions are:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 除了层，还有激活函数。这些是应用于张量每个元素的函数。通常，你会将层的输出传递给激活函数，然后再作为输入传递给后续层。一些常见的激活函数包括：
- en: '`nn.ReLU()`: Rectified linear unit, the most common activation nowadays'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nn.ReLU()`：整流线性单元，现在最常用的激活函数。'
- en: '`nn.Sigmoid()` and `nn.Tanh()`: Sigmoid and hyperbolic tangent functions, which
    are the usual choice in older literature'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nn.Sigmoid()` 和 `nn.Tanh()`：Sigmoid 和双曲正切函数，这些是旧文献中常用的选择。'
- en: '`nn.Softmax()`: To convert a vector into probability-like values; popular in
    classification networks'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nn.Softmax()`：将向量转换为类似概率的值；在分类网络中很受欢迎。'
- en: You can find a list of all the different layers and activation functions in
    PyTorch’s documentation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 PyTorch 的文档中找到所有不同层和激活函数的列表。
- en: 'The design of PyTorch is very modular. Therefore, you don’t have much to adjust
    in each component. Take this `Linear` layer as an example. You can only specify
    the input and output shape but not other details, such as how to initialize the
    weights. However, almost all the components can take two additional arguments:
    the device and the data type.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 的设计非常模块化。因此，你不需要在每个组件中进行太多调整。以 `Linear` 层为例，你只需指定输入和输出的形状，而不是其他细节，如如何初始化权重。然而，几乎所有组件都可以接受两个额外的参数：设备和数据类型。
- en: 'A PyTorch device specifies where this layer will execute. Normally, you choose
    between the CPU and the GPU or omit it and let PyTorch decide. To specify a device,
    you do the following (CUDA means a supported nVidia GPU):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 设备指定了此层将在哪个位置执行。通常，你可以选择 CPU 或 GPU，或者省略它，让 PyTorch 决定。要指定设备，你可以这样做（CUDA
    意味着支持的 nVidia GPU）：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: or
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The data type argument (`dtype`) specifies what kind of data type this layer
    should operate on. Usually, it is a 32-bit float, and usually, you don’t want
    to change that. But if you need to specify a different type, you must do so using
    PyTorch types, e.g.,
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 数据类型参数 (`dtype`) 指定了此层应操作的数据类型。通常，这是一个 32 位浮点数，通常你不想更改它。但如果需要指定不同的类型，必须使用 PyTorch
    类型，例如：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Loss Function and Model Optimizers
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数和模型优化器
- en: A neural network model is a sequence of matrix operations. The matrices that
    are independent of the input and kept inside the model are called weights. Training
    a neural network will **optimize** these weights so that they produce the output
    you want. In deep learning, the algorithm to optimize these weights is gradient
    descent.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络模型是矩阵操作的序列。与输入无关并保存在模型中的矩阵称为权重。训练神经网络将**优化**这些权重，以便它们生成你想要的输出。在深度学习中，优化这些权重的算法是梯度下降。
- en: There are many variations of gradient descent. You can make your choice by preparing
    an optimizer for your model. It is not part of the model, but you will use it
    alongside the model during training. The way you use it includes defining a **loss
    function** and minimizing the loss function using the optimizer. The loss function
    will give a **distance score** to tell how far away the model’s output is from
    your desired output. It compares the output tensor of the model to the expected
    tensor, which is called the **label** or the **ground truth** in a different context.
    Because it is provided as part of the training dataset, a neural network model
    is a supervised learning model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降有很多变体。你可以通过为模型准备一个优化器来选择适合你的优化器。它不是模型的一部分，但你会在训练过程中与模型一起使用它。使用方式包括定义一个**损失函数**并使用优化器最小化损失函数。损失函数会给出一个**距离分数**，以告诉模型输出距离你期望的输出有多远。它将模型的输出张量与期望的张量进行比较，在不同的上下文中，期望的张量被称为**标签**或**真实值**。因为它作为训练数据集的一部分提供，所以神经网络模型是一个监督学习模型。
- en: In PyTorch, you can simply take the model’s output tensor and manipulate it
    to calculate the loss. But you can also make use of the functions provided in
    PyTorch for that, e.g.,
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，你可以简单地取模型的输出张量并对其进行操作以计算损失。但你也可以利用 PyTorch 提供的函数，例如，
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this example, the `loss_fn` is a function, and `loss` is a tensor that supports
    automatic differentiation. You can trigger the differentiation by calling `loss.backward()`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`loss_fn` 是一个函数，而 `loss` 是一个支持自动微分的张量。你可以通过调用 `loss.backward()` 来触发微分。
- en: 'Below are some common loss functions in PyTorch:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 PyTorch 中一些常见的损失函数：
- en: '`nn.MSELoss()`: Mean square error, useful in regression problems'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nn.MSELoss()`：均方误差，适用于回归问题'
- en: '`nn.CrossEntropyLoss()`: Cross entropy loss, useful in classification problems'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nn.CrossEntropyLoss()`：交叉熵损失，适用于分类问题'
- en: '`nn.BCELoss()`: Binary cross entropy loss, useful in binary classification
    problems'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nn.BCELoss()`：二元交叉熵损失，适用于二分类问题'
- en: 'Creating an optimizer is similar:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 创建优化器类似：
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: All optimizers require a list of all parameters that it needs to optimize. It
    is because the optimizer is created outside the model, and you need to tell it
    where to look for the parameters (i.e., model weights). Then, the optimizer will
    take the gradient as computed by the `backward()` function call and apply it to
    the parameters based on the optimization algorithm.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 所有优化器都需要一个包含所有需要优化的参数的列表。这是因为优化器是在模型之外创建的，你需要告诉它在哪里查找参数（即模型权重）。然后，优化器会根据 `backward()`
    函数调用计算的梯度，并根据优化算法将其应用于参数。
- en: 'This is a list of some common optimizers:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一些常见优化器的列表：
- en: '`torch.optim.Adam()`: The Adam algorithm (adaptive moment estimation)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.optim.Adam()`：Adam 算法（自适应矩估计）'
- en: '`torch.optim.NAdam()`: The Adam algorithm with Nesterov momentum'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.optim.NAdam()`：具有 Nesterov 动量的 Adam 算法'
- en: '`torch.optim.SGD()`: Stochastic gradient descent'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.optim.SGD()`：随机梯度下降'
- en: '`torch.optim.RMSprop()`: The RMSprop algorithm'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.optim.RMSprop()`：RMSprop 算法'
- en: You can find a list of all provided loss functions and optimizers in PyTorch’s
    documentation. You can learn about the mathematical formula of each optimization
    algorithm on the respective optimizers’ page in the documentation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 PyTorch 的文档中找到所有提供的损失函数和优化器的列表。你可以在文档中相应优化器的页面上了解每个优化算法的数学公式。
- en: Model Training and Inference
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练和推理
- en: 'PyTorch doesn’t have a dedicated function for model training and evaluation.
    A defined model by itself is like a function. You pass in an input tensor and
    get back the output tensor. Therefore, it is your responsibility to write the
    training loop. A minimal training loop is like the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 没有专门的模型训练和评估函数。一个定义好的模型本身就像一个函数。你传入一个输入张量，并返回一个输出张量。因此，编写训练循环是你的责任。一个最简单的训练循环如下：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If you already have a model, you can simply take `y_pred = model(X)` and use
    the output tensor `y_pred` for other purposes. That’s how you use the model for
    prediction or inference. A model, however, does not expect one input sample but
    a batch of input samples in one tensor. If the model is to take an input vector
    (which is one-dimensional), you should provide a two-dimensional tensor to the
    model. Usually, in the case of inference, you deliberately create a batch of one
    sample.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经有一个模型，你可以简单地使用 `y_pred = model(X)` 并利用输出张量 `y_pred` 进行其他用途。这就是如何使用模型进行预测或推断。然而，模型不期望单个输入样本，而是一个包含多个输入样本的张量。如果模型要处理一个输入向量（即一维），你应当向模型提供一个二维张量。通常，在推断的情况下，你会故意创建一个包含一个样本的批次。
- en: Examination of a Model
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型检查
- en: 'Once you have a model, you can check what it is by printing it:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了模型，你可以通过打印模型来检查它：
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will give you, for example, the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为你提供例如以下内容：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If you would like to save the model, you can use the `pickle` library from
    Python. But you can also access it using PyTorch:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想保存模型，你可以使用 Python 的 `pickle` 库。但你也可以使用 PyTorch 来访问它：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This way, you have the entire model object saved in a pickle file. You can
    retrieve the model with:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，你就将整个模型对象保存到 pickle 文件中。你可以通过以下方式检索模型：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'But the recommended way of saving a model is to leave the model design in code
    and keep only the weights. You can do so with:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 但推荐的保存模型的方式是将模型设计留在代码中，只保存权重。你可以这样做：
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `state_dict()` function extracts only the states (i.e., weights in a model).
    To retrieve it, you need to rebuild the model from scratch and then load the weights
    like this:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`state_dict()` 函数仅提取状态（即模型中的权重）。要检索它，你需要从头开始重建模型，然后像这样加载权重：'
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Resources
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: 'You can learn more about how to create simple neural networks and deep learning
    models in PyTorch using the following resources:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下资源进一步了解如何在 PyTorch 中创建简单的神经网络和深度学习模型：
- en: Online resources
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在线资源
- en: '[`torch.nn` documentation](https://pytorch.org/docs/stable/nn.html)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`torch.nn` 文档](https://pytorch.org/docs/stable/nn.html)'
- en: '[`torch.optim` documentation](https://pytorch.org/docs/stable/optim.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`torch.optim` 文档](https://pytorch.org/docs/stable/optim.html)'
- en: '[PyTorch tutorials](https://pytorch.org/tutorials/)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch 教程](https://pytorch.org/tutorials/)'
- en: Summary
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this post, you discovered the PyTorch API that you can use to create artificial
    neural networks and deep learning models. Specifically, you learned about the
    life cycle of a PyToch model, including:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，你了解了可以用来创建人工神经网络和深度学习模型的 PyTorch API。具体来说，你学习了 PyTorch 模型的生命周期，包括：
- en: Constructing a model
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建模型
- en: Creating and adding layers and activations
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建和添加层及激活函数
- en: Preparing a model for training and inference
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为训练和推断准备模型
