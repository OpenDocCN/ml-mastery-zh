- en: 3 Ways of Using Gemma 2 Locally
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地使用 Gemma 2 的三种方式
- en: 原文：[https://machinelearningmastery.com/3-ways-of-using-gemma-2-locally/](https://machinelearningmastery.com/3-ways-of-using-gemma-2-locally/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/3-ways-of-using-gemma-2-locally/](https://machinelearningmastery.com/3-ways-of-using-gemma-2-locally/)
- en: '![3 Ways of Using Gemma 2 Locally](../Images/dc201ce24fbf2415fcd6b8efaa23d8b9.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![本地使用 Gemma 2 的三种方式](../Images/dc201ce24fbf2415fcd6b8efaa23d8b9.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: After the highly successful launch of Gemma 1, the Google team introduced an
    even more advanced model series called Gemma 2\. This new family of Large Language
    Models (LLMs) includes models with 9 billion (9B) and 27 billion (27B) parameters.
    Gemma 2 offers higher performance and greater inference efficiency than its predecessor,
    with significant safety advancements built in. Both models outperform the Llama
    3 and Gork 1 models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Gemma 1 成功推出后，Google 团队推出了一个更先进的模型系列，称为 Gemma 2。这个新的大型语言模型（LLMs）系列包括 90 亿（9B）和
    270 亿（27B）参数的模型。Gemma 2 提供比其前身更高的性能和更大的推理效率，同时具有显著的安全性提升。这两款模型都优于 Llama 3 和 Gork
    1 模型。
- en: In this tutorial, we will learn about the three applications that will help
    you run the Gemma 2 model locally faster than online. To experience the state-of-the-art
    model locally, you just have to install the application, download the model, and
    start using it. It is that simple.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将了解三种应用程序，这些应用程序将帮助你更快地在本地运行 Gemma 2 模型。要在本地体验最先进的模型，你只需安装应用程序，下载模型并开始使用。就是这么简单。
- en: 1\. Jan
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. Jan
- en: Download and install [Jan](https://jan.ai/) from the official website. Jan is
    my favorite application for running and testing various open-source and property
    LLMs. It is super easy to set up and highly flexible in terms of importing and
    using local models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从官方网站下载并安装 [Jan](https://jan.ai/)。Jan 是我最喜欢的运行和测试各种开源及专有 LLM 的应用程序。它非常容易设置，并且在导入和使用本地模型方面具有很高的灵活性。
- en: 'Launch the Jan application and go to the Model Hub menu. Then, paste the following
    link of the Hugging Face repository into the search bar and press enter: **bartowski/gemma-2-9b-it-GGUF**.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 Jan 应用程序并进入 Model Hub 菜单。然后，将 Hugging Face 仓库的以下链接粘贴到搜索栏中并按回车：**bartowski/gemma-2-9b-it-GGUF**。
- en: '![3 Ways of Using Gemma 2 Locally](../Images/c045fd6057727f77d34b830d311e49d4.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![本地使用 Gemma 2 的三种方式](../Images/c045fd6057727f77d34b830d311e49d4.png)'
- en: Image by Author
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: You will be redirected to a new window, where you have the option to select
    various quantized versions of the model. We will be downloading the “Q4-K-M” version.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你将被重定向到一个新窗口，在那里你可以选择不同的量化版本。我们将下载“Q4-K-M”版本。
- en: '![3 Ways of Using Gemma 2 Locally](../Images/feecad737c904007ebe0ce768d31faab.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![本地使用 Gemma 2 的三种方式](../Images/feecad737c904007ebe0ce768d31faab.png)'
- en: Image by Author
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Select the downloaded model from the model menu on the right panel and start
    using it.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从右侧面板的模型菜单中选择已下载的模型并开始使用。
- en: This version of the quantized model currently gives me 37 tokens per second,
    but you might improve your speed even further if you use a different version.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这个量化模型的当前版本每秒提供 37 个令牌，但如果使用不同的版本，你的速度可能会更快。
- en: '![3 Ways of Using Gemma 2 Locally](../Images/381427dd20956e2f431dcf699c811325.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![本地使用 Gemma 2 的三种方式](../Images/381427dd20956e2f431dcf699c811325.png)'
- en: Image by Author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 2\. Ollama
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. Ollama
- en: Go to the official website to download and install [Ollama](https://ollama.com/download).
    It is a favorite among developers and people who are familiar with terminals and
    CLI tools. Even for new users, it is simple to set up.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 访问官方网站下载并安装 [Ollama](https://ollama.com/download)。它在开发者和熟悉终端及 CLI 工具的人中非常受欢迎。即使是新用户，也很容易设置。
- en: After installation is completed, please launch the Ollama application and type
    the following command in your favorite terminal. I am using Powershell on Windows
    11.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，请启动 Ollama 应用程序并在你喜欢的终端中输入以下命令。我在 Windows 11 上使用 Powershell。
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Depending on your internet speed, it will take approximately half an hour to
    download the model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的互联网速度，下载模型大约需要半小时。
- en: '![3 Ways of Using Gemma 2 Locally](../Images/90d90e4cc42e6e641be1ee8f86ef102d.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![本地使用 Gemma 2 的三种方式](../Images/90d90e4cc42e6e641be1ee8f86ef102d.png)'
- en: Image by Author
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: After the download is complete, you can start prompting and start using it within
    your terminal.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下载完成后，你可以开始提示并在终端中使用它。
- en: '![3 Ways of Using Gemma 2 Locally](../Images/cb577dd2885bfbb57d95a15b4ce7a945.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![本地使用 Gemma 2 的三种方式](../Images/cb577dd2885bfbb57d95a15b4ce7a945.png)'
- en: Image by Author
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Using Gemma2 by Importing from GGUF model file
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过从 GGUF 模型文件导入来使用 Gemma2
- en: 'If you already have a GGUF model file and want to use it with Ollama, then
    you have to first create a new file with the name “Modelfile” and type the following
    command:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经有了一个 GGUF 模型文件并想与 Ollama 一起使用，那么你首先需要创建一个名为“Modelfile”的新文件，并输入以下命令：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After that, create the model using the Modelfile, which points to the GGUF file
    in your directory.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，使用 Modelfile 创建模型，该文件指向你目录中的 GGUF 文件。
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When model transferring is done successfully, please type the following command
    to start using it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型转移成功完成后，请输入以下命令开始使用它。
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![3 Ways of Using Gemma 2 Locally](../Images/09f32729029bc5a6271fb85a2e1a366c.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![本地使用 Gemma 2 的 3 种方法](../Images/09f32729029bc5a6271fb85a2e1a366c.png)'
- en: Image by Author
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 3\. Msty
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. Msty
- en: Download and install [Msty](https://msty.app/) from the official website. Msty
    is a new contender, and it is becoming my favorite. It offers tons of features
    and models. You can even connect to proprietary models or the Ollama servers.
    It is a simple and powerful application that you should give a try.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从官方网站下载并安装 [Msty](https://msty.app/)。Msty 是一个新兴的竞争者，正成为我最喜欢的应用程序。它提供了大量功能和模型。你甚至可以连接到专有模型或
    Ollama 服务器。它是一个简单而强大的应用程序，你应该试一试。
- en: After successfully installing the application, please launch the program and
    navigate to “Local AI Models” by clicking the button on the left panel.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 安装应用程序成功后，请启动程序并通过点击左侧面板上的按钮导航到“本地 AI 模型”。
- en: '![3 Ways of Using Gemma 2 Locally](../Images/65b590dc4d6f53a22d6d6ed3309221f4.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![本地使用 Gemma 2 的 3 种方法](../Images/65b590dc4d6f53a22d6d6ed3309221f4.png)'
- en: Image by Author
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'Click on the “Download More Models” button and type the following link into
    the search bar: **bartowski/gemma-2-9b-it-GGUF**. Make sure you have selected
    Hugging Face as the model Hub.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“下载更多模型”按钮，并在搜索栏中输入以下链接：**bartowski/gemma-2-9b-it-GGUF**。确保你已选择 Hugging Face
    作为模型中心。
- en: '![3 Ways of Using Gemma 2 Locally](../Images/a3916fdae1506bc3d0579bd6f49f3fc5.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![本地使用 Gemma 2 的 3 种方法](../Images/a3916fdae1506bc3d0579bd6f49f3fc5.png)'
- en: Image by Author
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: After downloading is completed, start using it.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下载完成后，开始使用它。
- en: '![3 Ways of Using Gemma 2 Locally](../Images/366b19acdb4f31ea717fac23260f2b66.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![本地使用 Gemma 2 的 3 种方法](../Images/366b19acdb4f31ea717fac23260f2b66.png)'
- en: Image by Author
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Using Msty with Ollama
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Msty 与 Ollama
- en: If you want to use the Ollama model in a chatbot application instead of a terminal,
    you can use Msty’s connect with Ollama option. It is straightforward.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在聊天机器人应用程序中使用 Ollama 模型而不是终端，可以使用 Msty 的与 Ollama 连接选项。这很简单。
- en: First, go to the terminal and start the Ollama server.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，去终端并启动 Ollama 服务器。
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Copy the server link.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 复制服务器链接。
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Navigate to the “Local AI Models” menu and click on the settings button located
    in the top right corner.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到“本地 AI 模型”菜单，并点击右上角的设置按钮。
- en: Then, select “Remote Model Providers” and click on the “Add New Provider” button.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，选择“远程模型提供者”并点击“添加新提供者”按钮。
- en: Next, choose the model provider as “Ollama remote” and enter the service endpoint
    link for the Ollama server.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将模型提供者选择为“Ollama 远程”，并输入 Ollama 服务器的服务端点链接。
- en: Click on the “Re-fetch models” button and select “gemma2:latest”, then click
    the “Add” button.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“重新获取模型”按钮，选择“gemma2:latest”，然后点击“添加”按钮。
- en: '![3 Ways of Using Gemma 2 Locally](../Images/3de2e35d87ed8a7b7c1a1ef031eaac78.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![本地使用 Gemma 2 的 3 种方法](../Images/3de2e35d87ed8a7b7c1a1ef031eaac78.png)'
- en: Image by Author
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: In the chat menu, select the new model and start using it.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在聊天菜单中，选择新的模型并开始使用它。
- en: '![3 Ways of Using Gemma 2 Locally](../Images/23a14432ea4ae3189717e92abefc426f.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![本地使用 Gemma 2 的 3 种方法](../Images/23a14432ea4ae3189717e92abefc426f.png)'
- en: Image by Author
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Conclusion
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: The three applications we reviewed are powerful and come with tons of features
    that will enhance your experience of using AI models locally. All you have to
    do is download the application and models, and the rest is pretty simple.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评测的这三款应用程序功能强大，具有大量功能，将提升你使用 AI 模型的本地体验。你需要做的就是下载应用程序和模型，其余的都很简单。
- en: I use the Jan application to test open-source LLM performance and generate the
    code and content. It is fast and private, and my data never leaves my laptop.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用 Jan 应用程序测试开源 LLM 的性能，并生成代码和内容。它速度快且私密，我的数据从未离开过我的笔记本电脑。
- en: In this tutorial, we have learned how to use the Jan, Ollama, and Msty to run
    the Gemma 2 model locally. These applications comes with important features that
    will enhance your experience of using LLMs locally.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们学习了如何使用 Jan、Ollama 和 Msty 本地运行 Gemma 2 模型。这些应用程序具有重要的功能，将提升你使用本地 LLM
    的体验。
- en: I hope you enjoyed my brief tutorial. I enjoy sharing the products and applications
    I am passionate about and use regularly.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你喜欢我的简短教程。 我喜欢分享我热衷并且经常使用的产品和应用。
