- en: Building Transformer Models with Attention Crash Course. Build a Neural Machine
    Translator in 12 Days
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器模型与注意力机制速成课程。12 天内构建一个神经机器翻译器
- en: 原文：[https://machinelearningmastery.com/building-transformer-models-with-attention-crash-course-build-a-neural-machine-translator-in-12-days/](https://machinelearningmastery.com/building-transformer-models-with-attention-crash-course-build-a-neural-machine-translator-in-12-days/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/building-transformer-models-with-attention-crash-course-build-a-neural-machine-translator-in-12-days/](https://machinelearningmastery.com/building-transformer-models-with-attention-crash-course-build-a-neural-machine-translator-in-12-days/)
- en: Transformer is a recent breakthrough in neural machine translation. Natural
    languages are complicated. A word in one language can be translated into multiple
    words in another, depending on the **context**. But what exactly a context is,
    and how you can teach the computer to understand the context was a big problem
    to solve. The invention of the attention mechanism solved the problem of how to
    encode a context into a word, or in other words, how you can present a word **and**
    its context together in a numerical vector. Transformer brings this to one level
    higher so that we can build a neural network for natural language translation
    using only the attention mechanism but no recurrent structure. This not only makes
    the network simpler, easier to train, and parallelizable in algorithm but also
    allows a more complicated language model to be built. As a result, we can see
    computer-translated sentences almost flawlessly.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器是神经机器翻译中的一种最新突破。自然语言很复杂。一种语言中的一个词可以在另一种语言中翻译成多个词，具体取决于**上下文**。但上下文究竟是什么，如何教计算机理解上下文是一个大问题。注意力机制的发明解决了如何将上下文编码到一个词中的问题，换句话说，就是如何将一个词**和**它的上下文一起呈现为一个数值向量。变换器将这一点提升到了一个更高的层次，使我们可以仅使用注意力机制而没有递归结构来构建自然语言翻译的神经网络。这不仅使网络更简单、更容易训练、并且算法上可并行化，还允许构建更复杂的语言模型。因此，我们几乎可以看到计算机翻译的句子几乎完美无瑕。
- en: Indeed, such a powerful deep learning model is not difficult to build. In TensorFlow
    and Keras, you have almost all the building blocks readily available, and training
    a model is only a matter of several hours. It is fun to see a transformer model
    built and trained. It is even more fun to see a trained model to translate sentences
    from one language to another.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，这样一个强大的深度学习模型并不难构建。在 TensorFlow 和 Keras 中，你几乎可以随时获取所有构建块，训练模型只需几个小时。看到变换器模型被构建和训练是很有趣的。看到一个训练好的模型将句子从一种语言翻译到另一种语言则更有趣。
- en: In this crash course, you will build a transformer model in the similar design
    as the original research paper.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个速成课程中，你将按照原始研究论文的类似设计构建一个变换器模型。
- en: This is a big and important post. You might want to bookmark it.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个重要的帖子。你可能想要收藏它。
- en: Let’s get started.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 开始吧。
- en: '![](../Images/a4c1293188ef944df5d17458c0f5f1de.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4c1293188ef944df5d17458c0f5f1de.png)'
- en: Building Transformer Models with Attention (12-day Mini-Course).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器模型与注意力机制（12 天迷你课程）。
- en: Photo by [Norbert Braun](https://unsplash.com/photos/uU8n5LuzpTc), some rights
    reserved.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [诺伯特·布劳恩](https://unsplash.com/photos/uU8n5LuzpTc) 提供，版权所有。
- en: Who Is This Crash-Course For?
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这个速成课程适合谁？
- en: Before you get started, let’s make sure you are in the right place.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们需要确保你在正确的位置。
- en: 'This course is for developers who are already familiar with TensorFlow/Keras.
    The lessons in this course do assume a few things about you, such as:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个课程适合那些已经熟悉 TensorFlow/Keras 的开发者。课程的内容假设你具备一些前提条件，例如：
- en: You know how to build a custom model, including the Keras functional API
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你知道如何构建自定义模型，包括 Keras 函数式 API
- en: You know how to train a deep learning model in Keras
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你知道如何在 Keras 中训练深度学习模型
- en: You know how to use a trained model for inference
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你知道如何使用训练好的模型进行推理
- en: 'You do NOT need to be:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要是：
- en: A natural language processing expert
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理专家
- en: A speaker of many languages
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多语言使用者
- en: This crash course can help you get a deeper understanding of what a transformer
    model is and what it can do.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这个速成课程可以帮助你更深入地理解什么是变换器模型及其功能。
- en: 'This crash course assumes you have a working TensorFlow 2.10 environment installed.
    If you need help with your environment, you can follow the step-by-step tutorial
    here:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本速成课程假设你已经安装了工作中的 TensorFlow 2.10 环境。如果你需要帮助，可以按照这里的逐步教程进行：
- en: '[How to Set Up Your Python Environment for Machine Learning With Anaconda](https://machinelearningmastery.com/setup-python-environment-machine-learning-deep-learning-anaconda/)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 Anaconda 设置 Python 环境以进行机器学习](https://machinelearningmastery.com/setup-python-environment-machine-learning-deep-learning-anaconda/)'
- en: Crash-Course Overview
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速课程概述
- en: This crash course is broken down into 12 lessons.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个速成课程分为 12 节课。
- en: You could complete one lesson per day (recommended) or complete all of the lessons
    in one day (hardcore). It really depends on the time you have available and your
    level of enthusiasm.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以每天完成一节课（推荐）或者在一天内完成所有课程（高强度）。这真的取决于你可用的时间和你的热情程度。
- en: Below is a list of the 12 lessons that will get you started and learn about
    the construction of a transformer model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 12 节课程的列表，它们将帮助你入门并了解 Transformer 模型的构建。
- en: 'Lesson 01: Obtaining Data'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程 01：获取数据
- en: 'Lesson 02: Text Normalization'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程 02：文本规范化
- en: 'Lesson 03: Vectorization and Making Datasets'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程 03：向量化和数据集创建
- en: 'Lesson 04: Positional Encoding Matrix'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程 04：位置编码矩阵
- en: 'Lesson 05: Positional Encoding Layer'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程 05：位置编码层
- en: 'Lesson 06: Transformer Building Blocks'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程 06：Transformer 构建模块
- en: 'Lesson 07: Transformer Encoder and Decoder'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程 07：Transformer 编码器和解码器
- en: 'Lesson 08: Building a Transformer'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程 08：构建 Transformer
- en: 'Lesson 09: Preparing the Transformer Model for Training'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程 09：准备 Transformer 模型进行训练
- en: 'Lesson 10: Training the Transformer'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程 10：训练 Transformer
- en: 'Lesson 11: Inference from the Transformer Model'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程 11：Transformer 模型的推理
- en: 'Lesson 12: Improving the Model'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程 12：改进模型
- en: Each lesson could take you between 15 and up to 60 minutes. Take your time and
    complete the lessons at your own pace. Ask questions, and even post results in
    the comments online.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 每节课可能需要你 15 到 60 分钟。花时间按照自己的节奏完成课程。提出问题，甚至在网上发布结果。
- en: The lessons might expect you to go off and find out how to do things. This guide
    will give you hints, but even if you just follow the code in the lesson, you can
    finish a transformer model that works quite well.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 课程可能会期望你去查找如何做事的方法。本指南将给你提示，但即使你只是按照课程中的代码，你也可以完成一个效果相当不错的 Transformer 模型。
- en: '**Post your results in the comments**; I’ll cheer you on!'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**在评论中发布你的结果**；我会为你加油！'
- en: Hang in there; don’t give up.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 坚持下去；不要放弃。
- en: 'Lesson 01: Obtaining Data'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 课程 01：获取数据
- en: As you are building a neural machine translator, you need data for training
    and testing. Let’s build a sentence-based English-to-French translator. There
    are many resources on the Internet. An example would be the user-contributed data
    for the flash card app Anki. You can download some data files at [https://ankiweb.net/shared/decks/french](https://ankiweb.net/shared/decks/french).
    The data file would be a ZIP file containing a SQLite database file, from which
    you can extract the English-French sentence pairs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你正在构建一个神经机器翻译器，你需要用于训练和测试的数据。让我们构建一个基于句子的英法翻译器。网络上有许多资源。例如，用户贡献的 Anki 闪卡应用程序的数据。你可以在
    [https://ankiweb.net/shared/decks/french](https://ankiweb.net/shared/decks/french)
    下载一些数据文件。数据文件将是一个包含 SQLite 数据库文件的 ZIP 文件，你可以从中提取英法句对。
- en: However, you may find it more convenient to have a text file version, which
    you can find it at [https://www.manythings.org/anki/](https://www.manythings.org/anki/).
    Google hosts a mirror of this file as
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，你可能会发现拥有文本文件版本更为方便，你可以在 [https://www.manythings.org/anki/](https://www.manythings.org/anki/)
    找到这个文件的镜像。Google 也托管了这个文件的镜像
- en: well, which we will be using.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们将使用这些数据。
- en: 'The code below will download the compressed data file and extract it:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将下载压缩的数据文件并解压它：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The data file will be a plaintext file named `fra.txt`. Its format would be
    lines of:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据文件将是一个名为 `fra.txt` 的纯文本文件。其格式将是：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Your Task
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你的任务
- en: Try to run the above code and open the file extracted. You should verify that
    the format of each line is like the above.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试运行上述代码并打开提取的文件。你应该验证每一行的格式是否与上述一致。
- en: In the next lesson, you will process this file and prepare the dataset suitable
    for training and testing.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节课中，你将处理此文件并准备适合训练和测试的数据集。
- en: 'Lesson 02: Text Normalization'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 课程 02：文本规范化
- en: Just like all NLP tasks, you need to normalize the text before you use it. French
    letters have accents which would be represented as Unicode characters, but such
    representation is not unique in Unicode. Therefore, you will convert the string
    into NFKC (compatibility and composition normal form).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 就像所有 NLP 任务一样，你需要在使用文本之前进行规范化。法语字母有重音符号，这些会以 Unicode 字符表示，但这种表示在 Unicode 中并不唯一。因此，你将把字符串转换为
    NFKC（兼容性和组合规范形式）。
- en: Next, you will tokenize the sentences. Each word should be a separate token
    as well as each punctuation mark. However, the punctuation used in contractions
    such as *don’t*, *va-t-il*, or *c’est* are not separated from the words. Also,
    convert everything into lowercase in the expectation that this will reduce the
    number of distinct words in the vocabulary.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将对句子进行标记化。每个单词和每个标点符号应该是一个独立的标记。然而，用于缩写形式的标点符号，如 *don’t*、*va-t-il* 或 *c’est*，不会与单词分开。此外，将所有内容转换为小写，期望这会减少词汇中的不同单词数量。
- en: Normalization and tokenization can go a lot deeper, such as subword tokenization,
    stemming, and lemmatization. But to keep things simple, you do not do these in
    this project.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化和标记化可以更深入，例如子词标记化、词干提取和词形还原。但为了简化起见，你在这个项目中不会做这些。
- en: Starting from scratch, the code to normalize the text is below. You will use
    the Python module `unicodedata` to convert a Unicode string into NFKC normal form.
    Then you will use regular expression to add space around punctuation marks. Afterward,
    you will wrap the French sentences (i.e., the target language) with sentinels
    `[start]` and `[end]`. You will see the purpose of the sentinels in later lessons.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始，标准化文本的代码如下。你将使用 Python 模块 `unicodedata` 将 Unicode 字符串转换为 NFKC 标准形式。然后你将使用正则表达式在标点符号周围添加空格。之后，你将用哨兵
    `[start]` 和 `[end]` 包裹法语句子（即目标语言）。你将在后续课程中看到哨兵的目的。
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When you run this, you should see the result from a few samples, such as these:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这段代码时，你应该会看到一些样本的结果，如下所示：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We saved the normalized sentence pairs in a pickle file, so we can reuse it
    in subsequent steps.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将标准化的句子对保存在一个 pickle 文件中，因此可以在后续步骤中重复使用。
- en: 'When you use it for your model, you want to know some statistics about this
    dataset. In particular, you want to see how many distinct tokens (words) in each
    language and how long the sentences are. You can figure these out as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将它用于模型时，你希望了解一些关于此数据集的统计信息。特别是，你想看看每种语言中有多少个不同的标记（单词）以及句子的长度。你可以按以下方式找出这些信息：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Your Task
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你的任务
- en: 'Run the above code. See not only the sample sentences but also the statistics
    you collected. Remember the output as they will be useful for your next lesson.
    Besides, knowing the maximum length of sentences is not as useful as knowing their
    distribution. You should plot a histogram for that. Try out this to produce the
    following chart:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码。查看不仅是样本句子，还有你收集的统计信息。记住这些输出，因为它们对你的下一节课非常有用。此外，了解句子的最大长度不如了解它们的分布有用。你应该为此绘制一个直方图。试着生成以下图表：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/303e1f8813a0d5d652d6a6971077a74b.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/303e1f8813a0d5d652d6a6971077a74b.png)'
- en: Sentence lengths in different languages
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 不同语言中的句子长度
- en: In the next lesson, you will vectorize this normalized text data and create
    datasets.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个课程中，你将对这些标准化的文本数据进行向量化并创建数据集。
- en: 'Lesson 03: Vectorization and Making Datasets'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 课程 03：向量化和创建数据集
- en: In the previous lesson, you cleaned up the sentences, but they are still text.
    Neural networks can handle only numbers. One way to convert the text into numbers
    is through vectorization. What this means is to transform the tokens from the
    text into an integer. Hence a sentence with $n$ tokens (words) will become a **vector**
    of $n$ integers.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的课程中，你清理了句子，但它们仍然是文本。神经网络只能处理数字。将文本转换为数字的一种方法是通过向量化。这意味着将文本中的标记转换为整数。因此，包含
    $n$ 个标记（单词）的句子将变成一个 **向量**，包含 $n$ 个整数。
- en: You can build your own vectorizer. Simply build a mapping table of each unique
    token to a unique integer. When it is used, you look up the token one by one in
    the table and return the integers in the form of a vector.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以构建自己的向量化器。简单地构建一个每个唯一标记到唯一整数的映射表。当使用时，你一个一个地查找表中的标记，并返回整数形式的向量。
- en: In Keras, you have `TextVectorization` layer to save us the effort of building
    a vectorizer. It supports padding, i.e., integer 0 is reserved to mean “empty.”
    This is useful when you give a sentence of $m < n$ tokens but want the vectorizer
    always to return a fixed length $n$ vector.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，你有 `TextVectorization` 层来节省构建向量化器的工作。它支持填充，即整数 0 被保留用于表示“空”。当你提供一个
    $m < n$ 的标记句子时，但希望向量化器始终返回固定长度 $n$ 的向量时，这非常有用。
- en: You will first split the sentence pair into training, validation, and testing
    sets as you need them for the model training. Then, create a `TextVectorization`
    layer and adapt it to the training set only (because you should not peek into
    the validation or testing dataset until the model training is completed).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 您将首先将句子对拆分为训练、验证和测试集，因为这些是模型训练所需的。然后，创建一个 `TextVectorization` 层并仅适应于训练集（因为在完成模型训练之前，您不应窥视验证或测试数据集）。
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that the parameter `max_tokens` to `TextVectorization` object can be omitted
    to let the vectorizer figure it out. But if you set them to a value smaller than
    the total vocabulary (such as this case), you limit the the vectorizer to learn
    only the more frequent words and make the rare words as **out-of-vocabulary**
    (OOV). This may be useful to skip the words of little value or with spelling mistakes.
    You also fix the output length of the vectorizer. We assumed that a sentence should
    have no more than 20 tokens in the above.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`TextVectorization` 对象的参数 `max_tokens` 可以省略以让向量化器自行处理。但是，如果将它们设置为一个比总词汇量更小的值（比如这种情况），则限制了向量化器仅学习更频繁的单词，并将罕见单词标记为**超出词汇表**（OOV）。这可能对跳过价值不大或拼写错误的单词很有用。您还要固定向量化器的输出长度。我们假设以上句子中的每个句子应不超过20个标记。
- en: The next step would be to make use of the vectorizer and create a TensorFlow
    Dataset object. This will be helpful in your later steps to train our model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要使用向量化器并创建一个 TensorFlow 数据集对象。这将有助于您在后续步骤中训练我们的模型。
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You will reuse this code later to make the `train_ds` and `val_ds` dataset objects.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后您将重复使用此代码以创建 `train_ds` 和 `val_ds` 数据集对象。
- en: Your Task
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 您的任务
- en: 'Run the above code. Verify that you can see an output similar to the below:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码。验证您是否能看到类似以下的输出：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The exact vector may not be the same, but you should see that the shape should
    all be (*batch size*, *sequence length*). Some code above is borrowed from the
    tutorial by François Chollet, [English-to-Spanish translation with a sequence-to-sequence
    Transformer](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/).
    You may also want to see how his implementation of transformer is different from
    this mini-course.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 精确的向量可能不同，但您应该看到形状应该都是（*批次大小*，*序列长度*）。以上一些代码是从 François Chollet 的教程中借用的，[使用序列到序列Transformer进行英语到西班牙语翻译](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/)。您可能还想看看他的Transformer实现与本小课程有何不同。
- en: In the next lesson, you will move to the topic of position encoding.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一课中，您将转到位置编码的主题。
- en: 'Lesson 04: Positional Encoding Matrix'
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第04课：位置编码矩阵
- en: When a sentence is vectorized, you get a vector of integers, where each integer
    represents a word. The integer here is only a label. We cannot assume two integers
    closer to each other means the words they represent are related.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个句子被向量化时，您会得到一个整数向量，其中每个整数代表一个单词。这里的整数只是一个标签。我们不能假设两个整数彼此更接近意味着它们表示的单词相关。
- en: In order to understand the meaning of words and hence quantify how two words
    are related to each other, you will use the technique **word embeddings**. But
    to understand the context, you also need to know the position of each word in
    a sentence. This is done by **positional encoding**.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解单词的含义以及量化两个单词如何相互关联，您将使用**词嵌入**技术。但是为了理解上下文，您还需要知道每个单词在句子中的位置。这是通过**位置编码**来实现的。
- en: In the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf),
    positional encoding represents each token position with a vector. The elements
    of the vector are values of the different phase and frequency of sine waves. Precisely,
    at position $k=0, 1, \cdots, L-1$, the positional encoding vector (of length $d$)
    is
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)中，位置编码使用一个向量来表示每个令牌的位置。向量的元素是正弦波的不同相位和频率的值。具体来说，在位置
    $k=0, 1, \cdots, L-1$ 处，位置编码向量（长度为 $d$）为
- en: $$
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: '[P(k,0), P(k,1), \cdots, P(k,d-2), P(k,d-1)]'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[P(k,0), P(k,1), \cdots, P(k,d-2), P(k,d-1)]'
- en: $$
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: where for $i=0, 1, \cdots, d/2$,
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中对于 $i=0, 1, \cdots, d/2$，
- en: $$
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: \begin{aligned}
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: \begin{aligned}
- en: P(k, 2i) &= \sin\Big(\frac{k}{n^{2i/d}}\Big) \\
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: P(k, 2i) &= \sin\Big(\frac{k}{n^{2i/d}}\Big) \\
- en: P(k, 2i+1) &= \cos\Big(\frac{k}{n^{2i/d}}\Big)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: P(k, 2i+1) &= \cos\Big(\frac{k}{n^{2i/d}}\Big)
- en: \end{aligned}
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: \end{aligned}
- en: $$
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: In the paper, they used $n=10000$.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中，他们使用了 $n=10000$。
- en: Implementing the positional encoding is not difficult, especially if you can
    use vector functions from NumPy.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 实现位置编码并不困难，特别是如果您可以使用 NumPy 中的向量函数。
- en: '[PRE9]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can see that we created a function to generate the positional encoding.
    We tested it out with $L=2048$ and $d=512$ above. The output would be a $2048\times
    512$ matrix. We also plot the encoding in a heatmap. This should look like the
    following.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我们创建了一个函数来生成位置编码。我们使用$L=2048$和$d=512$进行了测试。输出将是一个$2048\times 512$的矩阵。我们还在热图中绘制了编码。这应该看起来像下面这样。
- en: '![](../Images/80121f4ffaa84888976884f691fb00a7.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80121f4ffaa84888976884f691fb00a7.png)'
- en: Heatmap representation of the positional encoding matrix
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码矩阵的热图表示
- en: Your Task
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你的任务
- en: 'The heatmap above may not be very appealing to you. A better way to visualize
    it is to separate the sine curves from the cosine curves. Try out the code below
    to reuse the pickled positional encoding matrix and obtain a clearer visualization:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 上述热图可能对你来说不太吸引人。更好的可视化方法是将正弦曲线与余弦曲线分开。尝试下面的代码重新使用序列化的位置编码矩阵，并获得更清晰的可视化：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If you wish, you may check that the different “depth” in the matrix represents
    a sine curve of different frequency. An example to visualize them is the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意，你可以检查矩阵中不同“深度”代表不同频率的正弦曲线。一个可视化它们的例子如下所示：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'But if you visualize one “position” of the matrix, you see an interesting curve:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果你可视化矩阵的一个“位置”，你会看到一个有趣的曲线：
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'which shows you this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 显示给你这个：
- en: '![](../Images/c63a26964e15e4b7c17717435899a686.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c63a26964e15e4b7c17717435899a686.png)'
- en: One encoding vector
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一个编码向量
- en: 'The encoding matrix is useful in the sense that, when you compare two encoding
    vectors, you can tell how far apart their positions are. The dot-product of two
    normalized vectors is 1 if they are identical and drops quickly as they move apart.
    This relationship can be visualized below:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 编码矩阵在比较两个编码向量时很有用，可以告诉它们位置有多远。两个归一化向量的点积如果相同则为1，当它们移动时迅速下降。这种关系可以在下面的可视化中看到：
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/c100472938c63436bbd1463812abcf70.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c100472938c63436bbd1463812abcf70.png)'
- en: Dot-product of normalized positional encoding vectors
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化位置编码向量的点积
- en: In the next lesson, you will make use of the positional encoding matrix to build
    a positional encoding layer in Keras.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一课中，你将利用位置编码矩阵在Keras中构建一个位置编码层。
- en: 'Lesson 05: Positional Encoding Layer'
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第05课：位置编码层
- en: '![](../Images/fc1e4d7b9ab803c6ae6c1c8da35d116b.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc1e4d7b9ab803c6ae6c1c8da35d116b.png)'
- en: The transformer model
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型
- en: 'The transformer model from the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
    is illustrated below:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 论文[注意力机制](https://arxiv.org/pdf/1706.03762.pdf)中的Transformer模型如下所示：
- en: The positional encoding layer is at the entry point of a transformer model.
    However, the Keras library does not provide us one. You can create a custom layer
    to implement the positional encoding, as follows.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码层位于Transformer模型的入口处。但是，Keras库并未提供此功能。你可以创建一个自定义层来实现位置编码，如下所示。
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This layer is indeed combining an embedding layer with position encoding. The
    embedding layer creates word embeddings, namely, converting an integer token label
    from the vectorized sentence into a vector that can carry the meaning of the word.
    With the embedding, you can tell how close in meaning the two different words
    are.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层确实结合了嵌入层和位置编码。嵌入层创建词嵌入，即将向量化的句子中的整数标记标签转换为能够携带单词含义的向量。通过嵌入，你可以知道两个不同单词在含义上有多接近。
- en: The embedding output depends on the tokenized input sentence. But the positional
    encoding is a constant matrix as it depends only on the position. Hence you create
    a constant tensor for that at the time you created this layer. TensorFlow is smart
    enough to match the dimensions when you add the embedding output to the positional
    encoding matrix, in the `call()` function.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入输出取决于标记化的输入句子。但是位置编码是一个常数矩阵，因为它仅依赖于位置。因此，你在创建此层时创建了一个常量张量。当你将嵌入输出添加到位置编码矩阵中时，TensorFlow会智能地匹配维度，在`call()`函数中。
- en: Two additional functions are defined in the layer above. The `compute_mask()`
    function is passed on to the embedding layer. This is needed to tell which positions
    of the output are padded. This will be used internally by Keras. The `get_config()`
    function is defined to remember all the config parameters of this layer. This
    is a standard practice in Keras so that you remember all the parameters you passed
    on to the constructor and return them in `get_config()`, so the model can be saved
    and loaded.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一层定义了两个额外的函数。`compute_mask()` 函数传递给了嵌入层。这是为了告诉输出的哪些位置是填充的。这将在 Keras 内部使用。`get_config()`
    函数被定义为记住此层的所有配置参数。这是 Keras 中的标准实践，以便记住传递给构造函数的所有参数，并在 `get_config()` 中返回它们，以便可以保存和加载模型。
- en: Your Task
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你的任务
- en: 'Combine the above code together with the dataset `train_ds` created in Lesson
    03 and the code snippet below:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 将上述代码与第03课中创建的数据集 `train_ds` 以及下面的代码片段结合起来：
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You should see the output like the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到类似以下的输出：
- en: '[PRE16]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You can see that the first tensor printed above is one batch (64 samples) of
    the vectorized input sentences, padded with zero to length 20\. Each token is
    an integer but will be converted into an embedding of dimension 512\. Hence the
    shape of `en_emb` above is `(64, 20, 512)`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到上面打印的第一个张量是一个批次（64个样本）的向量化输入句子，填充为长度20。每个标记是一个整数，但将转换为512维的嵌入。因此，`en_emb`
    的形状如上所示为 `(64, 20, 512)`。
- en: The last tensor printed above is the mask used. This essentially matches the
    input where the position is not zero. When you compute the accuracy, you have
    to remember the padded locations should not be counted.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 上面打印的最后一个张量是使用的掩码。这实质上与输入匹配，其中位置不为零。当计算准确率时，必须记住不应计算填充位置。
- en: In the next lesson, you will complete the other building block of the transformer
    model.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一课中，您将完成 Transformer 模型的另一个构建模块。
- en: 'Lesson 06: Transformer Building Blocks'
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第06课：Transformer 构建模块
- en: Reviewing the diagram of transformer in Lesson 05, you will see that beyond
    the embedding and positional encoding, you have the encoder (left half of the
    figure) and decoder (right half of the figure). They share some similarities.
    Most notably, they have a multi-head attention block at the beginning and a feed
    forward block at the end.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾第05课中 Transformer 的图表，您将看到除了嵌入和位置编码之外，还有编码器（图表的左半部分）和解码器（图表的右半部分）。它们共享一些相似之处。特别是它们在开始处都有一个多头注意力块，并在末尾有一个前馈块。
- en: It would be easier if you create each building block as separate submodels and
    later combine them into a bigger model.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将每个构建模块单独创建为子模型，稍后再将它们组合成一个更大的模型将会更容易。
- en: 'First, you create the **self-attention model**. It is in the part of the diagram
    that is at the bottom of both encoder and decoder. A multi-head attention layer
    will take three inputs, namely, the key, the value, and the query. If all three
    inputs are the same, we call this multi-head attention layer self-attention. This
    submodel will have an **add & norm** layer with **skip connection** to normalize
    the output of the attention layer. Its implementation is as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您创建**自注意力模型**。它位于图中编码器和解码器的底部部分。一个多头注意力层将接受三个输入，即键、值和查询。如果三个输入都相同，我们称这个多头注意力层为自注意力。这个子模型将具有一个**加和标准化**层，其具有**跳跃连接**以归一化注意力层的输出。其实现如下：
- en: '[PRE17]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The function defined above is generic for both encoder and decoder. The decoder
    will set the option `mask=True` to apply **causal mask** to the input.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 上面定义的函数对编码器和解码器都是通用的。解码器将设置选项 `mask=True` 来对输入应用**因果掩码**。
- en: Set some parameters and create a model. The model plotted would look like the
    following.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 设置一些参数并创建一个模型。绘制的模型将如下所示：
- en: '![](../Images/3ee0bd0cd6890d56f4cebbda67e3783d.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ee0bd0cd6890d56f4cebbda67e3783d.png)'
- en: Self-attention architecture with key dimension=128
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力架构，关键维度=128
- en: 'In the decoder, you have a **cross-attention model** that takes input from
    the self-attention model as well as the encoder. In this case, the value and key
    are the output from the encoder whereas the query is the output from the self-attention
    model. At the high level, it is based on what the encoder understands about the
    context of the source sentence, and takes the partial sentence at the decoder’s
    input as the query (which can be empty), to predict how to complete the sentence.
    This is the only difference from the self-attention model; hence the code is very
    similar:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码器中，您有一个从自注意力模型和编码器输入的**交叉注意力模型**。在这种情况下，编码器的输出是值和键，而自注意力模型的输出是查询。在高层次上，它基于编码器对源句子上下文的理解，并将解码器输入的部分句子作为查询（可以为空），以预测如何完成句子。这与自注意力模型的唯一区别是，因此代码非常相似：
- en: '[PRE18]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The model plotted would look like the following. Note that there are two inputs
    in this model, one for the **context** and another for the input from self-attention.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制的模型将如下所示。请注意，该模型有两个输入，一个用于**上下文**，另一个用于自注意力的输入。
- en: '![](../Images/4da6f6fe47cf8db86f12412ae25e8bea.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4da6f6fe47cf8db86f12412ae25e8bea.png)'
- en: Cross-attention architecture with key dimension=128
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 具有键维度为 128 的交叉注意力架构
- en: 'Finally, there are feed forward models at the output of both encoder and decoder.
    It is implemented as `Dense` layers in Keras:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在编码器和解码器的输出处都有前馈模型。在 Keras 中，它实现为`Dense`层：
- en: '[PRE19]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The model plotted would look like the following. Note that the first `Dense`
    layer uses ReLU activation and the second has no activation. A dropout layer is
    then appended for regularization.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制的模型将如下所示。请注意，第一个`Dense`层使用 ReLU 激活，第二个层没有激活。然后添加了一个 dropout 层以进行正则化。
- en: '![](../Images/1659b00be6198b7697b7cd48eb7f5c47.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1659b00be6198b7697b7cd48eb7f5c47.png)'
- en: Feed-forward submodel
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈子模型
- en: Your Task
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 您的任务
- en: Run the above code and verify you see the same model diagram. It is important
    you match the layout as the final transformer model depends on them.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码并验证您看到相同的模型图。重要的是您与布局相匹配，因为最终的变压器模型取决于它们。
- en: In the code above, Keras functional API is used. In Keras, you can build a model
    using sequential API, functional API, or subclass the `Model` class. Subclassing
    can also be used here, but sequential API cannot. Can you tell why?
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，使用了 Keras 的函数式 API。在 Keras 中，您可以使用顺序 API、函数式 API 或者子类化`Model`类来构建模型。子类化也可以在这里使用，但顺序
    API 不行。您能告诉为什么吗？
- en: In the next lesson, you will make use of these building block to create the
    encoder and decoder.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节课中，您将利用这些构建块来创建编码器和解码器。
- en: 'Lesson 07: Transformer Encoder and Decoder'
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 07 课：变压器编码器和解码器
- en: Look again at the diagram of the transformer in Lesson 05\. You will see that
    the encoder is the self-attention submodel connected to the feed-forward submodel.
    The decoder, on the other hand, is a self-attention submodel, a cross-attention
    submodel, and a feed-forward submodel connected in tandem.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 再次查看第 05 课中变压器的图表。您会看到编码器是自注意力子模型连接到前馈子模型。另一方面，解码器是一个自注意力子模型，一个交叉注意力子模型，以及一个串联的前馈子模型。
- en: Making an encoder and a decoder is therefore not difficult once you have these
    submodels as building blocks. Firstly, you have the encoder. It is simple enough
    that you can build an encoder model using Keras sequential API.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦有了这些子模型作为构建块，创建编码器和解码器就不难了。首先，您有编码器。它足够简单，可以使用 Keras 顺序 API 构建编码器模型。
- en: '[PRE20]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Plotting the model would see that it is simple as the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制模型会看到它就像下面一样简单：
- en: '![](../Images/02cc2087f24e808f5cb0985d8533bd65.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02cc2087f24e808f5cb0985d8533bd65.png)'
- en: Encoder submodel
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器子模型
- en: 'The decoder is a bit complicated because the cross-attention block takes input
    from the encoder as well; hence it is a model that takes two input. It is implemented
    as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器有些复杂，因为交叉注意力块还从编码器那里获取输入；因此，这是一个接受两个输入的模型。它实现如下：
- en: '[PRE21]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The model will look like the following:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将如下所示：
- en: '![](../Images/3b547c34ab19f16d9019f19a0d729dc6.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b547c34ab19f16d9019f19a0d729dc6.png)'
- en: Decoder submodel
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器子模型
- en: Your Task
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 您的任务
- en: Copy over the three building block functions from Lesson 06 and run the above
    code to make sure you see the same layout as shown, in both the encoder and decoder.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 从第 06 课中复制三个构建块函数，并运行上述代码以确保您看到与编码器和解码器中所示的相同布局。
- en: In the next lesson, you will complete the transformer model with the building
    block you have created so far.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节课中，您将使用到目前为止创建的构建块来完成变压器模型。
- en: 'Lesson 08: Building a Transformer'
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 08 课：构建变压器
- en: Indeed, a transformer has encoder and decoder parts, and each part is not one
    but a series of encoders or decoders. It sounds complicated but not if you have
    the building block submodels to hide the details.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，一个变压器有编码器和解码器部分，每部分不仅仅是一个，而是一系列的编码器或解码器。听起来复杂，但如果你有构建块子模型来隐藏细节，就不那么复杂了。
- en: Refer to the figure in Lesson 05, and you see the encoder and decoder parts
    are just a chain of encoder and decoder blocks. Only the output of the final encoder
    block is used as input to the decoder blocks.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 参见第 05 课的图示，你会看到编码器和解码器部分只是编码器和解码器块的链。只有最后一个编码器块的输出被用作解码器块的输入。
- en: 'Therefore, the complete transformer model can be built as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，完整的变压器模型可以按如下方式构建：
- en: '[PRE22]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `try`–`except` block in the code is to handle a bug in certain versions
    of TensorFlow that may cause the training error calculated erroneously. The model
    plotted above would be like the following. Not very simple, but the architecture
    is still tractable.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的 `try`–`except` 块用于处理某些版本的 TensorFlow 中可能导致训练错误计算不正确的 bug。上面绘制的模型将如下所示。虽然不简单，但架构仍然是可处理的。
- en: '![](../Images/4ca6fa7ca12defc6a1c8e5bdfcdbbf7b.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ca6fa7ca12defc6a1c8e5bdfcdbbf7b.png)'
- en: Transformer with 4 layers in encoder and 4 layers in decoder
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器中有 4 层，解码器中有 4 层的变压器
- en: Your Task
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你的任务
- en: Copy over the three building block functions from Lessons 05, 06, and 07, so
    you can run the above code and generate the same diagram. You will reuse this
    model in the subsequent lessons.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 从第 05、06 和 07 课中复制三个构建块函数，以便你可以运行上述代码并生成相同的图示。你将在后续课程中重用此模型。
- en: In the next lesson, you will set up the other training parameters for this model.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一课中，你将为此模型设置其他训练参数。
- en: 'Lesson 09: Prepare the Transformer Model for Training'
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 09 课：准备变压器模型进行训练
- en: Before you can train your transformer, you need to decide how you should train
    it.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在你可以训练你的变压器之前，你需要决定如何训练它。
- en: According to the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf),
    you are using Adam as the optimizer but with a custom learning rate schedule,
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 根据论文 [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)，你正在使用
    Adam 作为优化器，但使用了自定义学习率调度，
- en: $$\text{LR} = \frac{1}{\sqrt{d_{\text{model}}}} \min\big(\frac{1}{\sqrt{n}},
    \frac{n}{\sqrt{m^3}}\big)$$
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: $$\text{LR} = \frac{1}{\sqrt{d_{\text{model}}}} \min\big(\frac{1}{\sqrt{n}},
    \frac{n}{\sqrt{m^3}}\big)$$
- en: 'It is implemented as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 实现如下：
- en: '[PRE23]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The learning rate schedule is designed in such a way that it learns slowly
    at the beginning but accelerates as it learns. This helps because the model is
    totally random at the beginning, and you cannot even trust the output much. But
    as you train the model enough, the result should be sufficiently sensible and
    thus you can learn faster to help convergence. The learning rate as plotted would
    look like the following:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率调度的设计方式是，开始时学习较慢，但随着学习加速。这有助于因为模型在开始时完全是随机的，你甚至无法过多信任输出。但随着你足够训练模型，结果应该足够合理，因此你可以更快地学习以帮助收敛。绘制的学习率看起来如下：
- en: '![](../Images/d9e961b1139a64f8f714751c4cbe6353.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9e961b1139a64f8f714751c4cbe6353.png)'
- en: Customized learning rate schedule
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义学习率调度
- en: 'Next, you also need to define the loss metric and accuracy metric for training.
    This model is special because you need to apply a mask to the output to calculate
    the loss and accuracy only on the non-padding elements. Borrow the implementation
    from TensorFlow’s tutorial [Neural machine translation with a Transformer and
    Keras](https://www.tensorflow.org/text/tutorials/transformer):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你还需要定义用于训练的损失度量和准确度量。这个模型很特别，因为你需要对输出应用掩码，仅在非填充元素上计算损失和准确度。从 TensorFlow
    的教程 [使用 Transformer 和 Keras 进行神经机器翻译](https://www.tensorflow.org/text/tutorials/transformer)
    中借用实现：
- en: '[PRE24]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'With all these, you can now **compile** your Keras model as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，你现在可以如以下方式**编译**你的 Keras 模型：
- en: '[PRE25]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Your Task
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你的任务
- en: If you have implemented everything correctly, you should be able to provide
    all building block functions to make the above code run. Try to keep everything
    you made so far in one Python script or one Jupyter notebook and run it once to
    ensure no errors produced and no exceptions are raised.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已正确实现所有功能，你应该能够提供所有构建块函数以使上述代码运行。尽量将到目前为止所做的所有内容保留在一个 Python 脚本或一个 Jupyter
    笔记本中，并运行一次以确保没有错误产生且没有引发异常。
- en: 'If everything run smoothly, you should see the `summary()` above prints the
    following:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利运行，你应该看到 `summary()` 输出如下：
- en: '[PRE26]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Moreover, when you look at the diagram of the transformer model and your implementation
    here, you should notice the diagram shows a softmax layer at the output, but we
    omitted that. The softmax is indeed added in this lesson. Do you see where is
    it?
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当您查看 Transformer 模型的图表及其在此处的实现时，您应该注意到图表显示了一个 softmax 层作为输出，但我们省略了它。在本课程中确实添加了
    softmax。你看到它在哪里了吗？
- en: In the next lesson, you will train this compiled model, on 14 million parameters
    as we can see in the summary above.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一课中，您将训练此编译模型，该模型有 1400 万个参数，正如我们在上面的摘要中所看到的。
- en: 'Lesson 10: Training the Transformer'
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 10 课：训练 Transformer
- en: Training the transformer depends on everything you created in all previous lessons.
    Most importantly, the vectorizer and dataset from Lesson 03 must be saved as they
    will be reused in this and the next lessons.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 Transformer 取决于您在所有先前课程中创建的一切。最重要的是，从第 03 课中的向量化器和数据集必须保存下来，因为它们将在本课程和接下来的课程中重复使用。
- en: '[PRE27]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: That’s it!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！
- en: 'Running this script will take several hours, but once it is finished, you will
    have the model saved and the loss and accuracy plotted. It should look like the
    following:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此脚本将需要几个小时，但一旦完成，您将保存模型和损失与准确率绘图。它应该看起来像下面这样：
- en: '![](../Images/c1d0c0963087d4467470e7987dec3eac.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1d0c0963087d4467470e7987dec3eac.png)'
- en: Loss and accuracy history from the training
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中的损失和准确率历史记录
- en: Your Task
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你的任务
- en: In the training set up above, we did not make use of the early stopping and
    checkpoint callbacks in Keras. Before you run it, try to modify the code above
    to add these callbacks.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述的训练设置中，我们没有使用 Keras 中的提前停止和检查点回调功能。在运行之前，请尝试修改上述代码以添加这些回调。
- en: The early stopping callback can help you interrupt the training when no progress
    is made. The checkpoint callback can help you keep the best-score model rather
    than return to you only the final model at the last epoch.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 提前停止回调可以在没有进展时中断训练。检查点回调可以帮助您保存最佳分数的模型，而不是仅返回最后一个 epoch 的最终模型。
- en: In the next lesson, you will load this trained model and test it out.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一课中，您将加载这个训练好的模型并进行测试。
- en: 'Lesson 11: Inference from the Transformer Model'
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 11 课：从 Transformer 模型进行推断
- en: In Lesson 03, you split the original dataset into training, validation, and
    test sets in the ratio of 70%-15%-15%. You used the training and validation dataset
    in the training of the transformer model in Lesson 10\. And in this lesson, you
    are going to use the test set to see how good your trained model is.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 03 课中，您将原始数据集按 70%-15%-15% 的比例分割为训练、验证和测试集。您在第 10 课的 Transformer 模型训练中使用了训练和验证数据集。而在本课程中，您将使用测试集来查看您训练好的模型表现如何。
- en: You saved your transformer model in the previous lesson. Because you have some
    custom made layers and functions in the model, you need to create a **custom object
    scope** to load your saved model.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 您在前一课中保存了您的 Transformer 模型。由于模型中有一些自定义层和函数，您需要创建一个 **自定义对象作用域** 来加载您保存的模型。
- en: The transformer model can give you a token index. You need the vectorizer to
    look up the word that this index represents. You have to reuse the same vectorizer
    that you used in creating the dataset to maintain consistency.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型可以给您一个标记索引。您需要使用向量化器查找此索引代表的单词。为了保持一致性，您必须重用与创建数据集时使用的相同向量化器。
- en: Create a loop to scan the generated tokens. In other words, do not use the model
    to generate the entire translated sentence but consider only the next generated
    word in the sentence until you see the end sentinel. The first generated word
    would be the one generated by the start sentinel. It is the reason you processed
    the target sentences this way in Lesson 02.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个循环以扫描生成的标记。换句话说，不要使用模型生成整个翻译的句子，而是只考虑直到看到结束标志的下一个生成词。第一个生成的词将是由起始标志生成的词。这也是您在第
    02 课中处理目标句子的方式的原因。
- en: 'The code is as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码：
- en: '[PRE28]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Your Task
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你的任务
- en: 'First, try to run this code and observe the inference result. Some examples
    are below:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，尝试运行此代码并观察推断结果。下面是一些示例：
- en: '[PRE29]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The second line of each test is the expected output while the third line is
    the output from the transformer.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 每个测试的第二行是预期输出，而第三行是 Transformer 的输出。
- en: The token `[UNK]` means “unknown” or out-of-vocabulary, which should appear
    rarely. Comparing the output, you should see the result is quite accurate. It
    will not be perfect. For example, *they* in English can map to *ils* or *elles*
    in French depending on the gender, and the transformer cannot always distinguish
    that.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 标记`[UNK]`表示“未知”或超出词汇范围，这种情况应该很少出现。对比输出，你会看到结果相当准确，但不会完美。例如，英语中的*they*在法语中可以映射为*ils*或*elles*，这取决于性别，而变压器模型并不总能区分这一点。
- en: You generated the translated sentence word by word, but indeed the transformer
    outputs the entire sentence in one shot. You should try to modify the program
    to decode the entire transformer output `pred` in the for-loop to see how the
    transformer gives you a better sentence as you provide more leading words in `dec_tokens`.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 你逐词生成了翻译句子，但实际上变压器一次性输出整个句子。你应该尝试修改程序，以在for循环中解码整个变压器输出`pred`，看看当你提供更多的前导词`dec_tokens`时，变压器如何为你生成更好的句子。
- en: In the next lesson, you will review what you did so far and see if any improvements
    can be made.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一课中，你将回顾到目前为止的工作，并查看是否可以进行改进。
- en: 'Lesson 12: Improving the Model'
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 课程12：改进模型
- en: You did it!
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 你做到了！
- en: Let’s go back and review what you did and what can be improved. You made a transformer
    model that takes an entire English sentence and a partial French sentence (up
    to the $k$-th token) to predict the next (the $(k+1)$-th) token.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下你做了什么以及可以改进的地方。你创建了一个变压器模型，该模型接受整个英语句子和一个部分法语句子（最多到第$k$个标记）以预测下一个（第$(k+1)$个）标记。
- en: 'In training, you observed that the accuracy is at 70% to 80% at the best. How
    can you improve it? Here are some ideas, but surely, not exhaustive:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练中，你观察到最佳准确率为70%到80%。你如何改进它？这里有一些想法，但肯定不是详尽无遗的：
- en: You used a simple tokenizer for your text input. Libraries such as NLTK can
    provide better tokenizers. Also, you didn’t use subword tokenization. It is less
    a problem for English but problematic for French. That’s why you have vastly larger
    vocabulary size in French in your model (e.g., *l’air* (the air) and *d’air* (of
    air) would become distinct tokens).
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你为文本输入使用了一个简单的标记器。像NLTK这样的库可以提供更好的标记器。此外，你没有使用子词标记化。对于英语，这不是大问题，但对于法语则比较棘手。这就是为什么在你的模型中法语词汇量大得多（例如，*l’air*（空气）和*d’air*（的空气）将成为不同的标记）。
- en: You trained your own word embeddings with an embedding layer. There are pre-trained
    embeddings (such as GloVe) readily available, and they usually provide better
    quality embeddings. This may help your model to understand the **context** better.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你用嵌入层训练了自己的词嵌入。已经有现成的预训练嵌入（如GloVe），它们通常提供更好的质量的嵌入。这可能有助于你的模型更好地理解**上下文**。
- en: You designed the transformer with some parameters. You used 8 heads for multi-head
    attention, output vector dimension is 128, sentence length was limited to 20 tokens,
    drop out rate is 0.1, and so on. Tuning these parameters will surely impact the
    transformer one way or another. Similarly important are the training parameters
    such as number of epochs, learning rate schedule, and loss function.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你为变压器设计了一些参数。你使用了8个头的多头注意力，输出向量维度为128，句子长度限制为20个标记，丢弃率为0.1，等等。调整这些参数肯定会对变压器产生影响。训练参数同样重要，如轮次数量、学习率调度和损失函数。
- en: Your Task
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你的任务
- en: Figure out how to change the code to accomodate the above changes. But if we
    test it out, do you know the right way to tell if one model is better than another?
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 找出如何更改代码以适应上述变化。但如果我们测试一下，你知道如何判断一个模型是否优于另一个模型吗？
- en: Post your answer in the comments below. I would love to see what you come up
    with.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的评论中发布你的答案。我很想看看你提出了什么。
- en: This was the final lesson.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是最后一课。
- en: The End! (*Look How Far You Have Come*)
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结束！(*看看你走了多远*)
- en: You made it. Well done!
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 你做到了。干得好！
- en: Take a moment and look back at how far you have come.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 花点时间回顾一下你取得的进展。
- en: You learned how to take a plaintext sentence, process it, and vectorize it
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你学会了如何处理一个纯文本句子并将其向量化
- en: You analyzed the building block of a transformer model according to the paper
    [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf), and implemented
    each building block using Keras
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你根据论文[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)分析了变压器模型的构建块，并使用Keras实现了每个构建块。
- en: You connected the building blocks into a complete transformer model, and train
    it
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将构建块连接成一个完整的变压器模型，并进行了训练。
- en: Finally, you can witness the trained model to translate English sentences into
    French with high accuracy
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，你可以见证经过训练的模型以高精度将英语句子翻译成法语。
- en: Summary
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: '**How did you do with the mini-course?**'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**你在这个迷你课程中表现如何？**'
- en: Did you enjoy this crash course?
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 你喜欢这个速成课程吗？
- en: '**Do you have any questions? Were there any sticking points?**'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**你有任何问题吗？是否有遇到难点？**'
- en: Let me know. Leave a comment below.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 告诉我。请在下面留下评论。
