- en: The Vision Transformer Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉Transformer模型
- en: 原文：[https://machinelearningmastery.com/the-vision-transformer-model/](https://machinelearningmastery.com/the-vision-transformer-model/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/the-vision-transformer-model/](https://machinelearningmastery.com/the-vision-transformer-model/)
- en: With the Transformer architecture revolutionizing the implementation of attention,
    and achieving very promising results in the natural language processing domain,
    it was only a matter of time before we could see its application in the computer
    vision domain too. This was eventually achieved with the implementation of the
    Vision Transformer (ViT).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Transformer架构在自然语言处理领域实现了令人鼓舞的结果，计算机视觉领域的应用也只是时间问题。这最终通过视觉Transformer（ViT）的实现得以实现。
- en: In this tutorial, you will discover the architecture of the Vision Transformer
    model, and its application to the task of image classification.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您将发现视觉Transformer模型的架构，以及它在图像分类任务中的应用。
- en: 'After completing this tutorial, you will know:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本教程后，您将了解：
- en: How the ViT works in the context of image classification.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViT在图像分类中的工作原理。
- en: What the training process of the ViT entails.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViT的训练过程。
- en: How the ViT compares to convolutional neural networks in terms of inductive
    bias.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViT与卷积神经网络在归纳偏置方面的比较。
- en: How the ViT fares against ResNets on different datasets.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViT在不同数据集上与ResNets的比较表现如何。
- en: How the data is processed internally for the ViT to achieve its performance.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViT如何在内部处理数据以实现其性能。
- en: '**Kick-start your project** with my book [Building Transformer Models with
    Attention](https://machinelearningmastery.com/transformer-models-with-attention/).
    It provides **self-study tutorials** with **working code** to guide you into building
    a fully-working transformer model that can'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**启动您的项目**，可以参考我的书籍 [《构建注意力的Transformer模型》](https://machinelearningmastery.com/transformer-models-with-attention/)。它提供了**自学教程**和**工作代码**，帮助您构建一个完全可用的Transformer模型。'
- en: '*translate sentences from one language to another*...'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*将句子从一种语言翻译成另一种语言*……'
- en: Let’s get started.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: '[![](../Images/4dd1fd5278b5a856167c128069c49f24.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/vit_cover-scaled.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/4dd1fd5278b5a856167c128069c49f24.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/vit_cover-scaled.jpg)'
- en: The Vision Transformer Model
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉Transformer模型
- en: Photo by [Paul Skorupskas](https://unsplash.com/photos/7KLa-xLbSXA), some rights
    reserved.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Paul Skorupskas](https://unsplash.com/photos/7KLa-xLbSXA) 提供，部分权利保留。
- en: '**Tutorial Overview**'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**教程概述**'
- en: 'This tutorial is divided into six parts; they are:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程分为六个部分，它们是：
- en: Introduction to the Vision Transformer (ViT)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉Transformer（ViT）简介
- en: The ViT Architecture
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViT架构
- en: Training the ViT
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练ViT
- en: Inductive Bias in Comparison to Convolutional Neural Networks
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与卷积神经网络相比的归纳偏置
- en: Comparative Performance of ViT Variants with ResNets
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViT变体与ResNets的比较性能
- en: Internal Representation of Data
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的内部表示
- en: '**Prerequisites**'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**前提条件**'
- en: 'For this tutorial, we assume that you are already familiar with:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本教程，我们假设您已经熟悉：
- en: '[The concept of attention](https://machinelearningmastery.com/what-is-attention/)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[注意力的概念](https://machinelearningmastery.com/what-is-attention/)'
- en: '[The Transfomer attention mechanism](https://machinelearningmastery.com/the-transformer-attention-mechanism)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Transformer注意力机制](https://machinelearningmastery.com/the-transformer-attention-mechanism)'
- en: '[The Transformer Model](https://machinelearningmastery.com/the-transformer-model/)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Transformer模型](https://machinelearningmastery.com/the-transformer-model/)'
- en: '**Introduction to the Vision Transformer (ViT)**'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**视觉Transformer（ViT）简介**'
- en: We had seen how the emergence of the Transformer architecture of [Vaswani et
    al. (2017)](https://arxiv.org/abs/1706.03762) has revolutionized the use of attention,
    without relying on recurrence and convolutions as earlier attention models had
    previously done. In their work, Vaswani et al. had applied their model to the
    specific problem of natural language processing (NLP).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，[Vaswani等人（2017）](https://arxiv.org/abs/1706.03762)的Transformer架构如何革新了注意力的使用，避免了依赖于递归和卷积的早期注意力模型。在他们的工作中，Vaswani等人将他们的模型应用于自然语言处理（NLP）的具体问题。
- en: '*In computer vision, however, convolutional architectures remain dominant …*'
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*然而，在计算机视觉中，卷积架构仍然占据主导地位……*'
- en: ''
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*–* [An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929),
    2021.'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*–* [图像胜过16×16个词：用于大规模图像识别的Transformers](https://arxiv.org/abs/2010.11929)，2021年。'
- en: Inspired by its success in NLP, [Dosovitskiy et al. (2021)](https://arxiv.org/abs/2010.11929)
    sought to apply the standard Transformer architecture to images, as we shall see
    shortly. Their target application at the time was image classification.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 受到在自然语言处理中的成功启发，Dosovitskiy等人（2021年）试图将标准Transformer架构应用于图像，我们很快将看到。他们当时的目标应用是图像分类。
- en: Want to Get Started With Building Transformer Models with Attention?
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 想要开始构建带注意力的Transformer模型吗？
- en: Take my free 12-day email crash course now (with sample code).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在就参加我的免费12天电子邮件速成课程（附有示例代码）。
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 点击注册，还可获得免费PDF电子书版本的课程。
- en: '**The ViT Architecture**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ViT架构**'
- en: Recall that the standard Transformer model received a one-dimensional sequence
    of word embeddings as input, since it was originally meant for NLP. In contrast,
    when applied to the task of image classification in computer vision, the input
    data to the Transformer model is provided in the form of two-dimensional images.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，标准Transformer模型接收一维序列的单词嵌入作为输入，因为它最初是为自然语言处理设计的。相反，当应用于计算机视觉中的图像分类任务时，Transformer模型的输入数据以二维图像的形式提供。
- en: For the purpose of structuring the input image data in a manner that resembles
    how the input is structured in the NLP domain (in the sense of having a sequence
    of individual words), the input image, of height $H$, width $W$, and $C$ number
    of channels, is *cut up* into smaller two-dimensional patches. This results into
    $N = \tfrac{HW}{P^2}$ number of patches, where each patch has a resolution of
    ($P, P$) pixels.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以类似自然语言处理（NLP）领域中单词序列的方式结构化输入图像数据（意味着有一个单词序列的序列），输入图像的高度为$H$，宽度为$W$，有$C$个通道，被切割成更小的二维补丁。这导致产生$N
    = \tfrac{HW}{P^2}$个补丁，每个补丁的分辨率为($P, P$)像素。
- en: 'Before feeding the data into the Transformer, the following operations are
    applied:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据馈送到Transformer之前，执行以下操作：
- en: Each image patch is flattened into a vector, $\mathbf{x}_p^n$, of length $P^2
    \times C$, where $n = 1, \dots N$.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个图像补丁被扁平化为长度为$P^2 \times C$的向量$\mathbf{x}_p^n$，其中$n = 1, \dots N$。
- en: A sequence of embedded image patches is generated by mapping the flattened patches
    to $D$ dimensions, with a trainable linear projection, $\mathbf{E}$.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过可训练的线性投影$\mathbf{E}$，将扁平化的补丁映射到$D$维度，生成嵌入的图像补丁序列。
- en: A learnable class embedding, $\mathbf{x}_{\text{class}}$, is prepended to the
    sequence of embedded image patches. The value of $\mathbf{x}_{\text{class}}$ represents
    the classification output, $\mathbf{y}$.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可学习的类别嵌入$\mathbf{x}_{\text{class}}$被前置到嵌入图像补丁序列中。$\mathbf{x}_{\text{class}}$的值代表分类输出$\mathbf{y}$。
- en: The patch embeddings are finally augmented with one-dimensional positional embeddings,
    $\mathbf{E}_{\text{pos}}$, hence introducing positional information into the input,
    which is also learned during training.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终，补丁嵌入向量最终与一维位置嵌入$\mathbf{E}_{\text{pos}}$相结合，从而将位置信息引入输入中，该信息在训练期间也被学习。
- en: 'The sequence of embedding vectors that results from the aforementioned operations
    is the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由上述操作产生的嵌入向量序列如下：
- en: $$\mathbf{z}_0 = [ \mathbf{x}_{\text{class}}; \; \mathbf{x}_p^1 \mathbf{E};
    \; \dots ; \; \mathbf{x}_p^N \mathbf{E}] + \mathbf{E}_{\text{pos}}$$
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathbf{z}_0 = [ \mathbf{x}_{\text{class}}; \; \mathbf{x}_p^1 \mathbf{E};
    \; \dots ; \; \mathbf{x}_p^N \mathbf{E}] + \mathbf{E}_{\text{pos}}$$
- en: Dosovitskiy et al. make use of the encoder part of the Transformer architecture
    of Vaswani et al.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Dosovitskiy等人利用了Vaswani等人Transformer架构的编码器部分。
- en: In order to perform classification, they feed $\mathbf{z}_0$ at the input of
    the Transformer encoder, which consists of a stack of $L$ identical layers. Then,
    they proceed to take the value of $\mathbf{x}_{\text{class}}$ at the $L^{\text{th}}$
    layer of the encoder output, and feed it into a classification head.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行分类，他们在Transformer编码器的输入处输入$\mathbf{z}_0$，该编码器由$L$个相同的层堆叠而成。然后，他们继续从编码器输出的第$L^{\text{th}}$层取$\mathbf{x}_{\text{class}}$的值，并将其馈送到分类头部。
- en: '*The classification head is implemented by a MLP with one hidden layer at pre-training
    time and by a single linear layer at fine-tuning time.*'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*在预训练阶段，分类头部由具有一个隐藏层的MLP实现，在微调阶段则由单一线性层实现。*'
- en: ''
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*–* [An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929),
    2021.'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*–* [图像价值16×16字：大规模图像识别中的Transformer](https://arxiv.org/abs/2010.11929), 2021.'
- en: The multilayer perceptron (MLP) that forms the classification head implements
    Gaussian Error Linear Unit (GELU) non-linearity.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 形成分类头的多层感知机（MLP）实现了高斯误差线性单元（GELU）非线性。
- en: 'In summary, therefore, the ViT employs the encoder part of the original Transformer
    architecture. The input to the encoder is a sequence of embedded image patches
    (including a learnable class embedding prepended to the sequence), which is also
    augmented with positional information. A classification head attached to the output
    of the encoder receives the value of the learnable class embedding, to generate
    a classification output based on its state. All of this is illustrated by the
    figure below:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，ViT 使用了原始 Transformer 架构的编码器部分。编码器的输入是一个嵌入图像块的序列（包括一个附加到序列前面的可学习类别嵌入），并且还增加了位置位置信息。附加在编码器输出上的分类头接收可学习类别嵌入的值，以生成基于其状态的分类输出。所有这些都在下图中进行了说明：
- en: '[![](../Images/426d3653fd1dc676719f50c847f22bcb.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/vit_1.png)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/426d3653fd1dc676719f50c847f22bcb.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/vit_1.png)'
- en: The Architecture of the Vision Transformer (ViT)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉变换器（ViT）的架构
- en: 'Taken from “[An Image is Worth 16×16 Words: Transformers for Image Recognition
    at Scale](https://arxiv.org/abs/2010.11929)“'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 摘自 “[一张图片值 16×16 个词：用于大规模图像识别的变换器](https://arxiv.org/abs/2010.11929)”
- en: One further note that Dosovitskiy et al. make, is that the original image can,
    alternatively, be fed into a convolutional neural network (CNN) before being passed
    on to the Transformer encoder. The sequence of image patches would then be obtained
    from the feature maps of the CNN, while the ensuing process of embedding the feature
    map patches, prepending a class token, and augmenting with positional information
    remains the same.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Dosovitskiy 等人提到的另一个注意事项是，原始图像也可以在传递给 Transformer 编码器之前先输入到卷积神经网络（CNN）中。图像块序列将从
    CNN 的特征图中获得，而后续的特征图块嵌入、添加类别标记和增加位置位置信息的过程保持不变。
- en: '**Training the ViT**'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**训练 ViT**'
- en: The ViT is pre-trained on larger datasets (such as ImageNet, ImageNet-21k and
    JFT-300M) and fine-tuned to a smaller number of classes.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ViT 在更大的数据集上进行预训练（如 ImageNet、ImageNet-21k 和 JFT-300M），然后对较少的类别进行微调。
- en: During pre-training, the classification head in use that is attached to the
    encoder output, is implemented by a MLP with one hidden layer and GELU non-linearity,
    as has been described earlier.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练过程中，附加在编码器输出上的分类头由一个具有一个隐藏层和 GELU 非线性函数的 MLP 实现，如前所述。
- en: During fine-tuning, the MLP is replaced by a single (zero-initialized) feedforward
    layer of size, $D \times K$, with $K$ denoting the number of classes corresponding
    to the task at hand.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调过程中，MLP 被替换为一个大小为 $D \times K$ 的单层（零初始化）前馈层，其中 $K$ 表示与当前任务对应的类别数量。
- en: Fine-tuning is carried out on images that are of higher resolution than those
    used during pre-training, but the patch size into which the input images are cut
    is kept the same at all stages of training. This results in an input sequence
    of larger length at the fine-tuning stage, in comparison to that used during pre-training.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是在比预训练时使用的图像分辨率更高的图像上进行的，但输入图像被切割成的块大小在训练的所有阶段保持不变。这导致在微调阶段的输入序列长度比预训练阶段使用的更长。
- en: The implication of having a lengthier input sequence is that fine-tuning requires
    more position embeddings than pre-training. To circumvent this problem, Dosovitskiy
    et al. interpolate, in two-dimensions, the pre-training position embeddings according
    to their location in the original image, to obtain a longer sequence that matches
    the number of image patches in use during fine-tuning.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 输入序列长度更长的含义是，微调需要比预训练更多的位置嵌入。为了解决这个问题，Dosovitskiy 等人通过在二维上插值预训练位置嵌入，根据它们在原始图像中的位置，得到一个与微调过程中使用的图像块数量相匹配的更长序列。
- en: '**Inductive Bias in Comparison to Convolutional Neural Networks**'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**与卷积神经网络的归纳偏差比较**'
- en: Inductive bias refers to any assumptions that a model makes to generalise the
    training data and learn the target function.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 归纳偏差指的是模型为泛化训练数据和学习目标函数所做的任何假设。
- en: '*In CNNs, locality, two-dimensional neighborhood structure, and translation
    equivariance are baked into each layer throughout the whole model.*'
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*在 CNN 中，本地性、二维邻域结构和平移等变性被嵌入到模型的每一层中。*'
- en: ''
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*–* [An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929),
    2021.'
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*–* [图像的价值相当于 16×16 个词：用于大规模图像识别的 Transformers](https://arxiv.org/abs/2010.11929)，2021。'
- en: In convolutional neural networks (CNNs), each neuron is only connected to other
    neurons in its neighborhood. Furthermore, since neurons residing on the same layer
    share the same weight and bias values, any of these neurons will activate when
    a feature of interest falls within its receptive field. This results in a feature
    map that is equivariant to feature translation, which means that if the input
    image is translated, then the feature map is also equivalently translated.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积神经网络（CNNs）中，每个神经元仅与其邻域内的其他神经元连接。此外，由于同一层上的神经元共享相同的权重和偏置值，当感兴趣的特征落在其感受野内时，这些神经元中的任何一个都会被激活。这导致了一个对特征平移等变的特征图，这意味着如果输入图像被平移，则特征图也会相应平移。
- en: Dosovitskiy et al. argue that in the ViT, only the MLP layers are characterised
    by locality and translation equivariance. The self-attention layers, on the other
    hand, are described as global, because the computations that are performed at
    these layers are not constrained to a local two-dimensional neighborhood.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Dosovitskiy 等人认为在 ViT 中，只有 MLP 层具有局部性和平移等变性。另一方面，自注意力层被描述为全局的，因为在这些层上进行的计算并不局限于局部的二维邻域。
- en: 'They explain that bias about the two-dimensional neighborhood structure of
    the images is only used:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 他们解释说，对于图像的二维邻域结构的偏置仅在以下情况下使用：
- en: At the input to the model, where each image is cut into patches, hence inherently
    retaining the spatial relationship between the pixels in each patch.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型输入端，每个图像被切割成补丁，从而固有地保留了每个补丁内像素之间的空间关系。
- en: At fine-tuning, where the pre-training position embeddings are interpolated
    in two-dimensions according to their location in the original image, to produce
    a longer sequence that matches the number of image patches in use during fine-tuning.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在微调过程中，预训练的位置嵌入根据它们在原始图像中的位置进行二维插值，以生成一个更长的序列，这个序列的长度与微调过程中使用的图像补丁数量相匹配。
- en: '**Comparative Performance of ViT Variants with ResNets**'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ViT变体与ResNet的比较性能**'
- en: 'Dosovitskiy et al. pitted three ViT models of increasing size, against two
    modified ResNets of different sizes. Their experiments yield several interesting
    findings:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Dosovitskiy 等人将三个逐渐增大的 ViT 模型与两个不同尺寸的修改版 ResNet 进行对比。实验结果得出了几个有趣的发现：
- en: '**Experiment 1** – Fine-tuning and testing on ImageNet:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实验 1** – 在 ImageNet 上进行微调和测试：'
- en: When pre-trained on the smallest dataset (ImageNet), the two larger ViT models
    underperformed in comparison to their smaller counterpart. The performance of
    all ViT models remains, generally, below that of the ResNets.
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当在最小的数据集（ImageNet）上进行预训练时，两个较大的 ViT 模型的表现不如其较小的对应模型。所有 ViT 模型的表现普遍低于 ResNet。
- en: When pre-trained on a larger dataset (ImageNet-21k), the three ViT models performed
    similarly to one another, as well as to the ResNets.
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当在较大的数据集（ImageNet-21k）上进行预训练时，三个 ViT 模型的表现彼此相似，也与 ResNet 的表现相当。
- en: When pre-trained on the largest dataset (JFT-300M), the performance of the larger
    ViT models overtakes the performance of the smaller ViT and the ResNets.
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当在最大的数据集（JFT-300M）上进行预训练时，较大 ViT 模型的表现超过了较小 ViT 模型和 ResNet 的表现。
- en: '**Experiment 2** – Training on random subsets of different sizes of the JFT-300M
    dataset, and testing on ImageNet, to further investigate the effect of the dataset
    size:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实验 2** – 在 JFT-300M 数据集的随机子集上进行训练，并在 ImageNet 上进行测试，以进一步调查数据集大小的影响：'
- en: On smaller subsets of the dataset, the ViT models overfit more than the ResNet
    models, and underperform considerably.
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据集的较小子集上，ViT 模型的过拟合程度高于 ResNet 模型，并且表现显著较差。
- en: On the larger subset of the dataset, the performance of the larger ViT model
    surpasses the performance of the ResNets.
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据集的较大子集上，较大 ViT 模型的表现超过了 ResNet 模型的表现。
- en: '*This result reinforces the intuition that the convolutional inductive bias
    is useful for smaller datasets, but for larger ones, learning the relevant patterns
    directly from data is sufficient, even beneficial.*'
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*这一结果加强了这样的直觉：卷积的归纳偏置对较小的数据集是有用的，但对于较大的数据集，从数据中直接学习相关模式是足够的，甚至是有利的。*'
- en: ''
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*–* [An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929),
    2021.'
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*–* [图像的价值相当于 16×16 个词：用于大规模图像识别的 Transformers](https://arxiv.org/abs/2010.11929)，2021。'
- en: '**Internal Representation of Data**'
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**数据的内部表示**'
- en: 'In analysing the internal representation of the image data in the ViT, Dosovitskiy
    et al. find the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析ViT中图像数据的内部表示时，Dosovitskiy等人发现以下内容：
- en: 'The learned embedding filters that are initially applied to the image patches
    at the first layer of the ViT, resemble basis functions that can extract the low-level
    features within each patch:'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始应用于ViT第一层图像补丁的学习嵌入滤波器，类似于能够提取每个补丁内低级特征的基础功能：
- en: '[![](../Images/a54a98454e2146601f175248ee644209.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/vit_2.png)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/a54a98454e2146601f175248ee644209.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/vit_2.png)'
- en: Learned Embedding Filters
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 学习嵌入滤波器
- en: 'Taken from “[An Image is Worth 16×16 Words: Transformers for Image Recognition
    at Scale](https://arxiv.org/abs/2010.11929)“'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 摘自“[一张图像值16×16个词：大规模图像识别的变压器](https://arxiv.org/abs/2010.11929)”
- en: 'Image patches that are spatially close to one another in the original image,
    are characterised by learned positional embeddings that are similar:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始图像中空间接近的图像补丁，其学习的位置嵌入相似：
- en: '[![](../Images/a061c1396ef7c38eba5c9615ac33aae2.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/vit_3.png)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/a061c1396ef7c38eba5c9615ac33aae2.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/vit_3.png)'
- en: Learned Positional Embeddings
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 学习位置嵌入
- en: 'Taken from “[An Image is Worth 16×16 Words: Transformers for Image Recognition
    at Scale](https://arxiv.org/abs/2010.11929)“'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 摘自“[一张图像值16×16个词：大规模图像识别的变压器](https://arxiv.org/abs/2010.11929)”
- en: 'Several self-attention heads at the lowest layers of the model already attend
    to most of the image information (based on their attention weights), demonstrating
    the capability of the self-attention mechanism in integrating the information
    across the entire image:'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型最低层的几个自注意力头部已经关注了大部分图像信息（基于它们的注意力权重），展示了自注意力机制在整合整个图像信息方面的能力：
- en: '[![](../Images/1750de3126ed3f6f8b35d5739c0e7680.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/vit_4.png)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/1750de3126ed3f6f8b35d5739c0e7680.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/vit_4.png)'
- en: Size of Image Area Attended by Different Self-Attention Heads
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 不同自注意力头部关注的图像区域大小
- en: 'Taken from “[An Image is Worth 16×16 Words: Transformers for Image Recognition
    at Scale](https://arxiv.org/abs/2010.11929)“'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 摘自“[一张图像值16×16个词：大规模图像识别的变压器](https://arxiv.org/abs/2010.11929)”
- en: '**Further Reading**'
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: This section provides more resources on the topic if you are looking to go deeper.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望深入了解该主题，本节提供了更多资源。
- en: '**Papers**'
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**论文**'
- en: '[An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929),
    2021.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一张图像值16×16个词：大规模图像识别的变压器](https://arxiv.org/abs/2010.11929)，2021年。'
- en: '[Attention Is All You Need](https://arxiv.org/abs/1706.03762), 2017.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[注意力机制就是你所需要的](https://arxiv.org/abs/1706.03762)，2017年。'
- en: '**Summary**'
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**摘要**'
- en: In this tutorial, you discovered the architecture of the Vision Transformer
    model, and its application to the task of image classification.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您了解了Vision Transformer模型的架构及其在图像分类任务中的应用。
- en: 'Specifically, you learned:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，您学到了：
- en: How the ViT works in the context of image classification.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViT 在图像分类背景下的工作原理。
- en: What the training process of the ViT entails.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViT的训练过程包括哪些内容。
- en: How the ViT compares to convolutional neural networks in terms of inductive
    bias.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViT 在归纳偏差方面与卷积神经网络的比较。
- en: How the ViT fares against ResNets on different datasets.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViT在不同数据集上与ResNets的对比表现如何。
- en: How the data is processed internally for the ViT to achieve its performance.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViT内部如何处理数据以实现其性能。
- en: Do you have any questions?
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 您是否有任何问题？
- en: Ask your questions in the comments below and I will do my best to answer.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在下方评论区提出您的问题，我会尽力回答。
