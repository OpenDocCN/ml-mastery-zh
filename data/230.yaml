- en: Higher-Order Derivatives
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高阶导数
- en: 原文：[https://machinelearningmastery.com/higher-order-derivatives/](https://machinelearningmastery.com/higher-order-derivatives/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/higher-order-derivatives/](https://machinelearningmastery.com/higher-order-derivatives/)
- en: Higher-order derivatives can capture information about a function that first-order
    derivatives on their own cannot capture.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 高阶导数能够捕捉一阶导数无法捕捉到的信息。
- en: First-order derivatives can capture important information, such as the rate
    of change, but on their own they cannot distinguish between local minima or maxima,
    where the rate of change is zero for both. Several optimization algorithms address
    this limitation by exploiting the use of higher-order derivatives, such as in
    Newton’s method where the second-order derivatives are used to reach the local
    minimum of an optimization function.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一阶导数可以捕捉重要信息，例如变化率，但单独使用时无法区分局部最小值或最大值，在这些点的变化率为零。若干优化算法通过利用高阶导数来解决这一限制，例如在牛顿法中，使用二阶导数来达到优化函数的局部最小值。
- en: In this tutorial, you will discover how to compute higher-order univariate and
    multivariate derivatives.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你将学习如何计算高阶的单变量和多变量导数。
- en: 'After completing this tutorial, you will know:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本教程后，你将了解：
- en: How to compute the higher-order derivatives of univariate functions.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何计算单变量函数的高阶导数。
- en: How to compute the higher-order derivatives of multivariate functions.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何计算多变量函数的高阶导数。
- en: How the second-order derivatives can be exploited in machine learning by second-order
    optimization algorithms.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二阶导数如何通过二阶优化算法在机器学习中得到利用。
- en: Let’s get started.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: '[![](../Images/172c5a452062a099fb70b71428da7038.png)](https://machinelearningmastery.com/wp-content/uploads/2021/07/higher_order_cover-scaled.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/172c5a452062a099fb70b71428da7038.png)](https://machinelearningmastery.com/wp-content/uploads/2021/07/higher_order_cover-scaled.jpg)'
- en: Higher-Order Derivatives
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 高阶导数
- en: Photo by [Jairph](https://unsplash.com/photos/aT2jMKShKIs), some rights reserved.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Jairph](https://unsplash.com/photos/aT2jMKShKIs)提供，保留部分权利。
- en: '**Tutorial Overview**'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**教程概述**'
- en: 'This tutorial is divided into three parts; they are:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程分为三个部分，它们是：
- en: Higher-Order Derivatives of Univariate Functions
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单变量函数的高阶导数
- en: Higher-Order Derivatives of Multivariate Functions
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多变量函数的高阶导数
- en: Application in Machine Learning
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器学习中的应用
- en: '**Higher-Order Derivatives of Univariate Functions**'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**单变量函数的高阶导数**'
- en: In addition to [first-order derivatives](https://machinelearningmastery.com/a-gentle-introduction-to-function-derivatives/),
    which we have seen can provide us with important information about a function,
    such as its instantaneous [rate of change](https://machinelearningmastery.com/key-concepts-in-calculus-rate-of-change/),
    higher-order derivatives can also be equally useful. For example, the second derivative
    can measure the [acceleration](https://machinelearningmastery.com/applications-of-derivatives/)
    of a moving object, or it can help an optimization algorithm distinguish between
    a local maximum and a local minimum.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 除了[一阶导数](https://machinelearningmastery.com/a-gentle-introduction-to-function-derivatives/)，我们已经看到它能提供有关函数的重要信息，例如其瞬时[变化率](https://machinelearningmastery.com/key-concepts-in-calculus-rate-of-change/)，更高阶的导数也同样有用。例如，二阶导数可以测量一个移动物体的[加速度](https://machinelearningmastery.com/applications-of-derivatives/)，或者它可以帮助优化算法区分局部最大值和局部最小值。
- en: Computing higher-order (second, third or higher) derivatives of univariate functions
    is not that difficult.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 计算单变量函数的高阶（第二阶、第三阶或更高阶）导数并不难。
- en: '*The second derivative of a function is just the derivative of its first derivative.
    The third derivative is the derivative of the second derivative, the fourth derivative
    is the derivative of the third, and so on.*'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*函数的二阶导数只是其一阶导数的导数。三阶导数是二阶导数的导数，四阶导数是三阶导数的导数，以此类推。*'
- en: ''
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – Page 147, [Calculus for Dummies](https://www.amazon.com/Calculus-Dummies-Math-Science/dp/1119293499/ref=as_li_ss_tl?dchild=1&keywords=calculus&qid=1606170839&sr=8-2&linkCode=sl1&tag=inspiredalgor-20&linkId=539ed0b89e326b6eb27b1a9a028e9cee&language=en_US),
    2016.
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – 第147页，[傻瓜微积分](https://www.amazon.com/Calculus-Dummies-Math-Science/dp/1119293499/ref=as_li_ss_tl?dchild=1&keywords=calculus&qid=1606170839&sr=8-2&linkCode=sl1&tag=inspiredalgor-20&linkId=539ed0b89e326b6eb27b1a9a028e9cee&language=en_US)，2016年。
- en: 'Hence, computing higher-order derivatives simply involves differentiating the
    function repeatedly. In order to do so, we can simply apply our knowledge of the
    [power rule](https://machinelearningmastery.com/the-power-product-and-quotient-rules/).
    Let’s consider the function, *f*(*x*) = x³ + 2x² – 4x + 1, as an example. Then:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，计算高阶导数只是涉及到对函数的重复微分。为了做到这一点，我们可以简单地应用我们对[幂规则](https://machinelearningmastery.com/the-power-product-and-quotient-rules/)
    的知识。以函数 *f*(*x*) = x³ + 2x² – 4x + 1 为例：
- en: 'First derivative: *f*’(*x*) = 3*x*² + 4*x* – 4'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一阶导数：*f*’(*x*) = 3*x*² + 4*x* – 4
- en: 'Second derivative: *f*’’(*x*) = 6*x* + 4'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶导数：*f*’’(*x*) = 6*x* + 4
- en: 'Third derivative: *f*’’’(*x*) = 6'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 三阶导数：*f*’’’(*x*) = 6
- en: 'Fourth derivative: *f *^((4))(*x*) = 0'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 第四阶导数：*f*^((4))(*x*) = 0
- en: 'Fifth derivative: *f *^((5))(*x*) = 0 *etc.*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 第五阶导数：*f*^((5))(*x*) = 0 *等*
- en: What we have done here is that we have first applied the power rule to *f*(*x*)
    to obtain its first derivative, *f*’(*x*), then applied the power rule to the
    first derivative in order to obtain the second, and so on. The derivative will,
    eventually, go to zero as differentiation is applied repeatedly.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所做的是首先对 *f*(*x*) 应用幂规则以获得其一阶导数 *f*’(*x*)，然后对一阶导数应用幂规则以获得二阶导数，如此继续。导数最终会因为重复微分而趋于零。
- en: 'The application of the [product and quotient rules](https://machinelearningmastery.com/the-power-product-and-quotient-rules/)
    also remains valid in obtaining higher-order derivatives, but their computation
    can become messier and messier as the order increases. The general Leibniz rule
    simplifies the task in this aspect, by generalising the product rule to:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[乘积和商的规则](https://machinelearningmastery.com/the-power-product-and-quotient-rules/)
    的应用在求取高阶导数时仍然有效，但随着阶数的增加，其计算会变得越来越复杂。一般的莱布尼兹规则在这方面简化了任务，将乘积规则推广为：'
- en: '[![](../Images/b64895456ce4e79d3d23584147c2cd27.png)](https://machinelearningmastery.com/wp-content/uploads/2021/07/higher_order_1.png)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/b64895456ce4e79d3d23584147c2cd27.png)](https://machinelearningmastery.com/wp-content/uploads/2021/07/higher_order_1.png)'
- en: Here, the term, *n*! / *k*!(*n* – *k*)!, is the binomial coefficient from the
    binomial theorem, while *f *^(*^k*^) and *g*^(*^k*^) denote the *k*^(th) derivative
    of the functions, *f* and *g*, respectively.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，术语 *n*! / *k*!(*n* – *k*)! 是来自二项式定理的二项式系数，而 *f*^(*^k*^) 和 *g*^(*^k*^) 表示函数
    *f* 和 *g* 的 *k*^(th) 导数。
- en: 'Therefore, finding the first and second derivatives (and, hence, substituting
    for *n* = 1 and *n* = 2, respectively), by the general Leibniz rule, gives us:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，按照一般的莱布尼兹规则，找到一阶和二阶导数（因此，分别替代 *n* = 1 和 *n* = 2），我们得到：
- en: (*fg*)^((1)) = (*fg*)’ = *f *^((1)) *g* + *f* *g*^((1))
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: (*fg*)^((1)) = (*fg*)’ = *f*^((1)) *g* + *f* *g*^((1))
- en: (*fg*)^((2)) = (*fg*)’’ = *f *^((2)) *g* + 2*f *^((1)) *g*^((1)) + *f* *g*^((2))
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: (*fg*)^((2)) = (*fg*)’’ = *f*^((2)) *g* + 2*f*^((1)) *g*^((1)) + *f* *g*^((2))
- en: Notice the familiar first derivative as defined by the product rule. The Leibniz
    rule can also be used to find higher-order derivatives of rational functions,
    since the quotient can be effectively expressed into a product of the form, *f*
    *g*^(-1).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到乘积规则定义的一阶导数。莱布尼兹规则也可以用来寻找有理函数的高阶导数，因为商可以有效地表达为形式为 *f* *g*^(-1) 的乘积。
- en: '**Higher-Order Derivatives of Multivariate Functions**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**多变量函数的高阶导数**'
- en: 'The definition of higher-order [partial derivatives](https://machinelearningmastery.com/a-gentle-introduction-to-partial-derivatives-and-gradient-vectors)
    of [multivariate functions](https://machinelearningmastery.com/?p=12606&preview=true)
    is analogous to the univariate case: the *n*^(th) order partial derivative for
    *n* > 1, is computed as the partial derivative of the (*n* – 1)^(th) order partial
    derivative. For example, taking the second partial derivative of a function with
    two variables results in four, second partial derivatives: two *own* partial derivatives,
    *f**[xx]* and *f**[yy]*, and two cross partial derivatives, *f**[xy]* and *f**[yx]*.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 高阶[偏导数](https://machinelearningmastery.com/a-gentle-introduction-to-partial-derivatives-and-gradient-vectors)
    的定义对于[多变量函数](https://machinelearningmastery.com/?p=12606&preview=true) 类似于一变量情况：*n*^(th)
    阶偏导数对于 *n* > 1，是计算 (*n* – 1)^(th) 阶偏导数的偏导数。例如，对具有两个变量的函数进行二阶偏导数运算，会得到四个二阶偏导数：两个
    *自身* 偏导数 *f**[xx]* 和 *f**[yy]*，以及两个交叉偏导数 *f**[xy]* 和 *f**[yx]*。
- en: '*To take a “derivative,” we must take a partial derivative with respect to
    x or y, and there are four ways to do it: x then x, x then y, y then x, y then
    y.*'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*为了进行“导数”，我们必须对 x 或 y 进行偏导数，并且有四种方式：先对 x，然后对 x，先对 x，然后对 y，先对 y，然后对 x，先对 y，然后对
    y。*'
- en: ''
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – Page 371, [Single and Multivariable Calculus](https://www.whitman.edu/mathematics/multivariable/multivariable.pdf),
    2020.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – 第371页，[单变量和多变量微积分](https://www.whitman.edu/mathematics/multivariable/multivariable.pdf)，2020年。
- en: Let’s consider the multivariate function, *f*(*x*, *y*) = *x*² + 3*xy* + 4*y*²,
    for which we would like to find the second partial derivatives. The process starts
    with finding its first-order partial derivatives, first:[![](../Images/fa6f803d0acf16dd170b4da48d0cece0.png)](https://machinelearningmastery.com/wp-content/uploads/2021/07/higher_order_2.png)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑多元函数，*f*(*x*, *y*) = *x*² + 3*xy* + 4*y*²，我们希望找到其二阶偏导数。该过程始于找到其一阶偏导数，首先：[![](../Images/fa6f803d0acf16dd170b4da48d0cece0.png)](https://machinelearningmastery.com/wp-content/uploads/2021/07/higher_order_2.png)
- en: 'The four, second-order partial derivatives are then found by repeating the
    process of finding the partial derivatives, of the partial derivatives. The *own*
    partial derivatives are the most straightforward to find, since we simply repeat
    the partial differentiation process, with respect to either *x* or *y*, a second
    time:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通过重复找到偏导数的过程，找到四个二阶偏导数。*自己的*偏导数是最简单找到的，因为我们只需再次针对*x*或*y*进行偏导数过程的重复：
- en: '[![](../Images/7afb832169b7ad7ac5bc57accfaacd8e.png)](https://machinelearningmastery.com/wp-content/uploads/2021/07/higher_order_3.png)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/7afb832169b7ad7ac5bc57accfaacd8e.png)](https://machinelearningmastery.com/wp-content/uploads/2021/07/higher_order_3.png)'
- en: 'The cross partial derivative of the previously found *f**[x]* (that is, the
    partial derivative with respect to *x*) is found by taking the partial derivative
    of the result with respect to *y*, giving us *f**[xy]*. Similarly, taking the
    partial derivative of *f**[y]* with respect to *x*, gives us *f**[yx]*:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 先前找到的*f*[x]（即相对于*x*的偏导数）的交叉偏导数通过取其结果相对于*y*的偏导数得到，给出*f*[xy]。类似地，相对于*x*的偏导数取*f*[y]的结果，给出*f*[yx]：
- en: '[![](../Images/b0693ee100120ea030fadfc3ff86d163.png)](https://machinelearningmastery.com/wp-content/uploads/2021/07/higher_order_4.png)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/b0693ee100120ea030fadfc3ff86d163.png)](https://machinelearningmastery.com/wp-content/uploads/2021/07/higher_order_4.png)'
- en: It is not by accident that the cross partial derivatives give the same result.
    This is defined by Clairaut’s theorem, which states that as long as the cross
    partial derivatives are continuous, then they are equal.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉偏导数给出相同结果并非偶然。这由克莱罗定理定义，其表明只要交叉偏导数连续，则它们相等。
- en: Want to Get Started With Calculus for Machine Learning?
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 想要开始学习机器学习的微积分吗？
- en: Take my free 7-day email crash course now (with sample code).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在就参加我的免费7天电子邮件快速课程（附带示例代码）。
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 点击注册并获得该课程的免费PDF电子书版本。
- en: '**Application in Machine Learning**'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**在机器学习中的应用**'
- en: In machine learning, it is the second-order derivative that is mostly used.
    We had [previously mentioned](https://machinelearningmastery.com/applications-of-derivatives/)
    that the second derivative can provide us with information that the first derivative
    on its own cannot capture. Specifically, it can tell us whether a critical point
    is a local minimum or maximum (based on whether the second derivative is greater
    or smaller than zero, respectively), for which the first derivative would, otherwise,
    be zero in both cases.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，主要使用二阶导数。我们此前提到过，二阶导数可以提供第一阶导数无法捕捉的信息。具体来说，它可以告诉我们临界点是局部最小值还是最大值（基于二阶导数大于或小于零的情况），而在这两种情况下第一阶导数都将为零。
- en: There are several *second-order* optimization algorithms that leverage this
    information, one of which is Newton’s method.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种利用此信息的*二阶*优化算法，其中之一是牛顿法。
- en: '*Second-order information, on the other hand, allows us to make a quadratic
    approximation of the objective function and approximate the right step size to
    reach a local minimum …*'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*另一方面，二阶信息使我们能够对目标函数进行二次近似，并近似计算出达到局部最小值的正确步长……*'
- en: ''
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – Page 87, [Algorithms for Optimization](https://www.amazon.com/Algorithms-Optimization-Press-Mykel-Kochenderfer/dp/0262039427/ref=sr_1_1?dchild=1&keywords=algorithms+for+optimization&qid=1624019308&sr=8-1),
    2019.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – 第87页，[优化算法](https://www.amazon.com/Algorithms-Optimization-Press-Mykel-Kochenderfer/dp/0262039427/ref=sr_1_1?dchild=1&keywords=algorithms+for+optimization&qid=1624019308&sr=8-1)，2019年。
- en: In the univariate case, Newton’s method uses a second-order Taylor series expansion
    to perform the quadratic approximation around some point on the objective function.
    The update rule for Newton’s method, which is obtained by setting the derivative
    to zero and solving for the root, involves a division operation by the second
    derivative. If Newton’s method is extended to multivariate optimization, the derivative
    is replaced by the gradient, while the reciprocal of the second derivative is
    replaced with the inverse of the Hessian matrix.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在单变量情况下，牛顿法使用二阶泰勒级数展开式在目标函数的某一点进行二次近似。牛顿法的更新规则是通过将导数设为零并解出根得到的，这涉及到对二阶导数进行除法运算。如果牛顿法扩展到多变量优化，导数将被梯度取代，而二阶导数的倒数将被
    Hessian 矩阵的逆矩阵取代。
- en: We shall be covering the Hessian and Taylor Series approximations, which leverage
    the use of higher-order derivatives, in separate tutorials.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在不同的教程中涵盖 Hessian 矩阵和泰勒级数近似方法，这些方法利用了高阶导数。
- en: '**Further Reading**'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: This section provides more resources on the topic if you are looking to go deeper.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解该主题，本节提供了更多资源。
- en: '**Books**'
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**书籍**'
- en: '[Single and Multivariable Calculus](https://www.whitman.edu/mathematics/multivariable/multivariable.pdf),
    2020.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[单变量与多变量微积分](https://www.whitman.edu/mathematics/multivariable/multivariable.pdf)，2020年。'
- en: '[Calculus for Dummies](https://www.amazon.com/Calculus-Dummies-Math-Science/dp/1119293499/ref=as_li_ss_tl?dchild=1&keywords=calculus&qid=1606170839&sr=8-2&linkCode=sl1&tag=inspiredalgor-20&linkId=539ed0b89e326b6eb27b1a9a028e9cee&language=en_US),
    2016.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[傻瓜微积分](https://www.amazon.com/Calculus-Dummies-Math-Science/dp/1119293499/ref=as_li_ss_tl?dchild=1&keywords=calculus&qid=1606170839&sr=8-2&linkCode=sl1&tag=inspiredalgor-20&linkId=539ed0b89e326b6eb27b1a9a028e9cee&language=en_US)，2016年。'
- en: '[Deep Learning](https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=sr_1_1?dchild=1&keywords=deep+learning&qid=1622968138&sr=8-1),
    2017.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习](https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=sr_1_1?dchild=1&keywords=deep+learning&qid=1622968138&sr=8-1)，2017年。'
- en: '[Algorithms for Optimization](https://www.amazon.com/Algorithms-Optimization-Press-Mykel-Kochenderfer/dp/0262039427/ref=sr_1_1?dchild=1&keywords=algorithms+for+optimization&qid=1624019308&sr=8-1),
    2019.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化算法](https://www.amazon.com/Algorithms-Optimization-Press-Mykel-Kochenderfer/dp/0262039427/ref=sr_1_1?dchild=1&keywords=algorithms+for+optimization&qid=1624019308&sr=8-1)，2019年。'
- en: '**Summary**'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this tutorial, you discovered how to compute higher-order univariate and
    multivariate derivatives.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你学会了如何计算单变量和多变量函数的高阶导数。
- en: 'Specifically, you learned:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，你学到了：
- en: How to compute the higher-order derivatives of univariate functions.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何计算单变量函数的高阶导数。
- en: How to compute the higher-order derivatives of multivariate functions.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何计算多变量函数的高阶导数。
- en: How the second-order derivatives can be exploited in machine learning by second-order
    optimization algorithms.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过二阶优化算法在机器学习中利用二阶导数。
- en: Do you have any questions?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你有什么问题吗？
- en: Ask your questions in the comments below and I will do my best to answer.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的评论中提出你的问题，我会尽力回答。
