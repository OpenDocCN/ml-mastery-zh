- en: A Tour of Attention-Based Architectures
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[注意力机制架构之旅](https://machinelearningmastery.com/a-tour-of-attention-based-architectures/)'
- en: 原文：[https://machinelearningmastery.com/a-tour-of-attention-based-architectures/](https://machinelearningmastery.com/a-tour-of-attention-based-architectures/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/a-tour-of-attention-based-architectures/](https://machinelearningmastery.com/a-tour-of-attention-based-architectures/)
- en: As the popularity of attention in machine learning grows, so does the list of
    neural architectures that incorporate an attention mechanism.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着注意力在机器学习中的流行，整合注意力机制的神经架构列表也在增长。
- en: In this tutorial, you will discover the salient neural architectures that have
    been used in conjunction with attention.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您将了解与注意力结合使用的显著神经架构。
- en: After completing this tutorial, you will better understand how the attention
    mechanism is incorporated into different neural architectures and for which purpose.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本教程后，您将更好地理解注意力机制如何被整合到不同的神经架构中，以及其目的。
- en: '**Kick-start your project** with my book [Building Transformer Models with
    Attention](https://machinelearningmastery.com/transformer-models-with-attention/).
    It provides **self-study tutorials** with **working code** to guide you into building
    a fully-working transformer model that can'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**用我的书 [使用注意力构建变形金刚模型](https://machinelearningmastery.com/transformer-models-with-attention/)
    开始您的项目**。它提供了**自学教程**和**可工作的代码**，帮助您构建一个完全可工作的变形金刚模型，可以'
- en: '*translate sentences from one language to another*...'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*将一种语言的句子翻译成另一种语言*...'
- en: Let’s get started.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: '[![](../Images/b8c3ef52c390671d3a345070c9ef1881.png)](https://machinelearningmastery.com/wp-content/uploads/2021/09/tour_cover2-scaled.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/b8c3ef52c390671d3a345070c9ef1881.png)](https://machinelearningmastery.com/wp-content/uploads/2021/09/tour_cover2-scaled.jpg)'
- en: A tour of attention-based architectures
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制架构之旅
- en: Photo by [Lucas Clara](https://unsplash.com/photos/hvPB-UCAmmU), some rights
    reserved.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Lucas Clara](https://unsplash.com/photos/hvPB-UCAmmU)拍摄，部分权利保留。
- en: '**Tutorial Overview**'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**教程概述**'
- en: 'This tutorial is divided into four parts; they are:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程分为四个部分；它们是：
- en: The Encoder-Decoder Architecture
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器-解码器架构
- en: The Transformer
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变形金刚
- en: Graph Neural Networks
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图神经网络
- en: Memory-Augmented Neural Networks
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强记忆神经网络
- en: '**The Encoder-Decoder Architecture**'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**编码器-解码器架构**'
- en: The encoder-decoder architecture has been extensively applied to sequence-to-sequence
    (seq2seq) tasks for language processing. Examples of such tasks within the domain
    of language processing include machine translation and image captioning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器架构已被广泛应用于序列到序列（seq2seq）任务，例如语言处理中的机器翻译和图像字幕。
- en: '*The earliest use of attention was as part of RNN based encoder-decoder framework
    to encode long input sentences [Bahdanau et al. 2015]. Consequently, attention
    has been most widely used with this architecture.*'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*注意力最早作为RNN基础编码器-解码器框架的一部分用于编码长输入句子[Bahdanau et al. 2015]。因此，注意力在这种架构中被广泛使用。*'
- en: ''
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – [An Attentive Survey of Attention Models](https://arxiv.org/abs/1904.02874?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529),
    2021.
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – [注意模型的关注性调查](https://arxiv.org/abs/1904.02874?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529)，2021年。
- en: Within the context of machine translation, such a seq2seq task would involve
    the translation of an input sequence, $I = \{ A, B, C, <EOS> \}$, into an output
    sequence, $O = \{ W, X, Y, Z, <EOS> \}$, of a different length.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器翻译的背景下，这样的seq2seq任务将涉及将输入序列$I = \{ A, B, C, <EOS> \}$翻译成长度不同的输出序列$O = \{
    W, X, Y, Z, <EOS> \}$。
- en: 'For an RNN-based encoder-decoder architecture *without* attention, [unrolling
    each RNN](https://machinelearningmastery.com/rnn-unrolling/) would produce the
    following graph:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于没有注意力的基于RNN的编码器-解码器架构，[展开每个RNN](https://machinelearningmastery.com/rnn-unrolling/)将产生以下图表：
- en: '[![](../Images/7954d524251b1e15b9893f4737c46305.png)](https://machinelearningmastery.com/wp-content/uploads/2021/09/tour_1.png)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/7954d524251b1e15b9893f4737c46305.png)](https://machinelearningmastery.com/wp-content/uploads/2021/09/tour_1.png)'
- en: Unrolled RNN-based encoder and decoder
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 未展开的基于RNN的编码器和解码器
- en: Taken from “[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)“
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 摘自“[神经网络的序列到序列学习](https://arxiv.org/abs/1409.3215)“
- en: Here, the encoder reads the input sequence one word at a time, each time updating
    its internal state. It stops when it encounters the <EOS> symbol, which signals
    that the *end of sequence* has been reached. The hidden state generated by the
    encoder essentially contains a vector representation of the input sequence, which
    the decoder will then process.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，编码器一次读取一个词的输入序列，每次更新其内部状态。遇到<EOS>符号时，表明*序列结束*。编码器生成的隐藏状态本质上包含了输入序列的向量表示，解码器将处理这个表示。
- en: The decoder generates the output sequence one word at a time, taking the word
    at the previous time step ($t$ – 1) as input to generate the next word in the
    output sequence. An <EOS> symbol at the decoding side signals that the decoding
    process has ended.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器一次生成一个词的输出序列，以前一个时间步（$t$ – 1）的词作为输入生成输出序列中的下一个词。解码端的<EOS>符号表示解码过程已结束。
- en: '[As we have previously mentioned](https://machinelearningmastery.com/what-is-attention/),
    the problem with the encoder-decoder architecture without attention arises when
    sequences of different lengths and complexities are represented by a fixed-length
    vector, potentially resulting in the decoder missing important information.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[正如我们之前提到的](https://machinelearningmastery.com/what-is-attention/)，没有注意力机制的编码器-解码器架构的问题在于，当不同长度和复杂性的序列由固定长度的向量表示时，可能会导致解码器遗漏重要信息。'
- en: In order to circumvent this problem, an attention-based architecture introduces
    an attention mechanism between the encoder and decoder.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，基于注意力的架构在编码器和解码器之间引入了注意力机制。
- en: '[![](../Images/40308491999298c072adb44af4ffd737.png)](https://machinelearningmastery.com/wp-content/uploads/2021/09/tour_2.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/40308491999298c072adb44af4ffd737.png)](https://machinelearningmastery.com/wp-content/uploads/2021/09/tour_2.jpg)'
- en: Encoder-decoder architecture with attention
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 带有注意力机制的编码器-解码器架构
- en: Taken from “[Attention in Psychology, Neuroscience, and Machine Learning](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full)“
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 摘自“[心理学、神经科学与机器学习中的注意力](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full)”
- en: Here, the attention mechanism ($\phi$) learns a set of attention weights that
    capture the relationship between the encoded vectors (v) and the hidden state
    of the decoder (h) to generate a context vector (c) through a weighted sum of
    all the hidden states of the encoder. In doing so, the decoder would have access
    to the entire input sequence, with a specific focus on the input information most
    relevant for generating the output.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，注意力机制（$\phi$）学习一组注意力权重，这些权重捕捉编码向量（v）与解码器的隐藏状态（h）之间的关系，通过对编码器所有隐藏状态的加权求和生成上下文向量（c）。这样，解码器能够访问整个输入序列，特别关注生成输出时最相关的输入信息。
- en: '**The Transformer**'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Transformer**'
- en: The architecture of the transformer also implements an encoder and decoder.
    However, as opposed to the architectures reviewed above, it does not rely on the
    use of recurrent neural networks. For this reason, this post will review this
    architecture and its variants separately.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 的架构还实现了编码器和解码器。然而，与上述回顾的架构不同，它不依赖于递归神经网络。因此，本文将单独回顾这一架构及其变体。
- en: The transformer architecture dispenses of any recurrence and instead relies
    solely on a *self-attention* (or intra-attention) mechanism.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构摒弃了任何递归，而是完全依赖于*自注意力*（或内部注意力）机制。
- en: '*In terms of computational complexity, self-attention layers are faster than
    recurrent layers when the sequence length n is smaller than the representation
    dimensionality d …*'
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*在计算复杂性方面，当序列长度 n 小于表示维度 d 时，自注意力层比递归层更快……*'
- en: ''
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – [Advanced Deep Learning with Python](https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X),
    2019.
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – [高级深度学习与 Python](https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X)，2019。
- en: The self-attention mechanism relies on the use of *queries*, *keys,* and *values*,
    which are generated by multiplying the encoder’s representation of the same input
    sequence with different weight matrices. The transformer uses dot product (or
    *multiplicative*) attention, where each query is matched against a database of
    keys by a dot product operation in the process of generating the attention weights.
    These weights are then multiplied by the values to generate a final attention
    vector.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制依赖于使用*查询*、*键*和*值*，这些是通过将编码器对相同输入序列的表示与不同的权重矩阵相乘生成的。Transformer 使用点积（或*乘法*）注意力，在生成注意力权重的过程中，每个查询通过点积操作与键的数据库进行匹配。这些权重然后与值相乘，以生成最终的注意力向量。
- en: '[![](../Images/bdffca1b5f416aed7741d5b03a4acf82.png)](https://machinelearningmastery.com/wp-content/uploads/2021/09/tour_3.png)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/bdffca1b5f416aed7741d5b03a4acf82.png)](https://machinelearningmastery.com/wp-content/uploads/2021/09/tour_3.png)'
- en: Multiplicative attention
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 乘法注意力
- en: Taken from “[Attention Is All You Need](https://arxiv.org/abs/1706.03762)“
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 取自 “[Attention Is All You Need](https://arxiv.org/abs/1706.03762)“
- en: Intuitively, since all queries, keys, and values originate from the same input
    sequence, the self-attention mechanism captures the relationship between the different
    elements of the same sequence, highlighting those that are most relevant to one
    another.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，由于所有查询、键和值都源于相同的输入序列，自注意力机制捕捉了同一序列中不同元素之间的关系，突出显示了彼此之间最相关的元素。
- en: Since the transformer does not rely on RNNs, the positional information of each
    element in the sequence can be preserved by augmenting the encoder’s representation
    of each element with positional encoding. This means that the transformer architecture
    may also be applied to tasks where the information may not necessarily be related
    sequentially, such as for the computer vision tasks of image classification, segmentation,
    or captioning.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 transformer 不依赖于 RNN，序列中每个元素的位置信息可以通过增强编码器对每个元素的表示来保留。这意味着 transformer 架构还可以应用于信息可能不一定按顺序相关的任务，例如图像分类、分割或标注的计算机视觉任务。
- en: '*Transformers can capture global/long range dependencies between input and
    output, support parallel processing, require minimal inductive biases (prior knowledge),
    demonstrate scalability to large sequences and datasets, and allow domain-agnostic
    processing of multiple modalities (text, images, speech) using similar processing
    blocks.*'
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Transformers 可以捕捉输入和输出之间的全局/长期依赖，支持并行处理，要求最少的归纳偏置（先验知识），展示了对大序列和数据集的可扩展性，并允许使用类似的处理块进行多模态（文本、图像、语音）的领域无关处理。*'
- en: ''
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – [An Attentive Survey of Attention Models](https://arxiv.org/abs/1904.02874?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529),
    2021.
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – [An Attentive Survey of Attention Models](https://arxiv.org/abs/1904.02874?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529)，2021年。
- en: Furthermore, several attention layers can be stacked in parallel in what has
    been termed *multi-head attention*. Each head works in parallel over different
    linear transformations of the same input, and the outputs of the heads are then
    concatenated to produce the final attention result. The benefit of having a multi-head
    model is that each head can attend to different elements of the sequence.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，多个注意力层可以并行堆叠，这被称为*多头注意力*。每个头部在相同输入的不同线性变换上并行工作，然后将头部的输出连接起来生成最终的注意力结果。使用多头模型的好处是每个头部可以关注序列的不同元素。
- en: '[![](../Images/49d312ed799331ac86c88962132369f2.png)](https://machinelearningmastery.com/wp-content/uploads/2021/09/tour_4.png)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/49d312ed799331ac86c88962132369f2.png)](https://machinelearningmastery.com/wp-content/uploads/2021/09/tour_4.png)'
- en: Multi-head attention
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力
- en: Taken from “[Attention Is All You Need](https://arxiv.org/abs/1706.03762)“
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 取自 “[Attention Is All You Need](https://arxiv.org/abs/1706.03762)“
- en: '**Some variants of the transformer architecture that address the limitations
    of the vanilla model are:**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**一些解决原始模型局限性的 transformer 架构变体包括：**'
- en: '***   Transformer-XL: Introduces recurrence so that it can learn longer-term
    dependency beyond the fixed length of the fragmented sequences that are typically
    used during training.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '***   Transformer-XL：引入了递归机制，使其能够学习超越训练过程中通常使用的碎片化序列的固定长度的长期依赖。'
- en: 'XLNet: A bidirectional transformer that builds on Transfomer-XL by introducing
    a permutation-based mechanism, where training is carried out not only on the original
    order of the elements comprising the input sequence but also over different permutations
    of the input sequence order.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XLNet：一个双向变换器，它通过引入基于排列的机制在 Transformer-XL 的基础上进行构建，其中训练不仅在输入序列的原始顺序上进行，还包括输入序列顺序的不同排列。
- en: Want to Get Started With Building Transformer Models with Attention?
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 想开始构建带有注意力的变换器模型吗？
- en: Take my free 12-day email crash course now (with sample code).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 立即获取我的免费12天电子邮件速成课程（附示例代码）。
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 点击注册，并免费获得课程的 PDF Ebook 版本。
- en: '**Graph Neural Networks**'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**图神经网络**'
- en: A graph can be defined as a set of *nodes* (or vertices) that are linked through
    *connections* (or edges).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图可以定义为一组*节点*（或顶点），它们通过*连接*（或边）链接在一起。
- en: '*A graph is a versatile data structure that lends itself well to the way data
    is organized in many real-world scenarios.*'
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*图是一种多功能的数据结构，非常适合许多现实世界场景中数据的组织方式。*'
- en: ''
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – [Advanced Deep Learning with Python](https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X),
    2019.
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – [深入学习 Python](https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X)，2019年。
- en: For example, take a social network where users can be represented by nodes in
    a graph and their relationships with friends by edges. Or a molecule, where the
    nodes would be the atoms, and the edges would represent the chemical bonds between
    them.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个社交网络，其中用户可以通过图中的节点表示，朋友之间的关系通过边表示。或者是一个分子，其中节点是原子，边表示它们之间的化学键。
- en: '*We can think of an image as a graph, where each pixel is a node, directly
    connected to its neighboring pixels …*'
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我们可以将图像视为图形，其中每个像素是一个节点，直接连接到其邻近的像素…*'
- en: ''
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – [Advanced Deep Learning with Python](https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X),
    2019.
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – [深入学习 Python](https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X)，2019年。
- en: Of particular interest are the *Graph Attention Networks* (GAT) that employ
    a self-attention mechanism within a graph convolutional network (GCN), where the
    latter updates the state vectors by performing a convolution over the nodes of
    the graph. The convolution operation is applied to the central node and the neighboring
    nodes using a weighted filter to update the representation of the central node.
    The filter weights in a GCN can be fixed or learnable.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 特别感兴趣的是*图注意力网络*（GAT），它在图卷积网络（GCN）中使用自注意力机制，其中后者通过在图的节点上执行卷积来更新状态向量。卷积操作应用于中心节点和邻近节点，使用加权滤波器来更新中心节点的表示。GCN
    中的滤波器权重可以是固定的或可学习的。
- en: '[![](../Images/58f1adc6a9ec4bd7da2e8a89708bec04.png)](https://machinelearningmastery.com/wp-content/uploads/2021/09/tour_5.png)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/58f1adc6a9ec4bd7da2e8a89708bec04.png)](https://machinelearningmastery.com/wp-content/uploads/2021/09/tour_5.png)'
- en: Graph convolution over a central node (red) and a neighborhood of nodes
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在中心节点（红色）及其邻域节点上的图卷积
- en: Taken from “[A Comprehensive Survey on Graph Neural Networks](https://arxiv.org/abs/1901.00596)“
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 摘自“[图神经网络的综合调查](https://arxiv.org/abs/1901.00596)“
- en: In comparison, a GAT assigns weights to the neighboring nodes using attention
    scores.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，GAT 使用注意力分数为邻近节点分配权重。
- en: 'The computation of these attention scores follows a similar procedure as in
    the methods for the seq2seq tasks reviewed above: (1) alignment scores are first
    computed between the feature vectors of two neighboring nodes, from which (2)
    attention scores are computed by applying a softmax operation, and finally (3)
    an output feature vector for each node (equivalent to the context vector in a
    seq2seq task) can be computed by a weighted combination of the feature vectors
    of all its neighbors.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些注意力分数的计算遵循与上述 seq2seq 任务中的方法类似的程序：（1）首先计算两个邻近节点的特征向量之间的对齐分数，然后（2）通过应用 softmax
    操作计算注意力分数，最后（3）通过对所有邻居的特征向量进行加权组合，计算每个节点的输出特征向量（相当于 seq2seq 任务中的上下文向量）。
- en: Multi-head attention can also be applied here in a very similar manner to how
    it was proposed in the transformer architecture previously seen. Each node in
    the graph would be assigned multiple heads, and their outputs would be averaged
    in the final layer.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力也可以以非常类似于先前在变换器架构中提议的方式应用。在图中的每个节点会分配多个头，最终层中会对它们的输出进行平均。
- en: Once the final output has been produced, this can be used as the input for a
    subsequent task-specific layer. Tasks that can be solved by graphs can be the
    classification of individual nodes between different groups (for example, in predicting
    which of several clubs a person will decide to become a member of). Or they can
    be the classification of individual edges to determine whether an edge exists
    between two nodes (for example, to predict whether two persons in a social network
    might be friends) or even the classification of a full graph (for example, to
    predict if a molecule is toxic).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦生成最终输出，这可以作为后续任务特定层的输入。可以通过图解决的任务包括将个体节点分类到不同的群组中（例如，在预测一个人将决定加入哪些俱乐部时）。或者可以对个体边进行分类，以确定两个节点之间是否存在边（例如，在社交网络中预测两个人是否可能是朋友），甚至可以对整个图进行分类（例如，预测分子是否有毒）。
- en: '**Kick-start your project** with my book [Building Transformer Models with
    Attention](https://machinelearningmastery.com/transformer-models-with-attention/).
    It provides **self-study tutorials** with **working code** to guide you into building
    a fully-working transformer model that can'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**用我的书 [使用注意力构建Transformer模型](https://machinelearningmastery.com/transformer-models-with-attention/)
    开始你的项目**。它提供了带有**工作代码**的**自学教程**，指导你构建一个完全工作的Transformer模型。'
- en: '*translate sentences from one language to another*...'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*将一种语言的句子翻译成另一种语言*...'
- en: '**Memory-Augmented Neural Networks**'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**记忆增强神经网络**'
- en: In the encoder-decoder attention-based architectures reviewed so far, the set
    of vectors that encode the input sequence can be considered external memory, to
    which the encoder writes and from which the decoder reads. However, a limitation
    arises because the encoder can only write to this memory, and the decoder can
    only read.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止审查的基于编码器-解码器的注意力架构中，编码输入序列的向量集可以被视为外部记忆，编码器写入并解码器读取。然而，由于编码器只能写入此内存，解码器只能读取，因此存在限制。
- en: Memory-Augmented Neural Networks (MANNs) are recent algorithms that aim to address
    this limitation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆增强神经网络（MANNs）是最近旨在解决这一限制的算法。
- en: The Neural Turing Machine (NTM) is one type of MANN. It consists of a neural
    network controller that takes an input to produce an output and performs read
    and write operations to memory.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 神经图灵机（NTM）是MANN的一种类型。它由一个神经网络控制器组成，接受输入并产生输出，并对内存执行读写操作。
- en: '[![](../Images/93a78915c48b0e6f0525f857f624d11f.png)](https://machinelearningmastery.com/wp-content/uploads/2021/09/tour_6.png)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/93a78915c48b0e6f0525f857f624d11f.png)](https://machinelearningmastery.com/wp-content/uploads/2021/09/tour_6.png)'
- en: Neural Turing machine architecture
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 神经图灵机架构
- en: Taken from “[Neural Turing Machines](https://arxiv.org/abs/1410.5401)“
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 取自“[神经图灵机](https://arxiv.org/abs/1410.5401)”
- en: The operation performed by the read head is similar to the attention mechanism
    employed for seq2seq tasks, where an attention weight indicates the importance
    of the vector under consideration in forming the output.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 读头执行的操作类似于用于seq2seq任务的注意力机制，其中注意力权重指示考虑的向量在形成输出时的重要性。
- en: '*A read head always reads the full memory matrix, but it does so by attending
    to different memory vectors with different intensities.*'
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*读头总是读取完整的记忆矩阵，但通过对不同记忆向量进行不同强度的关注来执行此操作。*'
- en: ''
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – [Advanced Deep Learning with Python](https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X),
    2019.
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – [Python 深度学习进阶](https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X)，2019年。
- en: The output of a read operation is then defined by a weighted sum of the memory
    vectors.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 读操作的输出由记忆向量的加权和定义。
- en: The write head also makes use of an attention vector, together with an erase
    and add vectors. A memory location is erased based on the values in the attention
    and erase vectors, and information is written via the add vector.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 写头还利用注意力向量，与擦除和添加向量一起工作。根据注意力和擦除向量的值擦除内存位置，并通过添加向量写入信息。
- en: Examples of applications for MANNs include question-answering and chat bots,
    where an external memory stores a large database of sequences (or facts) that
    the neural network taps into. The role of the attention mechanism is crucial in
    selecting facts from the database that are more relevant than others for the task
    at hand.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: MANNs 的应用示例包括问答和聊天机器人，其中外部记忆存储大量序列（或事实），神经网络利用这些信息。关注机制在选择对当前任务更相关的数据库事实时起着关键作用。
- en: '**Further Reading**'
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: This section provides more resources on the topic if you are looking to go deeper.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了更多关于该主题的资源，如果你想深入了解的话。
- en: '**Books**'
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**书籍**'
- en: '[Advanced Deep Learning with Python](https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X),
    2019.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python高级深度学习](https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X)，2019年。'
- en: '[Deep Learning Essentials](https://www.amazon.com/Deep-Learning-Essentials-hands-fundamentals/dp/1785880365),
    2018.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习精粹](https://www.amazon.com/Deep-Learning-Essentials-hands-fundamentals/dp/1785880365)，2018年。'
- en: '**Papers**'
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**论文**'
- en: '[An Attentive Survey of Attention Models](https://arxiv.org/abs/1904.02874?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529),
    2021.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[注意力模型的综述](https://arxiv.org/abs/1904.02874?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529)，2021年。'
- en: '[Attention in Psychology, Neuroscience, and Machine Learning](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full),
    2020.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[心理学、神经科学和机器学习中的注意力](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full)，2020年。'
- en: '**Summary**'
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**摘要**'
- en: In this tutorial, you discovered the salient neural architectures that have
    been used in conjunction with attention.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你发现了与注意力机制结合使用的显著神经网络架构。
- en: Specifically, you gained a better understanding of how the attention mechanism
    is incorporated into different neural architectures and for which purpose.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，你更好地理解了注意力机制如何融入不同的神经网络架构以及其目的。
- en: Do you have any questions?
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你有任何问题吗？
- en: Ask your questions in the comments below, and I will do my best to answer.**
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的评论中提问，我会尽力回答。**
