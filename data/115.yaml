- en: Activation Functions in PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 中的激活函数
- en: 原文：[https://machinelearningmastery.com/activation-functions-in-pytorch/](https://machinelearningmastery.com/activation-functions-in-pytorch/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/activation-functions-in-pytorch/](https://machinelearningmastery.com/activation-functions-in-pytorch/)
- en: As neural networks become increasingly popular in the field of machine learning,
    it is important to understand the role that activation functions play in their
    implementation. In this article, you’ll explore the concept of activation functions
    that are applied to the output of each neuron in a neural network to introduce
    non-linearity into the model. Without activation functions, neural networks would
    simply be a series of linear transformations, which would limit their ability
    to learn complex patterns and relationships in data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着神经网络在机器学习领域的日益普及，了解激活函数在其实现中的作用变得越来越重要。在本文中，您将探讨应用于神经网络每个神经元输出的激活函数的概念，以引入模型的非线性。没有激活函数，神经网络将仅仅是一系列线性变换，这将限制它们学习复杂模式和数据间关系的能力。
- en: PyTorch offers a variety of activation functions, each with its own unique properties
    and use cases. Some common activation functions in PyTorch include ReLU, sigmoid,
    and tanh. Choosing the right activation function for a particular problem can
    be an important consideration for achieving optimal performance in a neural network.
    You will see how to train a neural network in PyTorch with different activation
    functions and analyze their performance.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 提供了多种激活函数，每种都具有独特的特性和用途。在 PyTorch 中一些常见的激活函数包括 ReLU、sigmoid 和 tanh。选择适合特定问题的正确激活函数对于在神经网络中实现最佳性能至关重要。您将学习如何使用不同的激活函数在
    PyTorch 中训练神经网络，并分析它们的性能。
- en: 'In this tutorial, you’ll learn:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您将学习：
- en: About various activation functions that are used in neural network architectures.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于在神经网络架构中使用的各种激活函数。
- en: How activation functions can be implemented in PyTorch.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在 PyTorch 中实现激活函数。
- en: How activation functions actually compare with each other in a real problem.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在实际问题中比较激活函数的效果。
- en: Let’s get started.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: '![](../Images/aa9028c6e30f12c6fd6d1a76c859a9f1.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa9028c6e30f12c6fd6d1a76c859a9f1.png)'
- en: Activation Functions in PyTorch
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 中的激活函数
- en: Image generated by Adrian Tam using stable diffusion. Some rights reserved.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Adrian Tam 使用稳定扩散生成的图像。部分权利保留。
- en: Overview
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'This tutorial is divided into four parts; they are:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程分为四个部分；它们分别是：
- en: Logistic activation function
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logistic 激活函数
- en: Tanh activation function
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双曲正切激活函数
- en: ReLU activation function
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU 激活函数
- en: Exploring activation functions in a neural network
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索神经网络中的激活函数
- en: Logistic Activation Function
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Logistic 激活函数
- en: You’ll start with the logistic function which is a commonly used activation
    function in neural networks and also known as the sigmoid function. It takes any
    input and maps it to a value between 0 and 1, which can be interpreted as a probability.
    This makes it particularly useful for binary classification tasks, where the network
    needs to predict the probability of an input belonging to one of two classes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 您将从逻辑函数开始，这是神经网络中常用的激活函数，也称为 sigmoid 函数。它接受任何输入并将其映射到 0 到 1 之间的值，可以被解释为概率。这使得它特别适用于二元分类任务，其中网络需要预测输入属于两个类别之一的概率。
- en: One of the main advantages of the logistic function is that it is differentiable,
    which means that it can be used in backpropagation algorithms to train the neural
    network. Additionally, it has a smooth gradient, which can help avoid issues such
    as exploding gradients. However, it can also introduce vanishing gradients during
    training.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Logistic 函数的主要优势之一是它是可微分的，这意味着它可以用于反向传播算法来训练神经网络。此外，它具有平滑的梯度，有助于避免梯度爆炸等问题。然而，在训练过程中它也可能引入梯度消失的问题。
- en: Now, let’s apply logistic function on a tensor using PyTorch and draw it to
    see how it looks like.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 PyTorch 对张量应用 logistic 函数，并绘制出它的图像看看。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/f9a455e3db55a513296b0d8ff4d46ff1.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9a455e3db55a513296b0d8ff4d46ff1.png)'
- en: In the example above, you have used the `torch.sigmoid()` function from the
    Pytorch library to apply the logistic activation function to a tensor `x`. You
    have used the matplotlib library to create the plot with a custom color.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，您使用了 PyTorch 库中的 `torch.sigmoid()` 函数将 logistic 激活函数应用到了张量 `x` 上。您还使用了
    matplotlib 库创建了一个具有自定义颜色的图表。
- en: Tanh Activation Function
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双曲正切激活函数
- en: Next, you will investigate the tanh activation function which outputs values
    between $-1$ and $1$, with a mean output of 0\. This can help ensure that the
    output of a neural network layer remains centered around 0, making it useful for
    normalization purposes. Tanh is a smooth and continuous activation function, which
    makes it easier to optimize during the process of gradient descent.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将研究tanh激活函数，该函数输出值介于$-1$和$1$之间，平均输出为0。这有助于确保神经网络层的输出保持在0附近，从而对归一化目的有用。Tanh是一个平滑且连续的激活函数，这使得在梯度下降过程中更容易优化。
- en: Like the logistic activation function, the tanh function can be susceptible
    to the vanishing gradient problem, especially for deep neural networks with many
    layers. This is because the slope of the function becomes very small for large
    or small input values, making it difficult for gradients to propagate through
    the network.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与逻辑激活函数类似，tanh函数在深度神经网络中尤其容易受到梯度消失问题的影响。这是因为函数的斜率在大或小的输入值下变得非常小，使得梯度在网络中传播变得困难。
- en: Also, due to the use of exponential functions, tanh can be computationally expensive,
    especially for large tensors or when used in deep neural networks with many layers.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于使用了指数函数，tanh在计算上可能比较昂贵，尤其是在大张量或用于多层深度神经网络时。
- en: Here is how to apply tanh on a tensor and visualize it.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何在张量上应用tanh并可视化的示例。
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/4c891af66131389e617d327300dc2ef6.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c891af66131389e617d327300dc2ef6.png)'
- en: ReLU Activation Function
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReLU 激活函数
- en: ReLU (Rectified Linear Unit) is another commonly used activation function in
    neural networks. Unlike the sigmoid and tanh functions, ReLU is a non-saturating
    function, which means that it does not become flat at the extremes of the input
    range. Instead, ReLU simply outputs the input value if it is positive, or 0 if
    it is negative.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU（修正线性单元）是神经网络中另一种常用的激活函数。与sigmoid和tanh函数不同，ReLU是一个非饱和函数，这意味着它在输入范围的极值处不会变得平坦。相反，如果输入值为正，ReLU直接输出该值；如果为负，则输出0。
- en: This simple, piecewise linear function has several advantages over sigmoid and
    tanh activation functions. First, it is computationally more efficient, making
    it well-suited for large-scale neural networks. Second, ReLU has been shown to
    be less susceptible to the vanishing gradient problem, as it does not have a flattened
    slope. Plus, ReLU can help sparsify the activation of neurons in a network, which
    can lead to better generalization.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的分段线性函数相比于sigmoid和tanh激活函数有几个优势。首先，它在计算上更高效，非常适合大规模神经网络。其次，ReLU显示出对梯度消失问题的抗性较强，因为它没有平坦的斜率。此外，ReLU可以帮助稀疏化网络中神经元的激活，从而可能提高泛化能力。
- en: Here’s an example of how to apply the ReLU activation function to a PyTorch
    tensor `x` and plot the results.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何将ReLU激活函数应用于PyTorch张量`x`并绘制结果的示例。
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/35477c1bd7e0c8a93689771a7feda52f.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35477c1bd7e0c8a93689771a7feda52f.png)'
- en: Below is the complete code to print all the activation functions discussed above.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是打印上述所有激活函数的完整代码。
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Exploring Activation Functions in a Neural Network
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索神经网络中的激活函数
- en: Activation functions play a vital role in the training of deep learning models,
    as they introduce non-linearity into the network, enabling it to learn complex
    patterns.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数在深度学习模型的训练中起着至关重要的作用，因为它们向网络引入了非线性，使其能够学习复杂的模式。
- en: Let’s take the popular MNIST dataset, which consists of 70000 grayscale images
    in 28×28 pixels of handwritten digits. You’ll create a simple feedforward neural
    network to classify these digits, and experiment with different activation functions
    like ReLU, Sigmoid, Tanh, and Leaky ReLU.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用流行的MNIST数据集，它包含70000张28×28像素的灰度手写数字图像。你将创建一个简单的前馈神经网络来分类这些数字，并实验不同的激活函数，如ReLU、Sigmoid、Tanh和Leaky
    ReLU。
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let’s create a `NeuralNetwork` class that inherits from `nn.Module`. This class
    has three linear layers and an activation function as an input parameter. The
    forward method defines the forward pass of the network, applying the activation
    function after each linear layer except the last one.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个继承自`nn.Module`的`NeuralNetwork`类。该类有三个线性层和一个激活函数作为输入参数。前向传播方法定义了网络的前向传递过程，在每个线性层之后应用激活函数，最后一个线性层除外。
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You’ve added an `activation_function` parameter to the `NeuralNetwork` class,
    which allows you to plug in any activation function you’d like to experiment with.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你已将`activation_function`参数添加到`NeuralNetwork`类中，这使得你可以插入任何你想实验的激活函数。
- en: Training and Testing the Model with Different Activation Functions
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用不同激活函数训练和测试模型
- en: Let’s create functions to help the training. The `train()` function trains the
    network for one epoch. It iterates through the training data loader, computes
    the loss, and performs backpropagation and optimization. The `test()` function
    evaluates the network on the test dataset, computing the test loss and accuracy.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一些函数来帮助训练。`train()` 函数在一个 epoch 中训练网络。它遍历训练数据加载器，计算损失，并进行反向传播和优化。`test()`
    函数在测试数据集上评估网络，计算测试损失和准确度。
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To compare them, let’s create a dictionary of activation functions and iterate
    over them. For each activation function, you instantiate the `NeuralNetwork` class,
    define the criterion (`CrossEntropyLoss`), and set up the optimizer (`Adam`).
    Then, train the model for a specified number of epochs, calling the `train()`
    and `test()` functions in each epoch to evaluate the model’s performance. You
    store the training loss, testing loss, and testing accuracy for each epoch in
    the results dictionary.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行比较，让我们创建一个激活函数的字典并进行迭代。对于每种激活函数，你实例化 `NeuralNetwork` 类，定义损失函数（`CrossEntropyLoss`），并设置优化器（`Adam`）。然后，训练模型若干个
    epoch，每个 epoch 中调用 `train()` 和 `test()` 函数来评估模型的性能。你将每个 epoch 的训练损失、测试损失和测试准确度存储在结果字典中。
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'When you run the above, it prints:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行上述代码时，它会输出：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You may use Matplotlib to create plots comparing the performance of each activation
    function. You can create three separate plots to visualize the training loss,
    testing loss, and testing accuracy for each activation function over the epochs.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 Matplotlib 创建图表，以比较各个激活函数的性能。你可以创建三个独立的图表，以可视化每种激活函数在各个 epoch 上的训练损失、测试损失和测试准确度。
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/acc9423f783521ee1e33b8bd5826aa72.png)![](../Images/bcab524bcca50d07878b3fb9e17e0169.png)![](../Images/7eff026adbf6d4c238a43c919c1957cb.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/acc9423f783521ee1e33b8bd5826aa72.png)![](../Images/bcab524bcca50d07878b3fb9e17e0169.png)![](../Images/7eff026adbf6d4c238a43c919c1957cb.png)'
- en: These plots provide a visual comparison of the performance of each activation
    function. By analyzing the results, you can determine which activation function
    works best for the specific task and dataset used in this example.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图表提供了各个激活函数性能的视觉比较。通过分析结果，你可以确定哪种激活函数最适合本示例中的特定任务和数据集。
- en: Summary
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this tutorial, you have implemented some of the most popular activation functions
    in PyTorch. You also saw how to train a neural network in PyTorch with different
    activation functions, using the popular MNIST dataset. You explored ReLU, Sigmoid,
    Tanh, and Leaky ReLU activation functions and analyzed their performance by plotting
    the training loss, testing loss, and testing accuracy.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你已经实现了 PyTorch 中一些最流行的激活函数。你还学习了如何使用流行的 MNIST 数据集在 PyTorch 中训练神经网络，尝试了
    ReLU、Sigmoid、Tanh 和 Leaky ReLU 激活函数，并通过绘制训练损失、测试损失和测试准确度来分析它们的性能。
- en: As you can see, the choice of activation function plays an essential role in
    model performance. However, keep in mind that the optimal activation function
    may vary depending on the task and dataset.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，激活函数的选择在模型性能中起着至关重要的作用。然而，请记住，最佳的激活函数可能会根据任务和数据集的不同而有所变化。
