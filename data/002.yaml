- en: Building Your mini-ChatGPT at Home
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在家构建你的迷你ChatGPT
- en: 原文：[https://machinelearningmastery.com/building-your-mini-chatgpt-at-home/](https://machinelearningmastery.com/building-your-mini-chatgpt-at-home/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/building-your-mini-chatgpt-at-home/](https://machinelearningmastery.com/building-your-mini-chatgpt-at-home/)
- en: ChatGPT is fun to play with. Chances are, you also want to have your own copy
    running privately. Realistically, that’s impossible because ChatGPT is not a software
    for download, and it needs tremendous computer power to run. But you can build
    a trimmed-down version that can run on commodity hardware. In this post, you will
    learn about
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT非常有趣。你可能也希望拥有一个私人运行的副本。实际上，这是不可能的，因为ChatGPT不是可以下载的软件，并且需要巨大的计算能力才能运行。但你可以构建一个可以在普通硬件上运行的简化版本。在这篇文章中，你将了解
- en: What are language models that can behave like ChatGPT
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能像ChatGPT一样表现的语言模型
- en: How to build a chatbot using the advanced language models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用高级语言模型构建聊天机器人
- en: '![](../Images/8ab0fc958bd90cb6da8d1ffa2cb9687c.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ab0fc958bd90cb6da8d1ffa2cb9687c.png)'
- en: Building Your mini-ChatGPT at Home
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在家构建你的迷你ChatGPT
- en: Picture generated by the author using Stable Diffusion. Some rights reserved.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用Stable Diffusion生成。保留一些权利。
- en: Let’s get started.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Overview
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'This post is divided into three parts; they are:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文分为三个部分；它们是：
- en: What are Instruction-Following Models?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是指令跟随模型？
- en: How to Find Instruction Following Models
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何寻找指令跟随模型
- en: Building a Simple Chatbot
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个简单的聊天机器人
- en: What are Instruction-Following Models?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是指令跟随模型？
- en: Language models are machine learning models that can predict word probability
    based on the sentence’s prior words. If we ask the model for the next word and
    feed it back to the model regressively to ask for more, the model is doing text
    generation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型是机器学习模型，它们可以根据句子的前文预测单词的概率。如果我们要求模型提供下一个单词，并将其反馈给模型以请求更多内容，模型就在进行文本生成。
- en: Text generation model is the idea behind many large language models such as
    GPT3\. Instruction-following models, however, are fine-tuned text generation models
    that learn about dialog and instructions. It is operated as a conversation between
    two people, and when one finishes a sentence, another person responds accordingly.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成模型是许多大型语言模型（如GPT3）的核心理念。然而，指令跟随模型是经过微调的文本生成模型，专门学习对话和指令。它的操作方式像是两个人之间的对话，一个人说完一句话，另一个人相应地回应。
- en: Therefore, a text generation model can help you finish a paragraph with a leading
    sentence. But an instruction following model can answer your questions or respond
    as requested.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，文本生成模型可以帮助你在有开头句子的情况下完成一段文字。但指令跟随模型可以回答你的问题或按要求做出回应。
- en: It doesn’t mean you cannot use a text generation model to build a chatbot. But
    you should find a better quality result with an instruction-following model, which
    is fine-tuned for such use.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着你不能使用文本生成模型来构建聊天机器人。但是，你应该使用经过微调的指令跟随模型，它能提供更高质量的结果。
- en: How to Find Instruction Following Models
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何寻找指令跟随模型
- en: You may find a lot of instruction following models nowadays. But to build a
    chatbot, you need something you can easily work with.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能会发现很多指令跟随模型。但是，要构建一个聊天机器人，你需要一个容易操作的模型。
- en: One handy repository that you can search on is Hugging Face. The models there
    are supposed to use with the transformers library from Hugging Face. It is helpful
    because different models may work slightly differently. It would be tedious to
    make your Python code to support multiple models, but the transformers library
    unified them and hide all those differences from your code.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一个方便的资源库是Hugging Face。那里提供的模型应与Hugging Face的transformers库一起使用。这非常有帮助，因为不同的模型可能会有细微的差别。虽然让你的Python代码支持多种模型可能很繁琐，但transformers库统一了它们，并隐藏了这些差异。
- en: '![](../Images/cae7a36c1110886474acb7651db3e9a0.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cae7a36c1110886474acb7651db3e9a0.png)'
- en: Usually, the instruction following models carries the keyword “instruct” in
    the model name. Searching with this keyword on Hugging Face can give you more
    than a thousand models. But not all can work. You need to check out each of them
    and read their model card to understand what this model can do in order to pick
    the most suitable one.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，指令跟随模型在模型名称中会带有关键词“instruct”。在Hugging Face上用这个关键词搜索可以找到超过一千个模型。但并非所有模型都能工作。你需要查看每一个模型，并阅读它们的模型卡，以了解这个模型可以做什么，从而选择最合适的一个。
- en: 'There are several technical criteria to pick your model:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 选择你的模型时，有几个技术标准需要考虑：
- en: '**What the model was trained on:** Specifically, that means which language
    the model can speak. A model trained with English text from novels probably is
    not helpful for a German chatbot for Physics.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型的训练数据是什么：** 具体来说，这意味着模型可以使用什么语言。一个用英语小说文本训练的模型可能对一个德语物理聊天机器人没有帮助。'
- en: '**What is the deep learning library it uses:** Usually models in Hugging Face
    are built with TensorFlow, PyTorch, and Flax. Not all models have a version for
    all libraries. You need to make sure you have that specific library installed
    before you can run a model with transformers.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它使用的深度学习库是什么：** 通常，Hugging Face中的模型是使用TensorFlow、PyTorch和Flax构建的。并非所有模型都有所有库的版本。你需要确保在运行`transformers`模型之前，已安装了特定的库。'
- en: '**What resources the model needs:** The model can be enormous. Often it would
    require a GPU to run. But some model needs a very high-end GPU or even multiple
    high-end GPUs. You need to verify if your resources can support the model inference.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型需要哪些资源：** 模型可能非常庞大。通常，它需要GPU来运行。但有些模型需要非常高端的GPU，甚至多个高端GPU。你需要确认你的资源是否支持模型推理。'
- en: Building a Simple Chatbot
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建一个简单的聊天机器人
- en: Let’s build a simple chatbot. The chatbot is just a program that runs on the
    command line, which takes one line of text as input from the user and responds
    with one line of text generated by the language model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个简单的聊天机器人。这个聊天机器人只是一个在命令行中运行的程序，它从用户那里获取一行文本作为输入，并生成一行由语言模型生成的文本作为回应。
- en: The model chosen for this task is `falcon-7b-instruct`. It is a 7-billion parameters
    model. You may need to run on a modern GPU such as nVidia RTX 3000 series since
    it was designed to run on bfloat16 floating point for best performance. Using
    the GPU resources on Google Colab, or from a suitable EC2 instance on AWS are
    also options.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为这个任务选择的模型是`falcon-7b-instruct`。它是一个拥有70亿参数的模型。由于它是为最佳性能设计的，需要在bfloat16浮点数下运行，因此你可能需要使用现代GPU，例如nVidia
    RTX 3000系列。利用Google Colab上的GPU资源或AWS上的适当EC2实例也是一种选择。
- en: 'To build a chatbot in Python, it is as simple as the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Python中构建聊天机器人，过程如下：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `input("> ")` function takes one line of input from the user. You will see
    the string `"> "` on the screen for your input. Input is captured once you press
    Enter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`input("> ")`函数从用户那里获取一行输入。你会在屏幕上看到字符串`"> "`来提示你的输入。输入会在你按下Enter后被捕获。'
- en: The reminaing question is how to get the response. In LLM, you provide your
    input, or prompt, as a sequence of token IDs (integers), and it will respond with
    another sequence of token IDs. You should convert between the sequence of integers
    and text string before and after interacting with LLMs. The token IDs are specific
    for each model; that is, for the same integer, it means a different word for a
    different model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的问题是如何获取响应。在LLM中，你将输入或提示作为令牌ID（整数）的序列提供，模型会回应另一个令牌ID序列。在与LLM交互前后，你应该在整数序列和文本字符串之间进行转换。令牌ID对每个模型都是特定的；也就是说，对于相同的整数，不同模型表示不同的词。
- en: 'Hugging Face library `transformers` is to make these steps easier. All you
    need is to create a pipeline and specify the model name some a few other paramters.
    Setting up a pipeline with the model name `tiiuae/falcon-7b-instruct`, with bfloat16
    floating point, and allows the model to use GPU if available is as the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face库`transformers`旨在简化这些步骤。你只需创建一个管道并指定模型名称以及其他几个参数。设置一个使用bfloat16浮点数的模型名称为`tiiuae/falcon-7b-instruct`的管道，并允许模型在有GPU时使用，配置如下：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The pipeline is created as `"text-generation"` because it is the way the model
    card suggested you to work with this model. A pipeline in `transformers` is a
    sequence of steps for a specific task. Text-generation is one of these tasks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个管道被创建为`"text-generation"`，因为这是模型卡建议的工作方式。`transformers`中的管道是用于特定任务的一系列步骤。文本生成就是这些任务之一。
- en: To use the pipeline, you need to specify a few more parameters for generating
    the text. Recall that the model is not generating the text directly but the probabilities
    of tokens. You have to determine what is the next word from these probabilities
    and repeat the process to generate more words. Usually, this process will introduce
    some variations by not picking the single token with the highest probability but
    sampling according to the probability distribution.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用管道时，你需要指定更多的参数来生成文本。请记住，模型并不是直接生成文本，而是生成令牌的概率。你必须从这些概率中确定下一个词，并重复这个过程以生成更多词。通常，这个过程会引入一些变异，通过不选择概率最高的单一令牌，而是根据概率分布进行采样。
- en: 'Below is how you’re going to use the pipeline:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用管道的步骤：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You provided the prompt in the variable `prompt` to generate the output sequences.
    You can ask the model to give you a few options, but here you set `num_return_sequences=1`
    so there would only be one. You also let the model to generate text using sampling,
    but only from the 10 highest probability tokens (`top_k=10`). The returned sequence
    will not contain your prompt since you have `return_full_text=False`. The most
    important one parameters are `eos_token_id=newline_token` and `pad_token_id=tokenizer.eos_token_id`.
    These are to let the model to generate text continuously, but only until a newline
    character. The newline character’s token id is 193, as obtained from the first
    line in the code snippet.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你将提示内容提供给变量`prompt`来生成输出序列。你可以让模型给出几个选项，但在这里你设置了`num_return_sequences=1`，因此只会有一个选项。你还让模型使用采样来生成文本，但只从10个最高概率的标记中进行采样（`top_k=10`）。返回的序列不会包含你的提示，因为你设置了`return_full_text=False`。最重要的参数是`eos_token_id=newline_token`和`pad_token_id=tokenizer.eos_token_id`。这些参数用于让模型连续生成文本，但仅到换行符为止。换行符的标记
    ID 是193，来自代码片段的第一行。
- en: 'The returned `sequences` is a list of dictionaries (list of one dict in this
    case). Each dictionary contains the token sequence and string. We can easily print
    the string as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的`sequences`是一个字典列表（在这种情况下是一个字典的列表）。每个字典包含标记序列和字符串。我们可以很容易地打印字符串，如下所示：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A language model is memoryless. It will not remember how many times you used
    the model and the prompts you used before. Every time is new, so you need to provide
    the history of the previous dialog to the model. This is easily done. But because
    it is an instruction-following model that knows how to process a dialog, you need
    to remember to identify which person said what in the prompt. Let’s assume it
    is a dialog between Alice and Bob (or any names). You prefix the name in each
    sentence they spoke in the prompt, like the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型是无记忆的。它不会记住你使用模型的次数以及之前使用的提示。每次都是新的，因此你需要向模型提供之前对话的历史。这很简单。但因为它是一个会话处理模型，你需要记住在提示中识别出谁说了什么。假设这是Alice和Bob之间的对话（或任何名字）。你需要在提示中每个句子前加上他们的名字，如下所示：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then the model should generate text that match the dialog. Once the response
    from the model is obtained, to append it together with another text from Alice
    to the prompt, and send to the model again. Putting everything together, below
    is a simple chatbot:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后模型应该生成与对话匹配的文本。一旦从模型获得响应，将其与来自Alice的其他文本一起追加到提示中，然后再次发送给模型。将所有内容结合起来，以下是一个简单的聊天机器人：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Notice how the `dialog` variable is updated to keep track on the dialog in each
    iteration, and how it is used to set variable `prompt` for the next run of the
    pipeline.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`dialog`变量如何在每次迭代中更新以跟踪对话，以及如何用它来设置变量`prompt`以进行下一次管道运行。
- en: 'When you try to ask “What is relativity” with the chatbot, it doesn’t sound
    very knowledgable. That’s where you need to do some prompt engineering. You can
    make Bob a Physics professor so he can have more detailed answer on this topic.
    That’s the magic of LLMs that can adjust the response by a simple change in the
    prompt. All you need is to add a description before the dialog started. Updated
    code is as follows (see now `dialog` is initialized with a persona description):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当你尝试用聊天机器人问“什么是相对论”时，它的回答听起来不够专业。这时你需要进行一些提示工程。你可以让Bob成为物理学教授，这样他就能对这个话题给出更详细的回答。这就是LLMs的魔力，通过简单地改变提示来调整响应。你只需要在对话开始之前添加一个描述。更新后的代码如下（现在`dialog`初始化时带有角色描述）：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This chatbot may be slow if you do not have powerful enough hardware. You may
    not see the exact result, but the following is an example dialog from the above
    code.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有足够强大的硬件，这个聊天机器人可能会很慢。你可能无法看到确切的结果，但以下是上述代码的示例对话。
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The chatbot will run until you press Ctrl-C to stop it or meet the maximum length
    (`max_length=500`) in the pipeline input. The maximum length is how much your
    model can read at a time. Your prompt must be no more than this many tokens. The
    higher this maximum length will make the model run slower, and every model has
    a limit on how large you can set this length. The `falcon-7b-instruct` model allows
    you to set this to 2048 only. ChatGPT, on the other hand, is 4096.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人将一直运行，直到你按下 Ctrl-C 停止它，或者遇到管道输入中的最大长度（`max_length=500`）。最大长度是指模型一次可以读取的字数。你的提示不能超过这个字数。最大长度越高，模型运行越慢，每个模型对这个长度的设置都有一个限制。`falcon-7b-instruct`
    模型仅允许将此设置为 2048，而 ChatGPT 则为 4096。
- en: You may also notice the output quality is not perfect. Partially because you
    didn’t attempt to polish the response from the model before sending back to the
    user, and partially because the model we chose is a 7-billion parameters model,
    which is the smallest in its family. Usually you will see a better result with
    a larger model. But that would also require more resources to run.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还会注意到输出质量不是很完美。这部分是因为你没有在将模型的响应发送回用户之前尝试润色，同时也是因为我们选择的模型是一个拥有 70 亿参数的模型，是该系列中最小的。通常，你会看到较大模型的结果更好，但这也需要更多的资源来运行。
- en: Further Readings
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Below is a paper that may help you understand better about the instruction
    following model:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一篇可能帮助你更好理解指令遵循模型的论文：
- en: '[Ouyang et al, Training language models to follow instructions with human feedback
    (2022)](https://arxiv.org/pdf/2203.02155.pdf)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[欧阳等，《训练语言模型以遵循指令并获取人类反馈》（2022）](https://arxiv.org/pdf/2203.02155.pdf)'
- en: Summary
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this post, you learned how to create a chatbot using a large language model
    from the Hugging Face library. Specifically, you learned:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，你学会了如何使用 Hugging Face 库中的大型语言模型创建一个聊天机器人。具体来说，你学会了：
- en: A language model that can do conversation is called instruction-following models
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能进行对话的语言模型称为指令遵循模型
- en: How to find such models in Hugging Face
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在 Hugging Face 中找到这些模型
- en: How to use the models using the `transformers` library, and build a chatbot
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 `transformers` 库中的模型，并构建一个聊天机器人
