- en: The Transformer Positional Encoding Layer in Keras, Part 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras中的变压器位置编码层，第2部分
- en: 原文：[https://machinelearningmastery.com/the-transformer-positional-encoding-layer-in-keras-part-2/](https://machinelearningmastery.com/the-transformer-positional-encoding-layer-in-keras-part-2/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://machinelearningmastery.com/the-transformer-positional-encoding-layer-in-keras-part-2/](https://machinelearningmastery.com/the-transformer-positional-encoding-layer-in-keras-part-2/)
- en: In [part 1, a gentle introduction to positional encoding in transformer models](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1),
    we discussed the positional encoding layer of the transformer model. We also showed
    how you could implement this layer and its functions yourself in Python. In this
    tutorial, you’ll implement the positional encoding layer in Keras and Tensorflow.
    You can then use this layer in a complete transformer model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1部分：变压器模型中位置编码的温和介绍](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1)中，我们讨论了变压器模型的位置信息编码层。我们还展示了如何在Python中自行实现该层及其功能。在本教程中，你将实现Keras和Tensorflow中的位置编码层。然后，你可以在完整的变压器模型中使用此层。
- en: 'After completing this tutorial, you will know:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本教程后，你将了解：
- en: Text vectorization in Keras
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras中的文本向量化
- en: Embedding layer in Keras
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras中的嵌入层
- en: How to subclass the embedding layer and write your own positional encoding layer.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何子类化嵌入层并编写你自己的位置编码层。
- en: '**Kick-start your project** with my book [Building Transformer Models with
    Attention](https://machinelearningmastery.com/transformer-models-with-attention/).
    It provides **self-study tutorials** with **working code** to guide you into building
    a fully-working transformer model that can'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**启动你的项目**，请参阅我的书籍[《构建具有注意力机制的变压器模型》](https://machinelearningmastery.com/transformer-models-with-attention/)。它提供了**自学教程**和**可运行的代码**，帮助你构建一个完全可用的变压器模型。'
- en: '*translate sentences from one language to another*...'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*将句子从一种语言翻译成另一种语言*...'
- en: Let’s get started.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: '[![](../Images/c49220b4a30fb49a2b6d0819242de294.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/ijaz-rafi-photo-1551102076-9f8bb5f3f897.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/c49220b4a30fb49a2b6d0819242de294.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/ijaz-rafi-photo-1551102076-9f8bb5f3f897.jpg)'
- en: The transformer positional encoding layer in Keras, part 2
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Keras中的变压器位置编码层，第2部分
- en: Photo by Ijaz Rafi. Some rights reserved
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由Ijaz Rafi提供。保留部分权利
- en: Tutorial Overview
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 教程概述
- en: 'This tutorial is divided into three parts; they are:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程分为三个部分；它们是：
- en: Text vectorization and embedding layer in Keras
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Keras中的文本向量化和嵌入层
- en: Writing your own positional encoding layer in Keras
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Keras中编写你自己的位置编码层
- en: Randomly initialized and tunable embeddings
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化和可调的嵌入
- en: Fixed weight embeddings from [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 来自[《Attention Is All You Need》](https://arxiv.org/abs/1706.03762)的固定权重嵌入
- en: Graphical view of the output of the positional encoding layer
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 位置信息编码层输出的图形视图
- en: The Import Section
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入部分
- en: 'First, let’s write the section to import all the required libraries:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来写一段代码以导入所有必需的库：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The Text Vectorization Layer
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本向量化层
- en: 'Let’s start with a set of English phrases that are already preprocessed and
    cleaned. The text vectorization layer creates a dictionary of words and replaces
    each word with its corresponding index in the dictionary. Let’s see how you can
    map these two sentences using the text vectorization layer:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一组已经预处理和清理过的英文短语开始。文本向量化层创建一个单词字典，并用字典中对应的索引替换每个单词。让我们看看如何使用文本向量化层来映射这两个句子：
- en: I am a robot
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我是一个机器人
- en: you too robot
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你也是机器人
- en: 'Note the text has already been converted to lowercase with all the punctuation
    marks and noise in the text removed. Next, convert these two phrases to vectors
    of a fixed length 5\. The `TextVectorization` layer of Keras requires a maximum
    vocabulary size and the required length of an output sequence for initialization.
    The output of the layer is a tensor of shape:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，文本已经被转换为小写，并且所有标点符号和文本中的噪声都已被移除。接下来，将这两个短语转换为固定长度为5的向量。Keras的`TextVectorization`层需要一个最大词汇量和初始化时所需的输出序列长度。该层的输出是一个形状为：
- en: '`(number of sentences, output sequence length)`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`(句子数量，输出序列长度)`'
- en: The following code snippet uses the `adapt` method to generate a vocabulary.
    It next creates a vectorized representation of the text.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段使用`adapt`方法生成词汇表。接下来，它创建文本的向量化表示。
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Output
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Want to Get Started With Building Transformer Models with Attention?
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 想要开始构建具有注意力机制的变压器模型吗？
- en: Take my free 12-day email crash course now (with sample code).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在免费参加我的12天电子邮件速成课程（附有示例代码）。
- en: Click to sign-up and also get a free PDF Ebook version of the course.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 单击注册，还可获得课程的免费PDF电子书版本。
- en: The Embedding Layer
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入层
- en: The Keras `Embedding` layer converts integers to dense vectors. This layer maps
    these integers to random numbers, which are later tuned during the training phase.
    However, you also have the option to set the mapping to some predefined weight
    values (shown later). To initialize this layer, you need to specify the maximum
    value of an integer to map, along with the length of the output sequence.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的`Embedding`层将整数转换为密集向量。此层将这些整数映射到随机数，后者在训练阶段进行调整。但是，您也可以选择将映射设置为一些预定义的权重值（稍后显示）。要初始化此层，您需要指定要映射的整数的最大值，以及输出序列的长度。
- en: The Word Embeddings
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词嵌入
- en: Let’s see how the layer converts the `vectorized_text` to tensors.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 看看这一层是如何将`vectorized_text`转换为张量的。
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The output has been annotated with some comments, as shown below. Note that
    you will see a different output every time you run this code because the weights
    have been initialized randomly.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 输出已经用一些注释进行了标注，如下所示。请注意，每次运行此代码时都会看到不同的输出，因为权重已随机初始化。
- en: '[![Word embeddings.](../Images/bf60b0857bca389b9fb20cb1009a3674.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/PEKeras_a.png)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[![词嵌入。](../Images/bf60b0857bca389b9fb20cb1009a3674.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/PEKeras_a.png)'
- en: Word embeddings. This output will be different every time you run the code because
    of the random numbers involved.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入。由于涉及到随机数，每次运行代码时，输出都会有所不同。
- en: The Position Embeddings
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 位置嵌入
- en: You also need the embeddings for the corresponding positions. The maximum positions
    correspond to the output sequence length of the `TextVectorization` layer.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要相应位置的嵌入。最大位置对应于`TextVectorization`层的输出序列长度。
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The output is shown below:[![Position Indices Embedding. ](../Images/cca2e4e47dade3ba170cdd2eb0c3393e.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/PEKeras_b.png)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：[![位置索引嵌入。](../Images/cca2e4e47dade3ba170cdd2eb0c3393e.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/PEKeras_b.png)
- en: Position indices embedding
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 位置索引嵌入
- en: The Output of Positional Encoding Layer in Transformers
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变换器中位置编码层的输出
- en: In a transformer model, the final output is the sum of both the word embeddings
    and the position embeddings. Hence, when you set up both embedding layers, you
    need to make sure that the `output_length` is the same for both.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在变换器模型中，最终输出是词嵌入和位置嵌入的总和。因此，当设置这两个嵌入层时，您需要确保`output_length`对两者都是相同的。
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output is shown below, annotated with comments. Again, this will be different
    from your run of the code because of the random weight initialization.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下，带有注释。同样，由于随机权重初始化的原因，这将与您的代码运行结果不同。
- en: '[![](../Images/ca74f591c2ab37e5ff0ab5dfe3069bfb.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/PEKeras_c.png)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/ca74f591c2ab37e5ff0ab5dfe3069bfb.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/PEKeras_c.png)'
- en: The final output after adding word embedding and position embedding
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 添加了词嵌入和位置嵌入后的最终输出
- en: SubClassing the Keras Embedding Layer
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 子类化Keras Embedding层
- en: When implementing a transformer model, you’ll have to write your own position
    encoding layer. This is quite simple, as the basic functionality is already provided
    for you. This [Keras example](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/)
    shows how you can subclass the `Embedding` layer to implement your own functionality.
    You can add more methods to it as you require.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当实现变换器模型时，您将不得不编写自己的位置编码层。这相当简单，因为基本功能已为您提供。这个[Keras示例](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/)展示了如何子类化`Embedding`层以实现自己的功能。您可以根据需要添加更多的方法。
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s run this layer.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行这一层。
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Output
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Positional Encoding in Transformers: Attention Is All You Need'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变换器中的位置编码：注意力机制是您所需的
- en: Note the above class creates an embedding layer that has trainable weights.
    Hence, the weights are initialized randomly and tuned in to the training phase.The
    authors of [Attention Is All You Need](https://arxiv.org/abs/1706.03762) have
    specified a positional encoding scheme, as shown below. You can read the full
    details in [part 1](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1)
    of this tutorial:\begin{eqnarray}
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，上述类创建了一个具有可训练权重的嵌入层。因此，权重被随机初始化并在训练阶段进行调整。[Attention Is All You Need](https://arxiv.org/abs/1706.03762)的作者指定了一个位置编码方案，如下所示。你可以在本教程的[第1部分](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1)中阅读详细信息：\begin{eqnarray}
- en: P(k, 2i) &=& \sin\Big(\frac{k}{n^{2i/d}}\Big)\\
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: P(k, 2i) &=& \sin\Big(\frac{k}{n^{2i/d}}\Big)\\
- en: P(k, 2i+1) &=& \cos\Big(\frac{k}{n^{2i/d}}\Big)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: P(k, 2i+1) &=& \cos\Big(\frac{k}{n^{2i/d}}\Big)
- en: \end{eqnarray}If you want to use the same positional encoding scheme, you can
    specify your own embedding matrix, as discussed in [part 1](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1),
    which shows how to create your own embeddings in NumPy. When specifying the `Embedding`
    layer, you need to provide the positional encoding matrix as weights along with
    `trainable=False`. Let’s create another positional embedding class that does exactly
    this. [PRE9]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: \end{eqnarray}如果你想使用相同的位置编码方案，你可以指定自己的嵌入矩阵，如[第1部分](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1)中讨论的那样，该部分展示了如何在NumPy中创建自己的嵌入。当指定`Embedding`层时，你需要提供位置编码矩阵作为权重，并设置`trainable=False`。让我们创建一个新的位置嵌入类来完成这一操作。[PRE9]
- en: Next, we set up everything to run this layer.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置一切以运行这一层。
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Output
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Visualizing the Final Embedding
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化最终嵌入
- en: 'In order to visualize the embeddings, let’s take two bigger sentences: one
    technical and the other one just a quote. We’ll set up the `TextVectorization`
    layer along with the positional encoding layer and see what the final output looks
    like.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化嵌入，我们将选择两个较大的句子：一个技术性的，另一个只是一个引用。我们将设置`TextVectorization`层以及位置编码层，看看最终输出的效果。
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now let’s see what the random embeddings look like for both phrases.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看两个短语的随机嵌入是什么样的。
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![Random embeddings](../Images/f7e01002176d1edc12b5dbfe2bac9c92.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/PEKeras1.png)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[![随机嵌入](../Images/f7e01002176d1edc12b5dbfe2bac9c92.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/PEKeras1.png)'
- en: Random embeddings
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 随机嵌入
- en: The embedding from the fixed weights layer are visualized below.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 固定权重层的嵌入如下图所示。
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![Embedding using sinusoidal positional encoding](../Images/20b112bbd293286cb12154715a6c5724.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/PEKeras2.png)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[![使用正弦位置编码的嵌入](../Images/20b112bbd293286cb12154715a6c5724.png)](https://machinelearningmastery.com/wp-content/uploads/2022/02/PEKeras2.png)'
- en: Embedding using sinusoidal positional encoding
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正弦位置编码的嵌入
- en: You can see that the embedding layer initialized using the default parameter
    outputs random values. On the other hand, the fixed weights generated using sinusoids
    create a unique signature for every phrase with information on each word position
    encoded within it.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，使用默认参数初始化的嵌入层输出随机值。另一方面，使用正弦波生成的固定权重为每个短语创建了一个独特的签名，其中包含了每个单词位置的信息。
- en: You can experiment with tunable or fixed-weight implementations for your particular
    application.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以根据具体应用尝试可调或固定权重的实现。
- en: Further Reading
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: This section provides more resources on the topic if you are looking to go deeper.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了更多资源，如果你想深入了解这个话题。
- en: Books
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 书籍
- en: '[Transformers for natural language processing](https://www.amazon.com/Transformers-Natural-Language-Processing-architectures/dp/1800565798)
    by Denis Rothman'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理中的Transformers](https://www.amazon.com/Transformers-Natural-Language-Processing-architectures/dp/1800565798)
    作者：Denis Rothman'
- en: Papers
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 论文
- en: '[Attention Is All You Need](https://arxiv.org/abs/1706.03762), 2017'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Attention Is All You Need](https://arxiv.org/abs/1706.03762)，2017年'
- en: Articles
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文章
- en: '[The Transformer Attention Mechanism](https://machinelearningmastery.com/the-transformer-attention-mechanism/)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Transformer注意力机制](https://machinelearningmastery.com/the-transformer-attention-mechanism/)'
- en: '[The Transformer Model](https://machinelearningmastery.com/the-transformer-model/)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Transformer模型](https://machinelearningmastery.com/the-transformer-model/)'
- en: '[Transformer Model for Language Understanding](https://www.tensorflow.org/text/tutorials/transformer)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于语言理解的Transformer模型](https://www.tensorflow.org/text/tutorials/transformer)'
- en: '[Using Pre-Trained Word Embeddings in a Keras Model](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Keras 模型中使用预训练的词嵌入](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)'
- en: '[English-to-Spanish translation with a sequence-to-sequence Transformer](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用序列到序列变换器进行英语到西班牙语翻译](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/)'
- en: '[A Gentle Introduction to Positional Encoding in Transformer Models, Part 1](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[转换器模型中位置编码的简介（第一部分）](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1)'
- en: Summary
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this tutorial, you discovered the implementation of positional encoding layer
    in Keras.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您了解了 Keras 中位置编码层的实现。
- en: 'Specifically, you learned:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，您学到了：
- en: Text vectorization layer in Keras
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 中的文本向量化层
- en: Positional encoding layer in Keras
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 中的位置编码层
- en: Creating your own class for positional encoding
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建自己的位置编码类
- en: Setting your own weights for the positional encoding layer in Keras
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 Keras 中的位置编码层设置自定义权重
- en: Do you have any questions about positional encoding discussed in this post?
    Ask your questions in the comments below, and I will do my best to answer.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中讨论的位置编码有任何问题吗？在下面的评论中提问，我会尽力回答。
