# 什么是注意力？

> 原文：[`machinelearningmastery.com/what-is-attention/`](https://machinelearningmastery.com/what-is-attention/)

注意力在机器学习中越来越受欢迎，但是什么使它成为一个如此吸引人的概念？在人工神经网络中应用的注意力与其生物学对应物之间有什么关系？在机器学习中，人们可以期待什么组件形成一个基于注意力的系统？

在本教程中，你将发现注意力的概述及其在机器学习中的应用。

完成本教程后，你将了解：

+   简要概述注意力如何在大脑中表现

+   组成基于注意力的系统的组件及其如何受到生物学注意力的启发

**启动你的项目**，请参考我的书 [构建具有注意力的变换器模型](https://machinelearningmastery.com/transformer-models-with-attention/)。它提供了**自学教程**和**工作代码**，帮助你构建一个完全可用的变换器模型。

*将句子从一种语言翻译成另一种语言*...

让我们开始吧。

![](https://machinelearningmastery.com/wp-content/uploads/2021/09/what_is_attention_cover-scaled.jpg)

什么是注意力？

图片来源：[Rod Long](https://unsplash.com/photos/J-ygvQbilXU)，保留部分权利。

## **教程概述**

本教程分为两部分；它们是：

+   +   注意力

    +   机器学习中的注意力

## **注意事项**

注意力是一个广泛研究的概念，通常与唤醒、警觉性和与周围环境的参与有关。

> *在最一般的形式下，注意力可以被描述为对周围环境的总体警觉性或参与能力。*
> 
> *–* [心理学、神经科学和机器学习中的注意力](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full)，2020 年。

*视觉* 注意力是神经科学和心理学领域中最常研究的领域之一。

当一个对象面对不同的图像时，该对象的眼动可以揭示出*显著*的图像部分，这些部分是对象的注意力最感兴趣的。在对视觉注意力的计算模型进行回顾时，[Itti 和 Koch (2001)](https://authors.library.caltech.edu/40408/1/391.pdf)提到这些显著的图像部分通常由视觉属性特征，包括强度对比、定向边缘、角落和交点以及运动所特征化。人脑在不同的神经阶段关注这些显著的视觉特征。

> *早期的神经元对简单的视觉属性如强度对比、颜色对抗、方向、运动方向和速度，或在多个空间尺度上的立体视差进行了调节。神经元的调节随着从低级到高级视觉区域的进展而变得越来越专业，以至于高级视觉区域包括仅对角点或交点、从阴影中获取形状线索或特定现实世界物体的视图做出反应的神经元。*
> 
> – [计算视觉注意力建模](https://authors.library.caltech.edu/40408/1/391.pdf)，2001 年。

有趣的是，研究还观察到不同的对象往往被相同的显著视觉线索所吸引。

研究还发现了记忆和注意力之间的几种交互形式。由于人脑的记忆容量有限，因此选择存储哪些信息在最大程度利用有限资源上变得至关重要。人脑通过依赖注意力来实现这一点，从而动态地将受关注的信息存储在记忆中。

## **机器学习中的注意力**

在人工神经网络中实现注意力机制并不一定追踪人脑的生物学和心理学机制。相反，使注意力在机器学习中如此有吸引力的概念是它能够动态地突出和利用手头信息中的*显著*部分，这与人脑的处理方式类似。

想象一个由三个组件组成的基于注意力的系统：

> 1.  *一个“读取”原始数据（如源句子中的源词）并将其转换为分布式表示的过程，每个词位置都关联一个特征向量。*
> 1.  
> 1.  *存储读取器输出的特征向量列表。这可以理解为一个包含事实序列的“记忆”，这些事实可以在之后检索，顺序不一定相同，无需访问全部内容。*
> 1.  
> 1.  *一个“利用”记忆内容来顺序执行任务的过程，每一步都能将注意力集中在一个记忆元素（或几个，具有不同权重）上。*
> 1.  
> – 第 491 页，[深度学习](https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=sr_1_1?dchild=1&keywords=deep+learning&qid=1622968138&sr=8-1)，2017 年。

以编码器-解码器框架为例，因为在这样的框架中首次引入了注意力机制。

如果我们处理一个输入的词序列，这将首先被送入一个编码器，该编码器会为序列中的每个元素输出一个向量。这对应于我们前面提到的基于注意力的系统的第一个组件。

这些向量列表（上面基于注意力系统的第二个组成部分），连同解码器的先前隐藏状态，将被注意机制利用来动态突出显示将用于生成输出的输入信息。

在每个时间步中，注意机制会取解码器的先前隐藏状态和编码向量列表，利用它们生成未归一化的*分数*值，表明输入序列的元素与当前输出的对齐程度如何。由于生成的分数值需要相对重要性，它们通过 softmax 函数进行归一化以生成*权重*。在 softmax 归一化之后，所有权重值将位于 [0, 1] 区间并加起来为 1，这意味着它们可以被解释为概率。最后，编码向量通过计算得到的权重进行缩放，生成*上下文向量*。这个注意过程形成了上面基于注意力系统的第三个组成部分。然后将这个上下文向量送入解码器以生成翻译输出。

> *这种类型的人工注意力因此是一种迭代重新加权的形式。具体来说，它会动态突出显示预处理输入的不同组件，因为它们对输出生成是必要的。这使其像生物学的注意力一样灵活和上下文相关。*
> 
> *–* [心理学、神经科学和机器学习中的注意力](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full)，2020 年。

包含注意机制的系统实施的过程与不包含的系统相比有所不同。在后者中，编码器会生成一个固定长度的向量，而不考虑输入的长度或复杂性。在没有突出显示整个输入中显著信息的机制的情况下，解码器只能访问编码在固定长度向量中的有限信息。这可能导致解码器错过重要信息。

注意机制最初是为了处理机器翻译中的单词序列提出的，这些序列具有暗含的时间因素。然而，它可以推广到处理静态信息，不一定以顺序方式相关，比如在图像处理的背景下。您将看到如何在单独的教程中实现这种泛化。

### 想要开始使用带注意力机制的 Transformer 模型吗？

立即参加我的免费 12 天电子邮件快速课程（附带示例代码）。

点击注册，还可获得课程的免费 PDF 电子书版本。

## **进一步阅读**

本节提供了关于该主题的更多资源，如果您希望深入了解。

### **书籍**

+   [深度学习基础](https://www.amazon.com/Deep-Learning-Essentials-hands-fundamentals/dp/1785880365)，2018 年。

+   [深度学习](https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=sr_1_1?dchild=1&keywords=deep+learning&qid=1622968138&sr=8-1)，2017 年。

### **论文**

+   [心理学、神经科学和机器学习中的注意力](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full)，2020 年。

+   [视觉注意力的计算建模](https://authors.library.caltech.edu/40408/1/391.pdf)，2001 年。

## **总结**

在本教程中，你了解了注意力的概述及其在机器学习中的应用。

具体来说，你学到了：

+   注意力如何在大脑中表现的简要概述

+   组成注意力系统的各个部分及其如何受到生物学注意力的启发

你有任何问题吗？

在下方评论中提问，我会尽力回答。
