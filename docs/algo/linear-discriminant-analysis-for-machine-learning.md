# 机器学习中的线性判别分析

> 原文： [`machinelearningmastery.com/linear-discriminant-analysis-for-machine-learning/`](https://machinelearningmastery.com/linear-discriminant-analysis-for-machine-learning/)

逻辑回归是一种传统上仅限于两类分类问题的分类算法。

如果您有两个以上的类，则线性判别分析是首选的线性分类技术。

在这篇文章中，您将发现用于分类预测性建模问题的线性判别分析（LDA）算法。阅读这篇文章后你会知道：

*   逻辑回归的局限性和线性判别分析的必要性。
*   从数据中学习的模型表示，可以保存到文件中。
*   如何根据您的数据估算模型。
*   如何从学习的 LDA 模型中做出预测。
*   如何准备数据以充分利用 LDA 模型。

本文面向对应用机器学习感兴趣的开发人员，模型如何工作以及如何使用它们。因此，不需要统计学或线性代数的背景，尽管如果您了解分布的[平均值](https://en.wikipedia.org/wiki/Mean)和[方差](https://en.wikipedia.org/wiki/Variance)，它确实有帮助。

LDA 是制备和应用中的简单模型。关于如何设置模型以及如何推导出预测方程，背后有一些有趣的统计量，但本文未对此进行介绍。

让我们开始吧。

![Linear Discriminant Analysis for Machine Learning](img/22b3c2c5fa8914da52405573a5198cc6.jpg)

机器学习的线性判别分析
照片由 [Jamie McCaffrey](https://www.flickr.com/photos/15609463@N03/14898932531) 拍摄，保留一些权利。

## 逻辑回归的局限性

逻辑回归是一种简单而强大的线性分类算法。它还有一些限制，表明需要备用线性分类算法。

*   **两类问题**。 逻辑回归旨在用于两类或二分类问题。它可以扩展为多分类，但很少用于此目的。
*   **不稳定，分离良好**。当类很好地分离时，逻辑回归可能变得不稳定。
*   **不稳定的几个例子**。当几乎没有用于估计参数的示例时，逻辑回归可能变得不稳定。

线性判别分析确实解决了这些问题，并且是多分类问题的首选线性方法。即使存在二分类问题，尝试逻辑回归和线性判别分析也是一个好主意。

## LDA 模型的表示

LDA 的代表是直截了当的。

它包含数据的统计属性，为每个类计算。对于单个输入变量（x），这是每个类的变量的均值和方差。对于多个变量，这是通过多元高斯计算的相同属性，即均值和协方差矩阵。

这些统计特性根据您的数据估算并插入 LDA 方程式做出预测。这些是您将保存为模型文件的模型值。

我们来看看如何估算这些参数。

## 获取免费算法思维导图

![Machine Learning Algorithms Mind Map](img/2ce1275c2a1cac30a9f4eea6edd42d61.jpg)

方便的机器学习算法思维导图的样本。

我已经创建了一个由类型组织的 60 多种算法的方便思维导图。

下载，打印并使用它。

## 学习 LDA 模型

LDA 对您的数据做了一些简化的假设：

1.  您的数据是高斯数据，每个变量在绘制时形状像钟形曲线。
2.  每个属性具有相同的方差，每个变量的值在均值周围平均变化相同的量。

通过这些假设，LDA 模型可以估算每个类别数据的均值和方差。在具有两个类的单变量（单输入变量）情况下，很容易想到这一点。

通过将值的总和除以值的总数，可以以正常方式估计每个类（k）的每个输入（x）的平均值（μ）。

muk = 1 / nk * sum（x）

其中 muk 是类 k 的 x 的平均值，nk 是类 k 的实例数。在所有类别中计算方差，作为每个值与平均值的平均平方差。

sigma ^ 2 = 1 /（n-K）* sum（（x-mu）^ 2）

其中 sigma ^ 2 是所有输入（x）的方差，n 是实例的数量，K 是类的数量，mu 是输入 x 的平均值。

## 用 LDA 做出预测

LDA 通过估计一组新输入属于每个类的概率来做出预测。获得概率最高的类是输出类，并做出预测。

该模型使用贝叶斯定理来估计概率。简言之[贝叶斯定理](https://en.wikipedia.org/wiki/Bayes%27_theorem)可用于估计输出类别（k）的概率，给定输入（x）使用每个类别的概率和属于每个类别的数据的概率：

P（Y = x | X = x）=（PIk * fk（x））/ sum（PIl * fl（x））

其中 PIk 指的是训练数据中观察到的每个类别（k）的基本概率（例如，对于两类问题中的 50-50 分裂，则为 0.5）。在贝叶斯定理中，这称为先验概率。

PIk = nk / n

上面的 f（x）是属于该类的 x 的估计概率。高斯分布函数用于 f（x）。将高斯插入上述方程并简化我们最终得到下面的等式。这被称为判别函数，并且类被计算为具有最大值将是输出分类（y）：

Dk（x）= x *（muk / siga ^ 2） - （muk ^ 2 /（2 * sigma ^ 2））+ ln（PIk）

Dk（x）是给定输入 x 的类 k 的判别函数，muk，sigma ^ 2 和 PIk 都是根据您的数据估计的。

## 如何为 LDA 准备数据

本节列出了在准备用于 LDA 的数据时可能会考虑的一些建议。

*   **分类问题**。这可能不言而喻，但 LDA 旨在用于输出变量是分类的分类问题。 LDA 支持二进制和多分类。
*   **高斯分布**。该模型的标准实现假设输入变量的高斯分布。考虑检查每个属性的单变量分布并使用变换使它们看起来更加高斯（例如，指数分布的 log 和 root 以及偏斜分布的 Box-Cox）。
*   **删除异常值**。考虑从数据中删除异常值。这些可能会扭曲用于在 LDA 中分离类的基本统计量，例如均值和标准差。
*   **相同的差异。 LDA** 假设每个输入变量具有相同的方差。在使用 LDA 之前标准化数据几乎总是一个好主意，因此它的平均值为 0，标准差为 1。

## LDA 的扩展

线性判别分析是一种简单有效的分类方法。因为它很简单并且很容易理解，所以该方法有许多扩展和变化。一些流行的扩展包括：

*   **二次判别分析（QDA）**：每个类使用自己的方差估计（或有多个输入变量时的协方差）。
*   **灵敏判别分析（FDA）**：使用非线性输入组合，如样条曲线。
*   **正则化判别分析（RDA）**：将正则化引入方差估计（实际上是协方差），缓和不同变量对 LDA 的影响。

最初的发展被称为线性判别分析或 Fisher 判别分析。多类版本被称为多判别分析。现在，这些都被简称为线性判别分析。

## 进一步阅读

如果您希望更深入，本节提供了一些额外的资源。我不得不相信这本书[统计学习简介：在 R](http://www.amazon.com/dp/1461471370?tag=inspiredalgor-20) 中的应用程序，本文中的一些描述和符号取自本文，它非常好。

### 图书

*   [统计学习导论：应用于 R](http://www.amazon.com/dp/1461471370?tag=inspiredalgor-20) ，第四章，第 138 页。
*   [现代多元统计技术：回归，分类和流形学习](http://www.amazon.com/dp/0387781889?tag=inspiredalgor-20)，第八章
*   [Applied Predictive Modeling](http://www.amazon.com/dp/1461468485?tag=inspiredalgor-20) ，第十二章，第 287 页

### 其他

*   [线性判别分析一点一点](http://sebastianraschka.com/Articles/2014_python_lda.html)（Python 的例子）
*   [线性判别分析维基百科页面](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)（我没有发现它有用）
*   [线性判别分析](http://www.saedsayad.com/lda.htm)（包括指向交互式 LDA 接口的链接）

## 摘要

在这篇文章中，您发现了线性判别分析，用于分类预测性建模问题。你了解到：

*   LDA 的模型表示以及学习模型的实际区别。
*   如何从训练数据估计 LDA 模型的参数。
*   如何使用该模型对新数据做出预测。
*   如何准备数据以充分利用该方法。

你对这篇文章有任何疑问吗？

发表评论并询问，我会尽力回答。