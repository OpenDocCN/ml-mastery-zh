# 机器学习中使用梯度下降的线性回归教程

> 原文： [`machinelearningmastery.com/linear-regression-tutorial-using-gradient-descent-for-machine-learning/`](https://machinelearningmastery.com/linear-regression-tutorial-using-gradient-descent-for-machine-learning/)

随机梯度下降是机器学习中一种重要且广泛使用的算法。

在本文中，您将了解如何使用随机梯度下降来通过最小化训练数据集上的误差来学习简单线性回归模型的系数。

阅读这篇文章后你会知道：

*   简单线性回归模型的形式。
*   梯度下降和随机梯度下降之间的差异
*   如何使用随机梯度下降来学习简单的线性回归模型。

让我们开始吧。

![Linear Regression Tutorial Using Gradient Descent for Machine Learning](img/bc2e90c85380e16fd5ca9f6fc7036057.jpg)

使用梯度下降进行机器学习的线性回归教程
照片由 [Stig Nygaard](https://www.flickr.com/photos/stignygaard/19990684452/) 拍摄，保留一些权利。

## 教程数据集

我们正在使用的数据集是完全组成的。

这是原始数据。属性 x 是输入变量，y 是我们试图预测的输出变量。如果我们得到更多数据，我们只有 x 值，我们会对预测 y 值感兴趣。

```py
x	y
1	1
2	3
4	3
3	2
5	5
```

下面是 x 与 y 的简单散点图。

![Plot of the Dataset for Simple Linear Regression](img/81b2cddb2dc00c781a6da7d5eab6c992.jpg)

简单线性回归数据集的图

我们可以看到 x 和 y 之间的关系看起来是线性的。在中，我们可能会在从图的左下角对角线到右上角绘制一条线，以概括地描述数据之间的关系。这是一个很好的迹象，表明使用线性回归可能适合这个小数据集。

## 获取免费算法思维导图

![Machine Learning Algorithms Mind Map](img/2ce1275c2a1cac30a9f4eea6edd42d61.jpg)

方便的机器学习算法思维导图的样本。

我已经创建了一个由类型组织的 60 多种算法的方便思维导图。

下载，打印并使用它。

## 简单线性回归

当我们有一个单一的输入属性（x）并且我们想要使用线性回归时，这称为简单线性回归。

通过简单的线性回归，我们希望对数据建模如下：

y = B0 + B1 * x

这是一条线，其中 y 是我们想要预测的输出变量，x 是我们知道的输入变量，B0 和 B1 是我们需要估计的系数。

B0 称为截距，因为它确定了线截取 y 轴的位置。在机器学习中，我们可以称之为偏差，因为它被添加以抵消我们所做的所有预测。 B1 项称为斜率，因为它定义了线的斜率或者在我们添加偏差之前 x 如何转换为 y 值。

该模型称为简单线性回归，因为只有一个输入变量（x）。如果有更多的输入变量（例如 x1，x2 等），那么这将被称为多元回归。

## 随机梯度下降

梯度下降是通过遵循成本函数的梯度来最小化函数的过程。

这包括了解成本的形式以及衍生物，以便从给定的点知道梯度并且可以在该方向上移动，例如，向下走向最小值。

在机器学习中，我们可以使用一种称为随机梯度下降的类似技术来最小化模型对训练数据的误差。

这种方式的工作方式是每个训练实例一次一个地显示给模型。该模型对训练实例做出预测，计算误差并更新模型以减少下一次预测的误差。

该过程可用于在模型中找到导致训练数据上模型的最小误差的系数集。每次迭代使用以下等式更新机器学习语言中称为权重（w）的系数：

w = w - alpha * delta

其中 w 是要优化的系数或权重，alpha 是您必须配置的学习率（例如 0.1），而梯度是归因于权重的训练数据上的模型的误差。

## 具有随机梯度下降的简单线性回归

可以使用随机梯度下降找到简单线性回归中使用的系数。

线性回归是线性系统，系数可以使用线性代数进行分析计算。随机梯度下降不用于计算实际中线性回归的系数（在大多数情况下）。

线性回归确实为学习随机梯度下降提供了有用的练习，这是用于通过机器学习算法最小化成本函数的重要算法。

如上所述，我们的线性回归模型定义如下：

y = B0 + B1 * x

### Gradient Descent Iteration＃1

让我们从两个系数的值 0.0 开始。

B0 = 0.0

B1 = 0.0

y = 0.0 + 0.0 * x

我们可以计算预测误差如下：

错误= p（i） - y（i）

其中 p（i）是我们数据集中第 i 个实例的预测，y（i）是数据集中实例的第 i 个输出变量。

我们现在可以使用第一个训练实例的起点系数来计算 y 的预测值：

x = 1，y = 1

p（i）= 0.0 + 0.0 * 1

p（i）= 0

使用预测输出，我们可以计算出错误：

错误= 0 - 1

错误= -1

我们现在可以在梯度下降的等式中使用此误差来更新权重。我们将首先更新拦截，因为它更容易。

我们可以说 B0 对所有错误负责。这就是说，更新权重将仅使用误差作为梯度。我们可以如下计算 B0 系数的更新：

B0（t + 1）= B0（t） - α*误差

其中 B0（t + 1）是我们将在下一个训练实例上使用的系数的更新版本，B0（t）是 B0 alpha 的当前值是我们的学习率，而误差是我们为训练实例计算的误差。让我们使用 0.01 的小学习率并将值插入等式中，以计算出 B0 的新的和略微优化的值：

B0（t + 1）= 0.0-0.01 * -1.0

B0（t + 1）= 0.01

现在，我们来看看更新 B1 的值。我们使用相同的方程式进行一次小改动。错误由导致它的输入过滤。我们可以使用以下等式更新 B1：

B1（t + 1）= B1（t） - alpha * error * x

在 B1（t + 1）是更新系数的情况下，B1（t）是系数的当前版本，α是与上述相同的学习率，误差是上面计算的相同误差，x 是输入值。

我们可以将我们的数字插入等式并计算 B1 的更新值：

B1（t + 1）= 0.0-0.01 * -1 * 1

B1（t + 1）= 0.01

我们刚刚完成了梯度下降的第一次迭代，我们将权重更新为 B0 = 0.01 和 B1 = 0.01。必须对我们数据集中剩余的 4 个实例重复此过程。

一次通过训练数据集称为时期。

### Gradient Descent Iteration＃20

让我们一起跳。

你可以再重复这个过程 19 次。这是暴露于模型并更新系数的训练数据的 4 个完整时期。

以下是您应该看到的 20 次迭代中系数的所有值的列表：

```py
B0	B1
0.01	0.01
0.0397	0.0694
0.066527	0.176708
0.08056049	0.21880847
0.1188144616	0.410078328
0.1235255337	0.4147894001
0.1439944904	0.4557273134
0.1543254529	0.4970511637
0.1578706635	0.5076867953
0.1809076171	0.6228715633
0.1828698253	0.6248337715
0.1985444516	0.6561830242
0.2003116861	0.6632519622
0.1984110104	0.657549935
0.2135494035	0.7332419008
0.2140814905	0.7337739877
0.2272651958	0.7601413984
0.2245868879	0.7494281668
0.219858174	0.7352420252
0.230897491	0.7904386102
```

我认为 20 个迭代或 4 个时期是一个很好的圆数和一个停下来的好地方。如果你愿意，你可以坚持下去。

您的值应该匹配，但由于不同的电子表格程序和不同的精度，可能会有微小的差异。您可以将每对系数插回到简单的线性回归方程中。这很有用，因为我们可以计算每个训练实例的预测，然后计算误差。

下面是学习过程展开时每组系数的误差图。这是一个有用的图表，因为它向我们显示错误随着每次迭代而减少并且开始在结束时反弹一点。

![Linear Regression Gradient Descent Error versus Iteration](img/ad7c7638b24fc9436fb84bdaee165786.jpg)

线性回归梯度下降误差与迭代

您可以看到我们的最终系数的值为 B0 = 0.230897491 和 B1 = 0.7904386102

让我们将它们插入到我们的简单线性回归模型中，并对训练数据集中的每个点做出预测。

```py
x	y	prediction
1	1	0.9551001992
2	3	1.690342224
4	3	3.160826275
3	2	2.42558425
5	5	3.8960683
```

我们可以再次绘制我们的数据集，覆盖这些预测（x vs y 和 x vs 预测）。通过 5 个预测绘制一条线让我们了解模型与训练数据的匹配程度。

![Simple Linear Regression Model](img/bd9ae26f552f9e7226451e2673caed66.jpg)

简单线性回归模型

## 摘要

在这篇文章中，您发现了简单的线性回归模型以及如何使用随机梯度下降来训练它。

您将完成梯度下降的更新规则的应用。您还学习了如何使用学习的线性回归模型做出预测。

您对此帖子或随机梯度下降的简单线性回归有任何疑问吗？发表评论并提出您的问题，我会尽力回答。